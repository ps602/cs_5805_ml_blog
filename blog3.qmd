---
title: Linear and Non-Linear Regression
jupyter: python3
---

Regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. The goal is to understand and quantify the impact of the independent variables on the dependent variable. Regression analysis is widely employed in various fields, including economics, finance, biology, and machine learning, to make predictions, infer relationships, and understand patterns within data. The most fundamental form of regression is linear regression, where the relationship between variables is assumed to be linear. However, when the relationship is more complex and cannot be adequately captured by a straight line, non-linear regression models are employed.

Linear regression assumes a linear relationship between the independent and dependent variables. The model equation is represented as a linear combination of the independent variables and an ε which is the error term. The coefficients represent the slope or impact of each independent variable on the dependent variable. Linear regression is straightforward, interpretable, and computationally efficient, making it a commonly used method. However, it may not capture complex, non-linear relationships effectively.

Non-linear regression allows for more flexibility in modeling relationships that are not linear. The model equation is more complex and may involve non-linear functions, such as exponentials, logarithms, polynomials, or trigonometric functions. This flexibility enables non-linear regression to better represent curved or intricate patterns in the data. Non-linear regression models are particularly useful when the relationship between variables is better described by a curve, wave, or other non-linear shapes. While non-linear regression introduces more complexity, it requires careful consideration of model selection, and the interpretation of parameters may not be as intuitive as in linear regression. Various techniques, such as gradient descent or optimization algorithms, are employed to estimate the parameters of non-linear models from data.

## Linear Regression

Linear Regression is a supervised machine learning algorithm used for predicting a continuous outcome variable (dependent variable) based on one or more predictor variables (independent variables). The basic idea is to find the best-fit straight line that minimizes the difference between the observed and predicted values.

### Simple Linear Regression

For a simple linear regression with one independent variable:

            y=β0+β1⋅x+ε

y: Dependent variable (the variable we want to predict)
x: Independent variable (predictor variable)
β0: Intercept (y-intercept), the value of y when x=0
β1: Slope (gradient), represents the change in y for a unit change in x
ε: Error term, represents the unobserved factors affecting y

The objective is to minimize the sum of squared differences between the observed (y) and predicted (y^) values:

            J(β0,β1 )= 1/2m∑i=1m (y^i−yi)^2 where:

m: Number of data points
y^i: Predicted value for the i-th data point
yi: Observed value for the i-th data point

Gradient Descent is commonly used to find the values of β0 and β1 that minimize the cost function J. The update rule is:

            βj :=βj-α ∂/∂βj J(β0,β1) where

α is the learning rate.

We choose the "Advertising" dataset from Kaggle where we want to analyze the relationship between "TV Advertising" and "Sales". 

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the California Housing dataset
advertising = pd.read_csv('advertising.csv')

advertising.head()
```

We perform exploratory data analysis (EDA) on the data to understand the characteristics of the data, unveil patterns, detect anomalies, and gather insights that can guide subsequent analyses and modeling. We perform data wrangling, where the important steps include data cleaning by removal null-values and outliers.

```{python}
advertising.describe()
```

Outliers in a dataset are often identified as points that fall beyond a specified distance from the edges of the box and whiskers. This distance is typically determined by a multiplier of the IQR, and data points beyond this range are considered potential outliers.

```{python}
import seaborn as sns
# Checking for outliers
sns.boxplot(advertising['Sales'])
plt.show()
```

Then we look at the feature interdependence using pairplots and the heatmap of the correlation between the different variables.

```{python}
# Let's see how Sales are related with other variables using scatter plot.
sns.pairplot(advertising, x_vars=['TV', 'Newspaper', 'Radio'], y_vars='Sales', height=4, aspect=1, kind='scatter')
plt.show()
# Let's see the correlation between different variables.
sns.heatmap(advertising.corr(), cmap="YlGnBu", annot = True)
plt.show()
```

As is visible from the pairplot and the heatmap, the variable TV seems to be most correlated with Sales. So let's go ahead and perform simple linear regression using TV as our feature variable.

We need to split our variable into training and testing sets. We'll accomplish this by utilizing the train_test_split function from the sklearn.model_selection library. It's customary to allocate 70% of the data to our training dataset, leaving the remaining 30% for the test dataset. By default, we fit a line on the dataset that passes through the origin using the statsmodels library. However, to introduce an intercept, we must manually utilize the add_constant attribute of statsmodels. Once we've added the constant to our X_train dataset, we can proceed to fit a regression line using the OLS (Ordinary Least Squares) attribute of statsmodels, as demonstrated below.

```{python}
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression, Lasso, Ridge

X = advertising['TV']
y = advertising['Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)

# Add a constant to get an intercept
X_train_sm = sm.add_constant(X_train)

# Fit the resgression line using 'OLS'
lr = sm.OLS(y_train, X_train_sm).fit()

print(lr.summary())
```

The key metrics in the summary that we note to see if the Linear Regression is a good fit are the following, 

1. R-squared: Indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. A higher R-squared value suggests a better fit.

2. p-value: The p-value associated with the t-statistic. The t-statistic tests the null hypothesis that the coefficient is equal to zero. A high absolute t-value and a low associated p-value suggest that the variable is significant. A low p-value (typically less than 0.05) suggests that the variable is statistically significant.

We can see from the above metrics that the fit is significant. So we visualize how well our model has fit the data by plotting the Line given by the fitted slope and intercept and see how it fits our data's scatter plot.

```{python}
plt.scatter(X_train, y_train)
plt.plot(X_train, 6.948 + 0.054*X_train, 'r',label="Predictions")
plt.xlabel('TV Advertising')
plt.ylabel('Sales')
plt.legend(loc="upper left")
plt.show()
```

The fit of the Linear Regression model can also be tested by seeing if the residual errors are normally distributed with zero mean and unit variance. We can see below that the residual distribution is normal as expected. 

```{python}
y_train_pred = lr.predict(X_train_sm)
res = (y_train - y_train_pred)
fig = plt.figure()
sns.displot(res, bins = 15)                # Plot heading 
plt.xlabel('Residual')         # X-label
plt.show()
```

These models need to be also evaluated on a test set that they were not exposed to while training, to understand if there is any overfitting or underfitting that has occurred due to bias and variance.

```{python}
X_test_sm = sm.add_constant(X_test)

# Predict the y values corresponding to X_test_sm
y_pred = lr.predict(X_test_sm)

plt.scatter(X_test, y_test)
plt.plot(X_test, 6.948 + 0.054 * X_test, 'r', label="Predictions")
plt.legend(loc="upper left")
plt.xlabel('TV Advertising')
plt.ylabel('Sales')
plt.show()
```

We can see that our model performs well on the test set as well, making it robust and generalizable to new data that it has not been exposed during training process.

### Regularized Linear Regression Models

Lasso Regression, or L1 regularization, is a linear regression technique that includes a penalty term in the cost function equivalent to the absolute values of the coefficients. This penalty encourages sparsity in the model, meaning it tends to force some of the coefficient estimates to be exactly zero. The regularization term is controlled by a hyperparameter, usually denoted as α. A higher α leads to a stronger regularization effect. Lasso regression is particularly useful when dealing with datasets with a large number of features, as it can automatically perform feature selection by setting some coefficients to zero, effectively ignoring less relevant predictors.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge

X = advertising['TV']
y = advertising['Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)

# Ordinary Least Squares (OLS)
ols_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()
ols_pred = ols_model.predict(sm.add_constant(X_test))

# Lasso Regression with statsmodels
lasso_model = sm.OLS(y_train, sm.add_constant(X_train)).fit_regularized(alpha=0.01, L1_wt=1)  # L1_wt=1 for Lasso
lasso_pred = lasso_model.predict(sm.add_constant(X_test))
print(lasso_model.params)
```

Ridge Regression, or L2 regularization, is another variant of linear regression that includes a penalty term proportional to the squared values of the coefficients in the cost function. Similar to Lasso, Ridge introduces regularization controlled by a hyperparameter α. Ridge tends to shrink the coefficients towards zero but rarely sets them exactly to zero. It is effective in mitigating the issue of multicollinearity, where predictor variables are highly correlated. Ridge can stabilize the model and prevent it from being too sensitive to variations in the input data.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso, Ridge

X = advertising['TV']
y = advertising['Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)

# Ridge Regression with statsmodels
ridge_model = sm.OLS(y_train, sm.add_constant(X_train)).fit_regularized(alpha=0.01, L1_wt=0)  # L1_wt=0 for Ridge
ridge_pred = ridge_model.predict(sm.add_constant(X_test))
print(ridge_model.params)
```

```{python}
# Compare performance using Mean Squared Error
mse_ols = np.mean((y_test - ols_pred)**2)
mse_lasso = np.mean((y_test - lasso_pred)**2)
mse_ridge = np.mean((y_test - ridge_pred)**2)

print(f'Mean Squared Error (OLS): {mse_ols}')
print(f'Mean Squared Error (Lasso): {mse_lasso}')
print(f'Mean Squared Error (Ridge): {mse_ridge}')

# Visualizing actual vs predicted values
plt.figure(figsize=(12, 6))

# Plot for OLS
plt.subplot(1, 3, 1)
plt.scatter(X_test, y_test)
plt.plot(X_test, 6.948+ X_test*0.054, 'r', label="Predictions")
plt.title('OLS: Actual vs Predicted')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')

# Plot for Lasso
plt.subplot(1, 3, 2)
plt.scatter(X_test, y_test)
plt.plot(X_test, 6.913 + X_test * 0.054, 'r', label="Predictions")
plt.title('Lasso: Actual vs Predicted')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')

# Plot for Ridge
plt.subplot(1, 3, 3)
plt.scatter(X_test, y_test)
plt.plot(X_test, 6.710 + 0.055 * X_test, 'r', label="Predictions")
plt.title('Ridge: Actual vs Predicted')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')

plt.tight_layout()
plt.show()
```

In this example, We've used the fit_regularized method from statsmodels with the L1 penalty (L1_wt=1) for Lasso regression and the L2 penalty (L1_wt=0) for Ridge regression. The performance is then compared using Mean Squared Error, and the scatter plots visualize the actual vs predicted values for each model. We can adjust the regularization strength (alpha) as needed, we are able to see the MSE is least for OLS case itself.

## Non-Linear Regression

Non-linear regression allows for more flexibility in modeling relationships that are not linear. The model equation is more complex and may involve non-linear functions, such as exponentials, logarithms, polynomials, or trigonometric functions. This flexibility enables non-linear regression to better represent curved or intricate patterns in the data. Non-linear regression models are particularly useful when the relationship between variables is better described by a curve, wave, or other non-linear shapes.

### Polynomial Regression

Polynomial regression is an extension of linear regression, allowing for the modeling of relationships that are not strictly linear. While linear regression assumes a linear relationship between the independent and dependent variables, polynomial regression accommodates curves and non-linear patterns. In polynomial regression, the relationship is represented by a polynomial equation, allowing for more flexibility in capturing complex patterns within the data. 

The polynomial regression equation of degree n is given by:
                Y=b0+b1X+b2X^2+…+bnX^n+ε
Here, 
Y is the dependent variable, 
X is the independent variable, 
b0 is the intercept, 
b1,b2,…,bn are the coefficients, 
X^n represents the terms with increasing powers of 
and ε is the error term.

The coefficients are estimated from the data using methods like the method of least squares. We create a synthetic polynomial dataset to see how well the regressor is able to fit the polynomial function we have defined. We can see that the scatter plot below shows the data to be quadratic in nature as defined by our function y.

```{python}
np.random.seed(42)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)
```

```{python}
plt.figure(figsize=(6, 4))
plt.plot(X, y, "b.")
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.axis([-3, 3, 0, 10])
plt.grid()
plt.show()
```

This example generates a quadratic dataset and fits a second-degree polynomial using scikit-learn. We can adjust the degree parameter in PolynomialFeatures to experiment with different polynomial degrees. We need to expand the features by adding columns for 
X2,X3,…,Xn up to the desired degree n. We use a linear regression algorithm to fit the polynomial equation to the expanded polynomial features. This involves estimating the coefficients b1,b2,…,bn that minimize the sum of squared differences between the observed and predicted values.
 
```{python}
from sklearn.preprocessing import PolynomialFeatures

poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
lin_reg.intercept_, lin_reg.coef_
```

Once the model is trained, we use it to make predictions on new or unseen data. We can assess the performance of the model using appropriate metrics such as Mean Squared Error (MSE) or R-squared. We can visualize the fitted polynomial curve along with the data points to understand how well the model captures the underlying patterns.

```{python}
X_new = np.linspace(-3, 3, 100).reshape(100, 1)
X_new_poly = poly_features.transform(X_new)
y_new = lin_reg.predict(X_new_poly)

plt.figure(figsize=(6, 4))
plt.plot(X, y, "b.")
plt.plot(X_new, y_new, "r-", linewidth=2, label="Predictions")
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.legend(loc="upper left")
plt.axis([-3, 3, 0, 10])
plt.grid()
plt.show()
```

The choice of the degree n is crucial. A too high degree may lead to overfitting, capturing noise in the data rather than the actual trend. Here we test for three different degrees [1,2,100] and see how it changes the fit and how well its able to capture the noisy points and outliers.

```{python}

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

plt.figure(figsize=(6, 4))

for style, width, degree in (("r-+", 2, 1), ("b--", 2, 2), ("g-", 1, 100)):
    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)
    std_scaler = StandardScaler()
    lin_reg = LinearRegression()
    polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)
    polynomial_regression.fit(X, y)
    y_newbig = polynomial_regression.predict(X_new)
    label = f"{degree} degree{'s' if degree > 1 else ''}"
    plt.plot(X_new, y_newbig, style, label=label, linewidth=width)

plt.plot(X, y, "b.", linewidth=3)
plt.legend(loc="upper left")
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.axis([-3, 3, 0, 10])
plt.grid()
plt.show()
```
