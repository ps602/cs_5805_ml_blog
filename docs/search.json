[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Priya Shanmugasundaram. This is my blog submission for CS 5805: Machine Learning final project. Happy Reading :)"
  },
  {
    "objectID": "index.html#the-normal-equation",
    "href": "index.html#the-normal-equation",
    "title": "Setup",
    "section": "The Normal Equation",
    "text": "The Normal Equation\n\nimport numpy as np\n\nnp.random.seed(42)  # to make this code example reproducible\nm = 100  # number of instances\nX = 2 * np.random.rand(m, 1)  # column vector\ny = 4 + 3 * X + np.random.randn(m, 1)  # column vector\n\n\n# extra code – generates and saves Figure 4–1\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"generated_data_plot\")\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import add_dummy_feature\n\nX_b = add_dummy_feature(X)  # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n\n\ntheta_best\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\nX_new = np.array([[0], [2]])\nX_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\ny_predict = X_new_b @ theta_best\ny_predict\n\narray([[4.21509616],\n       [9.75532293]])\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.plot(X, y, \"b.\")\n\n# extra code – beautifies and saves Figure 4–2\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.legend(loc=\"upper left\")\nsave_fig(\"linear_model_predictions_plot\")\n\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([4.21509616]), array([[2.77011339]]))\n\n\n\nlin_reg.predict(X_new)\n\narray([[4.21509616],\n       [9.75532293]])\n\n\nThe LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:\n\ntheta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\ntheta_best_svd\n\narray([[4.21509616],\n       [2.77011339]])\n\n\nThis function computes \\(\\mathbf{X}^+\\mathbf{y}\\), where \\(\\mathbf{X}^{+}\\) is the pseudoinverse of \\(\\mathbf{X}\\) (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly:\n\nnp.linalg.pinv(X_b) @ y\n\narray([[4.21509616],\n       [2.77011339]])"
  },
  {
    "objectID": "index.html#batch-gradient-descent",
    "href": "index.html#batch-gradient-descent",
    "title": "Setup",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\n\neta = 0.1  # learning rate\nn_epochs = 1000\nm = len(X_b)  # number of instances\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # randomly initialized model parameters\n\nfor epoch in range(n_epochs):\n    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n    theta = theta - eta * gradients\n\nThe trained model parameters:\n\ntheta\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\n# extra code – generates and saves Figure 4–8\n\nimport matplotlib as mpl\n\ndef plot_gradient_descent(theta, eta):\n    m = len(X_b)\n    plt.plot(X, y, \"b.\")\n    n_epochs = 1000\n    n_shown = 20\n    theta_path = []\n    for epoch in range(n_epochs):\n        if epoch &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(epoch / n_shown + 0.15))\n            plt.plot(X_new, y_predict, linestyle=\"solid\", color=color)\n        gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n        theta = theta - eta * gradients\n        theta_path.append(theta)\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 2, 0, 15])\n    plt.grid()\n    plt.title(fr\"$\\eta = {eta}$\")\n    return theta_path\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nplt.figure(figsize=(10, 4))\nplt.subplot(131)\nplot_gradient_descent(theta, eta=0.02)\nplt.ylabel(\"$y$\", rotation=0)\nplt.subplot(132)\ntheta_path_bgd = plot_gradient_descent(theta, eta=0.1)\nplt.gca().axes.yaxis.set_ticklabels([])\nplt.subplot(133)\nplt.gca().axes.yaxis.set_ticklabels([])\nplot_gradient_descent(theta, eta=0.5)\nsave_fig(\"gradient_descent_plot\")\nplt.show()"
  },
  {
    "objectID": "index.html#stochastic-gradient-descent",
    "href": "index.html#stochastic-gradient-descent",
    "title": "Setup",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\ntheta_path_sgd = []  # extra code – we need to store the path of theta in the\n                     #              parameter space to plot the next figure\n\n\nn_epochs = 50\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nn_shown = 20  # extra code – just needed to generate the figure below\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n\nfor epoch in range(n_epochs):\n    for iteration in range(m):\n\n        # extra code – these 4 lines are used to generate the figure\n        if epoch == 0 and iteration &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(iteration / n_shown + 0.15))\n            plt.plot(X_new, y_predict, color=color)\n\n        random_index = np.random.randint(m)\n        xi = X_b[random_index : random_index + 1]\n        yi = y[random_index : random_index + 1]\n        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n        eta = learning_schedule(epoch * m + iteration)\n        theta = theta - eta * gradients\n        theta_path_sgd.append(theta)  # extra code – to generate the figure\n\n# extra code – this section beautifies and saves Figure 4–10\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"sgd_plot\")\nplt.show()\n\n\n\n\n\ntheta\n\narray([[4.21076011],\n       [2.74856079]])\n\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n                       n_iter_no_change=100, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n\nSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)\n\n\n\nsgd_reg.intercept_, sgd_reg.coef_\n\n(array([4.21278812]), array([2.77270267]))"
  },
  {
    "objectID": "index.html#mini-batch-gradient-descent",
    "href": "index.html#mini-batch-gradient-descent",
    "title": "Setup",
    "section": "Mini-batch gradient descent",
    "text": "Mini-batch gradient descent\nThe code in this section is used to generate the next figure, it is not in the book.\n\n# extra code – this cell generates and saves Figure 4–11\n\nfrom math import ceil\n\nn_epochs = 50\nminibatch_size = 20\nn_batches_per_epoch = ceil(m / minibatch_size)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nt0, t1 = 200, 1000  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\ntheta_path_mgd = []\nfor epoch in range(n_epochs):\n    shuffled_indices = np.random.permutation(m)\n    X_b_shuffled = X_b[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    for iteration in range(0, n_batches_per_epoch):\n        idx = iteration * minibatch_size\n        xi = X_b_shuffled[idx : idx + minibatch_size]\n        yi = y_shuffled[idx : idx + minibatch_size]\n        gradients = 2 / minibatch_size * xi.T @ (xi @ theta - yi)\n        eta = learning_schedule(iteration)\n        theta = theta - eta * gradients\n        theta_path_mgd.append(theta)\n\ntheta_path_bgd = np.array(theta_path_bgd)\ntheta_path_sgd = np.array(theta_path_sgd)\ntheta_path_mgd = np.array(theta_path_mgd)\n\nplt.figure(figsize=(7, 4))\nplt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1,\n         label=\"Stochastic\")\nplt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2,\n         label=\"Mini-batch\")\nplt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3,\n         label=\"Batch\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$   \", rotation=0)\nplt.axis([2.6, 4.6, 2.3, 3.4])\nplt.grid()\nsave_fig(\"gradient_descent_paths_plot\")\nplt.show()"
  },
  {
    "objectID": "index.html#ridge-regression",
    "href": "index.html#ridge-regression",
    "title": "Setup",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nLet’s generate a very small and noisy linear dataset:\n\n# extra code – we've done this type of generation several times before\nnp.random.seed(42)\nm = 20\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\nX_new = np.linspace(0, 3, 100).reshape(100, 1)\n\n\n# extra code – a quick peek at the dataset we just generated\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \".\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$  \", rotation=0)\nplt.axis([0, 3, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55325833]])\n\n\n\n# extra code – this cell generates and saves Figure 4–17\n\ndef plot_model(model_class, polynomial, alphas, **model_kwargs):\n    plt.plot(X, y, \"b.\", linewidth=3)\n    for alpha, style in zip(alphas, (\"b:\", \"g--\", \"r-\")):\n        if alpha &gt; 0:\n            model = model_class(alpha, **model_kwargs)\n        else:\n            model = LinearRegression()\n        if polynomial:\n            model = make_pipeline(\n                PolynomialFeatures(degree=10, include_bias=False),\n                StandardScaler(),\n                model)\n        model.fit(X, y)\n        y_new_regul = model.predict(X_new)\n        plt.plot(X_new, y_new_regul, style, linewidth=2,\n                 label=fr\"$\\alpha = {alpha}$\")\n    plt.legend(loc=\"upper left\")\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 3, 0, 3.5])\n    plt.grid()\n\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"ridge_regression_plot\")\nplt.show()\n\n\n\n\n\nsgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n                       max_iter=1000, eta0=0.01, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\nsgd_reg.predict([[1.5]])\n\narray([1.55302613])\n\n\n\n# extra code – show that we get roughly the same solution as earlier when\n#              we use Stochastic Average GD (solver=\"sag\")\nridge_reg = Ridge(alpha=0.1, solver=\"sag\", random_state=42)\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55326019]])\n\n\n\n# extra code – shows the closed form solution of Ridge regression,\n#              compare with the next Ridge model's learned parameters below\nalpha = 0.1\nA = np.array([[0., 0.], [0., 1.]])\nX_b = np.c_[np.ones(m), X]\nnp.linalg.inv(X_b.T @ X_b + alpha * A) @ X_b.T @ y\n\narray([[0.97898394],\n       [0.3828496 ]])\n\n\n\nridge_reg.intercept_, ridge_reg.coef_  # extra code\n\n(array([0.97896386]), array([[0.38286422]]))"
  },
  {
    "objectID": "index.html#lasso-regression",
    "href": "index.html#lasso-regression",
    "title": "Setup",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nfrom sklearn.linear_model import Lasso\n\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X, y)\nlasso_reg.predict([[1.5]])\n\narray([1.53788174])\n\n\n\n# extra code – this cell generates and saves Figure 4–18\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"lasso_regression_plot\")\nplt.show()\n\n\n\n\n\n# extra code – this BIG cell generates and saves Figure 4–19\n\nt1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n\nt1s = np.linspace(t1a, t1b, 500)\nt2s = np.linspace(t2a, t2b, 500)\nt1, t2 = np.meshgrid(t1s, t2s)\nT = np.c_[t1.ravel(), t2.ravel()]\nXr = np.array([[1, 1], [1, -1], [1, 0.5]])\nyr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n\nJ = (1 / len(Xr) * ((T @ Xr.T - yr.T) ** 2).sum(axis=1)).reshape(t1.shape)\n\nN1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\nN2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n\nt_min_idx = np.unravel_index(J.argmin(), J.shape)\nt1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n\nt_init = np.array([[0.25], [-1]])\n\ndef bgd_path(theta, X, y, l1, l2, core=1, eta=0.05, n_iterations=200):\n    path = [theta]\n    for iteration in range(n_iterations):\n        gradients = (core * 2 / len(X) * X.T @ (X @ theta - y)\n                     + l1 * np.sign(theta) + l2 * theta)\n        theta = theta - eta * gradients\n        path.append(theta)\n    return np.array(path)\n\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n\nfor i, N, l1, l2, title in ((0, N1, 2.0, 0, \"Lasso\"), (1, N2, 0, 2.0, \"Ridge\")):\n    JR = J + l1 * N1 + l2 * 0.5 * N2 ** 2\n\n    tr_min_idx = np.unravel_index(JR.argmin(), JR.shape)\n    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n\n    levels = np.exp(np.linspace(0, 1, 20)) - 1\n    levelsJ = levels * (J.max() - J.min()) + J.min()\n    levelsJR = levels * (JR.max() - JR.min()) + JR.min()\n    levelsN = np.linspace(0, N.max(), 10)\n\n    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n    path_N = bgd_path(theta=np.array([[2.0], [0.5]]), X=Xr, y=yr,\n                      l1=np.sign(l1) / 3, l2=np.sign(l2), core=0)\n    ax = axes[i, 0]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, N / 2.0, levels=levelsN)\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.set_title(fr\"$\\ell_{i + 1}$ penalty\")\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n    ax.set_ylabel(r\"$\\theta_2$\", rotation=0)\n\n    ax = axes[i, 1]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.plot(t1r_min, t2r_min, \"rs\")\n    ax.set_title(title)\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n\nsave_fig(\"lasso_vs_ridge_plot\")\nplt.show()"
  },
  {
    "objectID": "index.html#elastic-net",
    "href": "index.html#elastic-net",
    "title": "Setup",
    "section": "Elastic Net",
    "text": "Elastic Net\n\nfrom sklearn.linear_model import ElasticNet\n\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic_net.fit(X, y)\nelastic_net.predict([[1.5]])\n\narray([1.54333232])"
  },
  {
    "objectID": "index.html#early-stopping",
    "href": "index.html#early-stopping",
    "title": "Setup",
    "section": "Early Stopping",
    "text": "Early Stopping\nLet’s go back to the quadratic dataset we used earlier:\n\nfrom copy import deepcopy\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n# extra code – creates the same quadratic dataset as earlier and splits it\nnp.random.seed(42)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\nX_train, y_train = X[: m // 2], y[: m // 2, 0]\nX_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n                              StandardScaler())\nX_train_prep = preprocessing.fit_transform(X_train)\nX_valid_prep = preprocessing.transform(X_valid)\nsgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\nn_epochs = 500\nbest_valid_rmse = float('inf')\ntrain_errors, val_errors = [], []  # extra code – it's for the figure below\n\nfor epoch in range(n_epochs):\n    sgd_reg.partial_fit(X_train_prep, y_train)\n    y_valid_predict = sgd_reg.predict(X_valid_prep)\n    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n    if val_error &lt; best_valid_rmse:\n        best_valid_rmse = val_error\n        best_model = deepcopy(sgd_reg)\n\n    # extra code – we evaluate the train error and save it for the figure\n    y_train_predict = sgd_reg.predict(X_train_prep)\n    train_error = mean_squared_error(y_train, y_train_predict, squared=False)\n    val_errors.append(val_error)\n    train_errors.append(train_error)\n\n# extra code – this section generates and saves Figure 4–20\nbest_epoch = np.argmin(val_errors)\nplt.figure(figsize=(6, 4))\nplt.annotate('Best model',\n             xy=(best_epoch, best_valid_rmse),\n             xytext=(best_epoch, best_valid_rmse + 0.5),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], \"k:\", linewidth=2)\nplt.plot(val_errors, \"b-\", linewidth=3, label=\"Validation set\")\nplt.plot(best_epoch, best_valid_rmse, \"bo\")\nplt.plot(train_errors, \"r--\", linewidth=2, label=\"Training set\")\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"RMSE\")\nplt.axis([0, n_epochs, 0, 3.5])\nplt.grid()\nsave_fig(\"early_stopping_plot\")\nplt.show()"
  },
  {
    "objectID": "index.html#estimating-probabilities",
    "href": "index.html#estimating-probabilities",
    "title": "Setup",
    "section": "Estimating Probabilities",
    "text": "Estimating Probabilities\n\n# extra code – generates and saves Figure 4–21\n\nlim = 6\nt = np.linspace(-lim, lim, 100)\nsig = 1 / (1 + np.exp(-t))\n\nplt.figure(figsize=(8, 3))\nplt.plot([-lim, lim], [0, 0], \"k-\")\nplt.plot([-lim, lim], [0.5, 0.5], \"k:\")\nplt.plot([-lim, lim], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\")\nplt.axis([-lim, lim, -0.1, 1.1])\nplt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\nplt.grid()\nsave_fig(\"logistic_function_plot\")\nplt.show()"
  },
  {
    "objectID": "index.html#decision-boundaries",
    "href": "index.html#decision-boundaries",
    "title": "Setup",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\nlist(iris)\n\n['data',\n 'target',\n 'frame',\n 'target_names',\n 'DESCR',\n 'feature_names',\n 'filename',\n 'data_module']\n\n\n\nprint(iris.DESCR)  # extra code – it's a bit too long\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n|details-start|\n**References**\n|details-split|\n\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n  Mathematical Statistics\" (John Wiley, NY, 1950).\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n  Structure and Classification Rule for Recognition in Partially Exposed\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n  on Information Theory, May 1972, 431-433.\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n  conceptual clustering system finds 3 classes in the data.\n- Many, many more ...\n\n|details-end|\n\n\n\niris.data.head(3)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n\n\n\n\n\n\niris.target.head(3)  # note that the instances are not shuffled\n\n0    0\n1    0\n2    0\nName: target, dtype: int64\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\n\nLogisticRegression(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=42)\n\n\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\ny_proba = log_reg.predict_proba(X_new)\ndecision_boundary = X_new[y_proba[:, 1] &gt;= 0.5][0, 0]\n\nplt.figure(figsize=(8, 3))  # extra code – not needed, just formatting\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n         label=\"Not Iris virginica proba\")\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\nplt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n         label=\"Decision boundary\")\n\n# extra code – this section beautifies and saves Figure 4–23\nplt.arrow(x=decision_boundary, y=0.08, dx=-0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\nplt.arrow(x=decision_boundary, y=0.92, dx=0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\nplt.plot(X_train[y_train == 0], y_train[y_train == 0], \"bs\")\nplt.plot(X_train[y_train == 1], y_train[y_train == 1], \"g^\")\nplt.xlabel(\"Petal width (cm)\")\nplt.ylabel(\"Probability\")\nplt.legend(loc=\"center left\")\nplt.axis([0, 3, -0.02, 1.02])\nplt.grid()\nsave_fig(\"logistic_regression_plot\")\n\nplt.show()\n\n\n\n\n\ndecision_boundary\n\n1.6516516516516517\n\n\n\nlog_reg.predict([[1.7], [1.5]])\n\narray([ True, False])\n\n\n\n# extra code – this cell generates and saves Figure 4–24\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(C=2, random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# for the contour plot\nx0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1),\n                     np.linspace(0.8, 2.7, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]  # one instance per point on the figure\ny_proba = log_reg.predict_proba(X_new)\nzz = y_proba[:, 1].reshape(x0.shape)\n\n# for the decision boundary\nleft_right = np.array([2.9, 7])\nboundary = -((log_reg.coef_[0, 0] * left_right + log_reg.intercept_[0])\n             / log_reg.coef_[0, 1])\n\nplt.figure(figsize=(10, 4))\nplt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \"bs\")\nplt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \"g^\")\ncontour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1)\nplt.plot(left_right, boundary, \"k--\", linewidth=3)\nplt.text(3.5, 1.27, \"Not Iris virginica\", color=\"b\", ha=\"center\")\nplt.text(6.5, 2.3, \"Iris virginica\", color=\"g\", ha=\"center\")\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.axis([2.9, 7, 0.8, 2.7])\nplt.grid()\nsave_fig(\"logistic_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "index.html#softmax-regression",
    "href": "index.html#softmax-regression",
    "title": "Setup",
    "section": "Softmax Regression",
    "text": "Softmax Regression\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax_reg = LogisticRegression(C=30, random_state=42)\nsoftmax_reg.fit(X_train, y_train)\n\nLogisticRegression(C=30, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=30, random_state=42)\n\n\n\nsoftmax_reg.predict([[5, 2]])\n\narray([2])\n\n\n\nsoftmax_reg.predict_proba([[5, 2]]).round(2)\n\narray([[0.  , 0.04, 0.96]])\n\n\n\n# extra code – this cell generates and saves Figure 4–25\n\nfrom matplotlib.colors import ListedColormap\n\ncustom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\n\ny_proba = softmax_reg.predict_proba(X_new)\ny_predict = softmax_reg.predict(X_new)\n\nzz1 = y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"center left\")\nplt.axis([0.5, 7, 0, 3.5])\nplt.grid()\nsave_fig(\"softmax_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "index.html#to-11.",
    "href": "index.html#to-11.",
    "title": "Setup",
    "section": "1. to 11.",
    "text": "1. to 11.\n\nIf you have a training set with millions of features you can use Stochastic Gradient Descent or Mini-batch Gradient Descent, and perhaps Batch Gradient Descent if the training set fits in memory. But you cannot use the Normal Equation or the SVD approach because the computational complexity grows quickly (more than quadratically) with the number of features.\nIf the features in your training set have very different scales, the cost function will have the shape of an elongated bowl, so the Gradient Descent algorithms will take a long time to converge. To solve this you should scale the data before training the model. Note that the Normal Equation or SVD approach will work just fine without scaling. Moreover, regularized models may converge to a suboptimal solution if the features are not scaled: since regularization penalizes large weights, features with smaller values will tend to be ignored compared to features with larger values.\nGradient Descent cannot get stuck in a local minimum when training a Logistic Regression model because the cost function is convex. Convex means that if you draw a straight line between any two points on the curve, the line never crosses the curve.\nIf the optimization problem is convex (such as Linear Regression or Logistic Regression), and assuming the learning rate is not too high, then all Gradient Descent algorithms will approach the global optimum and end up producing fairly similar models. However, unless you gradually reduce the learning rate, Stochastic GD and Mini-batch GD will never truly converge; instead, they will keep jumping back and forth around the global optimum. This means that even if you let them run for a very long time, these Gradient Descent algorithms will produce slightly different models.\nIf the validation error consistently goes up after every epoch, then one possibility is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However, if the training error is not going up, then your model is overfitting the training set and you should stop training.\nDue to their random nature, neither Stochastic Gradient Descent nor Mini-batch Gradient Descent is guaranteed to make progress at every single training iteration. So if you immediately stop training when the validation error goes up, you may stop much too early, before the optimum is reached. A better option is to save the model at regular intervals; then, when it has not improved for a long time (meaning it will probably never beat the record), you can revert to the best saved model.\nStochastic Gradient Descent has the fastest training iteration since it considers only one training instance at a time, so it is generally the first to reach the vicinity of the global optimum (or Mini-batch GD with a very small mini-batch size). However, only Batch Gradient Descent will actually converge, given enough training time. As mentioned, Stochastic GD and Mini-batch GD will bounce around the optimum, unless you gradually reduce the learning rate.\nIf the validation error is much higher than the training error, this is likely because your model is overfitting the training set. One way to try to fix this is to reduce the polynomial degree: a model with fewer degrees of freedom is less likely to overfit. Another thing you can try is to regularize the model—for example, by adding an ℓ₂ penalty (Ridge) or an ℓ₁ penalty (Lasso) to the cost function. This will also reduce the degrees of freedom of the model. Lastly, you can try to increase the size of the training set.\nIf both the training error and the validation error are almost equal and fairly high, the model is likely underfitting the training set, which means it has a high bias. You should try reducing the regularization hyperparameter α.\nLet’s see:\n\n\nA model with some regularization typically performs better than a model without any regularization, so you should generally prefer Ridge Regression over plain Linear Regression.\nLasso Regression uses an ℓ₁ penalty, which tends to push the weights down to exactly zero. This leads to sparse models, where all weights are zero except for the most important weights. This is a way to perform feature selection automatically, which is good if you suspect that only a few features actually matter. When you are not sure, you should prefer Ridge Regression.\nElastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training instances). However, it does add an extra hyperparameter to tune. If you want Lasso without the erratic behavior, you can just use Elastic Net with an l1_ratio close to 1.\n\n\nIf you want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not exclusive classes (i.e., all four combinations are possible) you should train two Logistic Regression classifiers."
  },
  {
    "objectID": "index.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "href": "index.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "title": "Setup",
    "section": "12. Batch Gradient Descent with early stopping for Softmax Regression",
    "text": "12. Batch Gradient Descent with early stopping for Softmax Regression\nExercise: Implement Batch Gradient Descent with early stopping for Softmax Regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset.\nLet’s start by loading the data. We will just reuse the Iris dataset we loaded earlier.\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"].values\n\nWe need to add the bias term for every instance (\\(x_0 = 1\\)). The easiest option to do this would be to use Scikit-Learn’s add_dummy_feature() function, but the point of this exercise is to get a better understanding of the algorithms by implementing them manually. So here is one possible implementation:\n\nX_with_bias = np.c_[np.ones(len(X)), X]\n\nThe easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn’s train_test_split() function, but again, we want to do it manually:\n\ntest_ratio = 0.2\nvalidation_ratio = 0.2\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nnp.random.seed(42)\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\nThe targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for any given instance is a one-hot vector). Let’s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance. To understand this code, you need to know that np.diag(np.ones(n)) creates an n×n matrix full of 0s except for 1s on the main diagonal. Moreover, if a is a NumPy array, then a[[1, 3, 2]] returns an array with 3 rows equal to a[1], a[3] and a[2] (this is advanced NumPy indexing).\n\ndef to_one_hot(y):\n    return np.diag(np.ones(y.max() + 1))[y]\n\nLet’s test this function on the first 10 instances:\n\ny_train[:10]\n\narray([1, 0, 2, 1, 1, 0, 1, 2, 1, 1])\n\n\n\nto_one_hot(y_train[:10])\n\narray([[0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.]])\n\n\nLooks good, so let’s create the target class probabilities matrix for the training set and the test set:\n\nY_train_one_hot = to_one_hot(y_train)\nY_valid_one_hot = to_one_hot(y_valid)\nY_test_one_hot = to_one_hot(y_test)\n\nNow let’s scale the inputs. We compute the mean and standard deviation of each feature on the training set (except for the bias feature), then we center and scale each feature in the training set, the validation set, and the test set:\n\nmean = X_train[:, 1:].mean(axis=0)\nstd = X_train[:, 1:].std(axis=0)\nX_train[:, 1:] = (X_train[:, 1:] - mean) / std\nX_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\nX_test[:, 1:] = (X_test[:, 1:] - mean) / std\n\nNow let’s implement the Softmax function. Recall that it is defined by the following equation:\n\\(\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}\\)\n\ndef softmax(logits):\n    exps = np.exp(logits)\n    exp_sums = exps.sum(axis=1, keepdims=True)\n    return exps / exp_sums\n\nWe are almost ready to start training. Let’s define the number of inputs and outputs:\n\nn_inputs = X_train.shape[1]  # == 3 (2 features plus the bias term)\nn_outputs = len(np.unique(y_train))  # == 3 (there are 3 iris classes)\n\nNow here comes the hardest part: training! Theoretically, it’s simple: it’s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it’s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it’s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won’t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what’s going on under the hood.\nSo the equations we will need are the cost function:\n\\(J(\\mathbf{\\Theta}) = - \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}\\)\nAnd the equation for the gradients:\n\\(\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}\\)\nNote that \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) may not be computable if \\(\\hat{p}_k^{(i)} = 0\\). So we will add a tiny value \\(\\epsilon\\) to \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) to avoid getting nan values.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        print(epoch, xentropy_losses.sum(axis=1).mean())\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    Theta = Theta - eta * gradients\n\n0 3.7085808486476917\n1000 0.14519367480830647\n2000 0.13013095755040877\n3000 0.12009639326384532\n4000 0.11372961364786878\n5000 0.11002459532472424\n\n\nAnd that’s it! The Softmax model is trained. Let’s look at the model parameters:\n\nTheta\n\narray([[ 0.41931626,  6.11112089, -5.52429876],\n       [-6.53054533, -0.74608616,  8.33137102],\n       [-5.28115784,  0.25152675,  6.90680425]])\n\n\nLet’s make predictions for the validation set and check the accuracy score:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nWell, this model looks pretty ok. For the sake of the exercise, let’s add a bit of \\(\\ell_2\\) regularization. The following training code is similar to the one above, but the loss now has an additional \\(\\ell_2\\) penalty, and the gradients have the proper additional term (note that we don’t regularize the first element of Theta since this corresponds to the bias term). Also, let’s try increasing the learning rate eta.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\nalpha = 0.01  # regularization hyperparameter\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n        total_loss = xentropy_losses.sum(axis=1).mean() + alpha * l2_loss\n        print(epoch, total_loss.round(4))\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n1000 0.3259\n2000 0.3259\n3000 0.3259\n4000 0.3259\n5000 0.3259\n\n\nBecause of the additional \\(\\ell_2\\) penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let’s find out:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nIn this case, the \\(\\ell_2\\) penalty did not change the test accuracy. Perhaps try fine-tuning alpha?\nNow let’s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing.\n\neta = 0.5\nn_epochs = 50_001\nm = len(X_train)\nepsilon = 1e-5\nC = 100  # regularization hyperparameter\nbest_loss = np.infty\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    Y_proba_valid = softmax(X_valid @ Theta)\n    xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n    l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n    total_loss = xentropy_losses.sum(axis=1).mean() + 1 / C * l2_loss\n    if epoch % 1000 == 0:\n        print(epoch, total_loss.round(4))\n    if total_loss &lt; best_loss:\n        best_loss = total_loss\n    else:\n        print(epoch - 1, best_loss.round(4))\n        print(epoch, total_loss.round(4), \"early stopping!\")\n        break\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), 1 / C * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n281 0.3256\n282 0.3256 early stopping!\n\n\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nOh well, still no change in validation accuracy, but at least early stopping shortened training a bit.\nNow let’s plot the model’s predictions on the whole dataset (remember to scale all features fed to the model):\n\ncustom_cmap = mpl.colors.ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new = (X_new - mean) / std\nX_new_with_bias = np.c_[np.ones(len(X_new)), X_new]\n\nlogits = X_new_with_bias @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"upper left\")\nplt.axis([0, 7, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\nAnd now let’s measure the final model’s accuracy on the test set:\n\nlogits = X_test @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_test).mean()\naccuracy_score\n\n0.9666666666666667\n\n\nWell we get even better performance on the test set. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the tapestry of our daily experiences, uncertainty weaves its intricate threads, creating a landscape where the unknown is an ever-present companion. From deciding whether to carry an umbrella to predicting stock market trends, uncertainty surrounds us, shaping our decisions, actions, and perceptions. It is this pervasive uncertainty that beckons us to explore the realm of probability theory, a powerful tool that empowers us to navigate the unpredictable nature of the world.\nEnter probability theory, a mathematical framework designed to bring order to the chaos of uncertainty. Probability theory provides us with a systematic way to quantify and analyze uncertainty, offering a language to express the likelihood of different outcomes. It serves as a compass, guiding us through the fog of unpredictability and enabling us to make informed decisions in the face of ambiguity.\nAt its core, probability theory explores the likelihood of events occurring in various situations. It equips us with the means to assign numerical values to the uncertainty inherent in any scenario, allowing us to make reasoned predictions and choices. Whether predicting the outcome of a dice roll or estimating the probability of a rare disease occurrence, probability theory provides the analytical tools essential for decision-making in uncertain environments.\nIn the following exploration of probability theory and random variables, we will unravel the intricacies of this indispensable field, delving into its fundamental concepts, applications in real life, and the ways it shapes our understanding of uncertainty. Join us on this journey as we unveil the mathematical underpinnings that empower us to confront the unpredictable with confidence and insight."
  },
  {
    "objectID": "about.html#the-normal-equation",
    "href": "about.html#the-normal-equation",
    "title": "Probability Theory and Random Variables",
    "section": "The Normal Equation",
    "text": "The Normal Equation\n\nimport numpy as np\n\nnp.random.seed(42)  # to make this code example reproducible\nm = 100  # number of instances\nX = 2 * np.random.rand(m, 1)  # column vector\ny = 4 + 3 * X + np.random.randn(m, 1)  # column vector\n\n\n# extra code – generates and saves Figure 4–1\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"generated_data_plot\")\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import add_dummy_feature\n\nX_b = add_dummy_feature(X)  # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n\n\ntheta_best\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\nX_new = np.array([[0], [2]])\nX_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\ny_predict = X_new_b @ theta_best\ny_predict\n\narray([[4.21509616],\n       [9.75532293]])\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.plot(X, y, \"b.\")\n\n# extra code – beautifies and saves Figure 4–2\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.legend(loc=\"upper left\")\nsave_fig(\"linear_model_predictions_plot\")\n\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([4.21509616]), array([[2.77011339]]))\n\n\n\nlin_reg.predict(X_new)\n\narray([[4.21509616],\n       [9.75532293]])\n\n\nThe LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:\n\ntheta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\ntheta_best_svd\n\narray([[4.21509616],\n       [2.77011339]])\n\n\nThis function computes \\(\\mathbf{X}^+\\mathbf{y}\\), where \\(\\mathbf{X}^{+}\\) is the pseudoinverse of \\(\\mathbf{X}\\) (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly:\n\nnp.linalg.pinv(X_b) @ y\n\narray([[4.21509616],\n       [2.77011339]])"
  },
  {
    "objectID": "about.html#batch-gradient-descent",
    "href": "about.html#batch-gradient-descent",
    "title": "Probability Theory and Random Variables",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\n\neta = 0.1  # learning rate\nn_epochs = 1000\nm = len(X_b)  # number of instances\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # randomly initialized model parameters\n\nfor epoch in range(n_epochs):\n    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n    theta = theta - eta * gradients\n\nThe trained model parameters:\n\ntheta\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\n# extra code – generates and saves Figure 4–8\n\nimport matplotlib as mpl\n\ndef plot_gradient_descent(theta, eta):\n    m = len(X_b)\n    plt.plot(X, y, \"b.\")\n    n_epochs = 1000\n    n_shown = 20\n    theta_path = []\n    for epoch in range(n_epochs):\n        if epoch &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(epoch / n_shown + 0.15))\n            plt.plot(X_new, y_predict, linestyle=\"solid\", color=color)\n        gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n        theta = theta - eta * gradients\n        theta_path.append(theta)\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 2, 0, 15])\n    plt.grid()\n    plt.title(fr\"$\\eta = {eta}$\")\n    return theta_path\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nplt.figure(figsize=(10, 4))\nplt.subplot(131)\nplot_gradient_descent(theta, eta=0.02)\nplt.ylabel(\"$y$\", rotation=0)\nplt.subplot(132)\ntheta_path_bgd = plot_gradient_descent(theta, eta=0.1)\nplt.gca().axes.yaxis.set_ticklabels([])\nplt.subplot(133)\nplt.gca().axes.yaxis.set_ticklabels([])\nplot_gradient_descent(theta, eta=0.5)\nsave_fig(\"gradient_descent_plot\")\nplt.show()"
  },
  {
    "objectID": "about.html#stochastic-gradient-descent",
    "href": "about.html#stochastic-gradient-descent",
    "title": "Probability Theory and Random Variables",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\ntheta_path_sgd = []  # extra code – we need to store the path of theta in the\n                     #              parameter space to plot the next figure\n\n\nn_epochs = 50\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nn_shown = 20  # extra code – just needed to generate the figure below\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n\nfor epoch in range(n_epochs):\n    for iteration in range(m):\n\n        # extra code – these 4 lines are used to generate the figure\n        if epoch == 0 and iteration &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(iteration / n_shown + 0.15))\n            plt.plot(X_new, y_predict, color=color)\n\n        random_index = np.random.randint(m)\n        xi = X_b[random_index : random_index + 1]\n        yi = y[random_index : random_index + 1]\n        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n        eta = learning_schedule(epoch * m + iteration)\n        theta = theta - eta * gradients\n        theta_path_sgd.append(theta)  # extra code – to generate the figure\n\n# extra code – this section beautifies and saves Figure 4–10\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"sgd_plot\")\nplt.show()\n\n\n\n\n\ntheta\n\narray([[4.21076011],\n       [2.74856079]])\n\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n                       n_iter_no_change=100, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n\nSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)\n\n\n\nsgd_reg.intercept_, sgd_reg.coef_\n\n(array([4.21278812]), array([2.77270267]))"
  },
  {
    "objectID": "about.html#mini-batch-gradient-descent",
    "href": "about.html#mini-batch-gradient-descent",
    "title": "Probability Theory and Random Variables",
    "section": "Mini-batch gradient descent",
    "text": "Mini-batch gradient descent\nThe code in this section is used to generate the next figure, it is not in the book.\n\n# extra code – this cell generates and saves Figure 4–11\n\nfrom math import ceil\n\nn_epochs = 50\nminibatch_size = 20\nn_batches_per_epoch = ceil(m / minibatch_size)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nt0, t1 = 200, 1000  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\ntheta_path_mgd = []\nfor epoch in range(n_epochs):\n    shuffled_indices = np.random.permutation(m)\n    X_b_shuffled = X_b[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    for iteration in range(0, n_batches_per_epoch):\n        idx = iteration * minibatch_size\n        xi = X_b_shuffled[idx : idx + minibatch_size]\n        yi = y_shuffled[idx : idx + minibatch_size]\n        gradients = 2 / minibatch_size * xi.T @ (xi @ theta - yi)\n        eta = learning_schedule(iteration)\n        theta = theta - eta * gradients\n        theta_path_mgd.append(theta)\n\ntheta_path_bgd = np.array(theta_path_bgd)\ntheta_path_sgd = np.array(theta_path_sgd)\ntheta_path_mgd = np.array(theta_path_mgd)\n\nplt.figure(figsize=(7, 4))\nplt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1,\n         label=\"Stochastic\")\nplt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2,\n         label=\"Mini-batch\")\nplt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3,\n         label=\"Batch\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$   \", rotation=0)\nplt.axis([2.6, 4.6, 2.3, 3.4])\nplt.grid()\nsave_fig(\"gradient_descent_paths_plot\")\nplt.show()"
  },
  {
    "objectID": "about.html#ridge-regression",
    "href": "about.html#ridge-regression",
    "title": "Probability Theory and Random Variables",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nLet’s generate a very small and noisy linear dataset:\n\n# extra code – we've done this type of generation several times before\nnp.random.seed(42)\nm = 20\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\nX_new = np.linspace(0, 3, 100).reshape(100, 1)\n\n\n# extra code – a quick peek at the dataset we just generated\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \".\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$  \", rotation=0)\nplt.axis([0, 3, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55325833]])\n\n\n\n# extra code – this cell generates and saves Figure 4–17\n\ndef plot_model(model_class, polynomial, alphas, **model_kwargs):\n    plt.plot(X, y, \"b.\", linewidth=3)\n    for alpha, style in zip(alphas, (\"b:\", \"g--\", \"r-\")):\n        if alpha &gt; 0:\n            model = model_class(alpha, **model_kwargs)\n        else:\n            model = LinearRegression()\n        if polynomial:\n            model = make_pipeline(\n                PolynomialFeatures(degree=10, include_bias=False),\n                StandardScaler(),\n                model)\n        model.fit(X, y)\n        y_new_regul = model.predict(X_new)\n        plt.plot(X_new, y_new_regul, style, linewidth=2,\n                 label=fr\"$\\alpha = {alpha}$\")\n    plt.legend(loc=\"upper left\")\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 3, 0, 3.5])\n    plt.grid()\n\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"ridge_regression_plot\")\nplt.show()\n\n\n\n\n\nsgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n                       max_iter=1000, eta0=0.01, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\nsgd_reg.predict([[1.5]])\n\narray([1.55302613])\n\n\n\n# extra code – show that we get roughly the same solution as earlier when\n#              we use Stochastic Average GD (solver=\"sag\")\nridge_reg = Ridge(alpha=0.1, solver=\"sag\", random_state=42)\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55326019]])\n\n\n\n# extra code – shows the closed form solution of Ridge regression,\n#              compare with the next Ridge model's learned parameters below\nalpha = 0.1\nA = np.array([[0., 0.], [0., 1.]])\nX_b = np.c_[np.ones(m), X]\nnp.linalg.inv(X_b.T @ X_b + alpha * A) @ X_b.T @ y\n\narray([[0.97898394],\n       [0.3828496 ]])\n\n\n\nridge_reg.intercept_, ridge_reg.coef_  # extra code\n\n(array([0.97896386]), array([[0.38286422]]))"
  },
  {
    "objectID": "about.html#lasso-regression",
    "href": "about.html#lasso-regression",
    "title": "Probability Theory and Random Variables",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nfrom sklearn.linear_model import Lasso\n\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X, y)\nlasso_reg.predict([[1.5]])\n\narray([1.53788174])\n\n\n\n# extra code – this cell generates and saves Figure 4–18\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"lasso_regression_plot\")\nplt.show()\n\n\n\n\n\n# extra code – this BIG cell generates and saves Figure 4–19\n\nt1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n\nt1s = np.linspace(t1a, t1b, 500)\nt2s = np.linspace(t2a, t2b, 500)\nt1, t2 = np.meshgrid(t1s, t2s)\nT = np.c_[t1.ravel(), t2.ravel()]\nXr = np.array([[1, 1], [1, -1], [1, 0.5]])\nyr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n\nJ = (1 / len(Xr) * ((T @ Xr.T - yr.T) ** 2).sum(axis=1)).reshape(t1.shape)\n\nN1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\nN2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n\nt_min_idx = np.unravel_index(J.argmin(), J.shape)\nt1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n\nt_init = np.array([[0.25], [-1]])\n\ndef bgd_path(theta, X, y, l1, l2, core=1, eta=0.05, n_iterations=200):\n    path = [theta]\n    for iteration in range(n_iterations):\n        gradients = (core * 2 / len(X) * X.T @ (X @ theta - y)\n                     + l1 * np.sign(theta) + l2 * theta)\n        theta = theta - eta * gradients\n        path.append(theta)\n    return np.array(path)\n\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n\nfor i, N, l1, l2, title in ((0, N1, 2.0, 0, \"Lasso\"), (1, N2, 0, 2.0, \"Ridge\")):\n    JR = J + l1 * N1 + l2 * 0.5 * N2 ** 2\n\n    tr_min_idx = np.unravel_index(JR.argmin(), JR.shape)\n    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n\n    levels = np.exp(np.linspace(0, 1, 20)) - 1\n    levelsJ = levels * (J.max() - J.min()) + J.min()\n    levelsJR = levels * (JR.max() - JR.min()) + JR.min()\n    levelsN = np.linspace(0, N.max(), 10)\n\n    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n    path_N = bgd_path(theta=np.array([[2.0], [0.5]]), X=Xr, y=yr,\n                      l1=np.sign(l1) / 3, l2=np.sign(l2), core=0)\n    ax = axes[i, 0]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, N / 2.0, levels=levelsN)\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.set_title(fr\"$\\ell_{i + 1}$ penalty\")\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n    ax.set_ylabel(r\"$\\theta_2$\", rotation=0)\n\n    ax = axes[i, 1]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.plot(t1r_min, t2r_min, \"rs\")\n    ax.set_title(title)\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n\nsave_fig(\"lasso_vs_ridge_plot\")\nplt.show()"
  },
  {
    "objectID": "about.html#elastic-net",
    "href": "about.html#elastic-net",
    "title": "Probability Theory and Random Variables",
    "section": "Elastic Net",
    "text": "Elastic Net\n\nfrom sklearn.linear_model import ElasticNet\n\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic_net.fit(X, y)\nelastic_net.predict([[1.5]])\n\narray([1.54333232])"
  },
  {
    "objectID": "about.html#early-stopping",
    "href": "about.html#early-stopping",
    "title": "Probability Theory and Random Variables",
    "section": "Early Stopping",
    "text": "Early Stopping\nLet’s go back to the quadratic dataset we used earlier:\n\nfrom copy import deepcopy\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n# extra code – creates the same quadratic dataset as earlier and splits it\nnp.random.seed(42)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\nX_train, y_train = X[: m // 2], y[: m // 2, 0]\nX_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n                              StandardScaler())\nX_train_prep = preprocessing.fit_transform(X_train)\nX_valid_prep = preprocessing.transform(X_valid)\nsgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\nn_epochs = 500\nbest_valid_rmse = float('inf')\ntrain_errors, val_errors = [], []  # extra code – it's for the figure below\n\nfor epoch in range(n_epochs):\n    sgd_reg.partial_fit(X_train_prep, y_train)\n    y_valid_predict = sgd_reg.predict(X_valid_prep)\n    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n    if val_error &lt; best_valid_rmse:\n        best_valid_rmse = val_error\n        best_model = deepcopy(sgd_reg)\n\n    # extra code – we evaluate the train error and save it for the figure\n    y_train_predict = sgd_reg.predict(X_train_prep)\n    train_error = mean_squared_error(y_train, y_train_predict, squared=False)\n    val_errors.append(val_error)\n    train_errors.append(train_error)\n\n# extra code – this section generates and saves Figure 4–20\nbest_epoch = np.argmin(val_errors)\nplt.figure(figsize=(6, 4))\nplt.annotate('Best model',\n             xy=(best_epoch, best_valid_rmse),\n             xytext=(best_epoch, best_valid_rmse + 0.5),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], \"k:\", linewidth=2)\nplt.plot(val_errors, \"b-\", linewidth=3, label=\"Validation set\")\nplt.plot(best_epoch, best_valid_rmse, \"bo\")\nplt.plot(train_errors, \"r--\", linewidth=2, label=\"Training set\")\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"RMSE\")\nplt.axis([0, n_epochs, 0, 3.5])\nplt.grid()\nsave_fig(\"early_stopping_plot\")\nplt.show()"
  },
  {
    "objectID": "about.html#estimating-probabilities",
    "href": "about.html#estimating-probabilities",
    "title": "Probability Theory and Random Variables",
    "section": "Estimating Probabilities",
    "text": "Estimating Probabilities\n\n# extra code – generates and saves Figure 4–21\n\nlim = 6\nt = np.linspace(-lim, lim, 100)\nsig = 1 / (1 + np.exp(-t))\n\nplt.figure(figsize=(8, 3))\nplt.plot([-lim, lim], [0, 0], \"k-\")\nplt.plot([-lim, lim], [0.5, 0.5], \"k:\")\nplt.plot([-lim, lim], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\")\nplt.axis([-lim, lim, -0.1, 1.1])\nplt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\nplt.grid()\nsave_fig(\"logistic_function_plot\")\nplt.show()"
  },
  {
    "objectID": "about.html#decision-boundaries",
    "href": "about.html#decision-boundaries",
    "title": "Probability Theory and Random Variables",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\nlist(iris)\n\n['data',\n 'target',\n 'frame',\n 'target_names',\n 'DESCR',\n 'feature_names',\n 'filename',\n 'data_module']\n\n\n\nprint(iris.DESCR)  # extra code – it's a bit too long\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n|details-start|\n**References**\n|details-split|\n\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n  Mathematical Statistics\" (John Wiley, NY, 1950).\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n  Structure and Classification Rule for Recognition in Partially Exposed\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n  on Information Theory, May 1972, 431-433.\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n  conceptual clustering system finds 3 classes in the data.\n- Many, many more ...\n\n|details-end|\n\n\n\niris.data.head(3)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n\n\n\n\n\n\niris.target.head(3)  # note that the instances are not shuffled\n\n0    0\n1    0\n2    0\nName: target, dtype: int64\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\n\nLogisticRegression(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=42)\n\n\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\ny_proba = log_reg.predict_proba(X_new)\ndecision_boundary = X_new[y_proba[:, 1] &gt;= 0.5][0, 0]\n\nplt.figure(figsize=(8, 3))  # extra code – not needed, just formatting\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n         label=\"Not Iris virginica proba\")\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\nplt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n         label=\"Decision boundary\")\n\n# extra code – this section beautifies and saves Figure 4–23\nplt.arrow(x=decision_boundary, y=0.08, dx=-0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\nplt.arrow(x=decision_boundary, y=0.92, dx=0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\nplt.plot(X_train[y_train == 0], y_train[y_train == 0], \"bs\")\nplt.plot(X_train[y_train == 1], y_train[y_train == 1], \"g^\")\nplt.xlabel(\"Petal width (cm)\")\nplt.ylabel(\"Probability\")\nplt.legend(loc=\"center left\")\nplt.axis([0, 3, -0.02, 1.02])\nplt.grid()\nsave_fig(\"logistic_regression_plot\")\n\nplt.show()\n\n\n\n\n\ndecision_boundary\n\n1.6516516516516517\n\n\n\nlog_reg.predict([[1.7], [1.5]])\n\narray([ True, False])\n\n\n\n# extra code – this cell generates and saves Figure 4–24\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(C=2, random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# for the contour plot\nx0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1),\n                     np.linspace(0.8, 2.7, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]  # one instance per point on the figure\ny_proba = log_reg.predict_proba(X_new)\nzz = y_proba[:, 1].reshape(x0.shape)\n\n# for the decision boundary\nleft_right = np.array([2.9, 7])\nboundary = -((log_reg.coef_[0, 0] * left_right + log_reg.intercept_[0])\n             / log_reg.coef_[0, 1])\n\nplt.figure(figsize=(10, 4))\nplt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \"bs\")\nplt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \"g^\")\ncontour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1)\nplt.plot(left_right, boundary, \"k--\", linewidth=3)\nplt.text(3.5, 1.27, \"Not Iris virginica\", color=\"b\", ha=\"center\")\nplt.text(6.5, 2.3, \"Iris virginica\", color=\"g\", ha=\"center\")\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.axis([2.9, 7, 0.8, 2.7])\nplt.grid()\nsave_fig(\"logistic_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "about.html#softmax-regression",
    "href": "about.html#softmax-regression",
    "title": "Probability Theory and Random Variables",
    "section": "Softmax Regression",
    "text": "Softmax Regression\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax_reg = LogisticRegression(C=30, random_state=42)\nsoftmax_reg.fit(X_train, y_train)\n\nLogisticRegression(C=30, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=30, random_state=42)\n\n\n\nsoftmax_reg.predict([[5, 2]])\n\narray([2])\n\n\n\nsoftmax_reg.predict_proba([[5, 2]]).round(2)\n\narray([[0.  , 0.04, 0.96]])\n\n\n\n# extra code – this cell generates and saves Figure 4–25\n\nfrom matplotlib.colors import ListedColormap\n\ncustom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\n\ny_proba = softmax_reg.predict_proba(X_new)\ny_predict = softmax_reg.predict(X_new)\n\nzz1 = y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"center left\")\nplt.axis([0.5, 7, 0, 3.5])\nplt.grid()\nsave_fig(\"softmax_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "about.html#to-11.",
    "href": "about.html#to-11.",
    "title": "Probability Theory and Random Variables",
    "section": "1. to 11.",
    "text": "1. to 11.\n\nIf you have a training set with millions of features you can use Stochastic Gradient Descent or Mini-batch Gradient Descent, and perhaps Batch Gradient Descent if the training set fits in memory. But you cannot use the Normal Equation or the SVD approach because the computational complexity grows quickly (more than quadratically) with the number of features.\nIf the features in your training set have very different scales, the cost function will have the shape of an elongated bowl, so the Gradient Descent algorithms will take a long time to converge. To solve this you should scale the data before training the model. Note that the Normal Equation or SVD approach will work just fine without scaling. Moreover, regularized models may converge to a suboptimal solution if the features are not scaled: since regularization penalizes large weights, features with smaller values will tend to be ignored compared to features with larger values.\nGradient Descent cannot get stuck in a local minimum when training a Logistic Regression model because the cost function is convex. Convex means that if you draw a straight line between any two points on the curve, the line never crosses the curve.\nIf the optimization problem is convex (such as Linear Regression or Logistic Regression), and assuming the learning rate is not too high, then all Gradient Descent algorithms will approach the global optimum and end up producing fairly similar models. However, unless you gradually reduce the learning rate, Stochastic GD and Mini-batch GD will never truly converge; instead, they will keep jumping back and forth around the global optimum. This means that even if you let them run for a very long time, these Gradient Descent algorithms will produce slightly different models.\nIf the validation error consistently goes up after every epoch, then one possibility is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However, if the training error is not going up, then your model is overfitting the training set and you should stop training.\nDue to their random nature, neither Stochastic Gradient Descent nor Mini-batch Gradient Descent is guaranteed to make progress at every single training iteration. So if you immediately stop training when the validation error goes up, you may stop much too early, before the optimum is reached. A better option is to save the model at regular intervals; then, when it has not improved for a long time (meaning it will probably never beat the record), you can revert to the best saved model.\nStochastic Gradient Descent has the fastest training iteration since it considers only one training instance at a time, so it is generally the first to reach the vicinity of the global optimum (or Mini-batch GD with a very small mini-batch size). However, only Batch Gradient Descent will actually converge, given enough training time. As mentioned, Stochastic GD and Mini-batch GD will bounce around the optimum, unless you gradually reduce the learning rate.\nIf the validation error is much higher than the training error, this is likely because your model is overfitting the training set. One way to try to fix this is to reduce the polynomial degree: a model with fewer degrees of freedom is less likely to overfit. Another thing you can try is to regularize the model—for example, by adding an ℓ₂ penalty (Ridge) or an ℓ₁ penalty (Lasso) to the cost function. This will also reduce the degrees of freedom of the model. Lastly, you can try to increase the size of the training set.\nIf both the training error and the validation error are almost equal and fairly high, the model is likely underfitting the training set, which means it has a high bias. You should try reducing the regularization hyperparameter α.\nLet’s see:\n\n\nA model with some regularization typically performs better than a model without any regularization, so you should generally prefer Ridge Regression over plain Linear Regression.\nLasso Regression uses an ℓ₁ penalty, which tends to push the weights down to exactly zero. This leads to sparse models, where all weights are zero except for the most important weights. This is a way to perform feature selection automatically, which is good if you suspect that only a few features actually matter. When you are not sure, you should prefer Ridge Regression.\nElastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training instances). However, it does add an extra hyperparameter to tune. If you want Lasso without the erratic behavior, you can just use Elastic Net with an l1_ratio close to 1.\n\n\nIf you want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not exclusive classes (i.e., all four combinations are possible) you should train two Logistic Regression classifiers."
  },
  {
    "objectID": "about.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "href": "about.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "title": "Probability Theory and Random Variables",
    "section": "12. Batch Gradient Descent with early stopping for Softmax Regression",
    "text": "12. Batch Gradient Descent with early stopping for Softmax Regression\nExercise: Implement Batch Gradient Descent with early stopping for Softmax Regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset.\nLet’s start by loading the data. We will just reuse the Iris dataset we loaded earlier.\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"].values\n\nWe need to add the bias term for every instance (\\(x_0 = 1\\)). The easiest option to do this would be to use Scikit-Learn’s add_dummy_feature() function, but the point of this exercise is to get a better understanding of the algorithms by implementing them manually. So here is one possible implementation:\n\nX_with_bias = np.c_[np.ones(len(X)), X]\n\nThe easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn’s train_test_split() function, but again, we want to do it manually:\n\ntest_ratio = 0.2\nvalidation_ratio = 0.2\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nnp.random.seed(42)\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\nThe targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for any given instance is a one-hot vector). Let’s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance. To understand this code, you need to know that np.diag(np.ones(n)) creates an n×n matrix full of 0s except for 1s on the main diagonal. Moreover, if a is a NumPy array, then a[[1, 3, 2]] returns an array with 3 rows equal to a[1], a[3] and a[2] (this is advanced NumPy indexing).\n\ndef to_one_hot(y):\n    return np.diag(np.ones(y.max() + 1))[y]\n\nLet’s test this function on the first 10 instances:\n\ny_train[:10]\n\narray([1, 0, 2, 1, 1, 0, 1, 2, 1, 1])\n\n\n\nto_one_hot(y_train[:10])\n\narray([[0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.]])\n\n\nLooks good, so let’s create the target class probabilities matrix for the training set and the test set:\n\nY_train_one_hot = to_one_hot(y_train)\nY_valid_one_hot = to_one_hot(y_valid)\nY_test_one_hot = to_one_hot(y_test)\n\nNow let’s scale the inputs. We compute the mean and standard deviation of each feature on the training set (except for the bias feature), then we center and scale each feature in the training set, the validation set, and the test set:\n\nmean = X_train[:, 1:].mean(axis=0)\nstd = X_train[:, 1:].std(axis=0)\nX_train[:, 1:] = (X_train[:, 1:] - mean) / std\nX_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\nX_test[:, 1:] = (X_test[:, 1:] - mean) / std\n\nNow let’s implement the Softmax function. Recall that it is defined by the following equation:\n\\(\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}\\)\n\ndef softmax(logits):\n    exps = np.exp(logits)\n    exp_sums = exps.sum(axis=1, keepdims=True)\n    return exps / exp_sums\n\nWe are almost ready to start training. Let’s define the number of inputs and outputs:\n\nn_inputs = X_train.shape[1]  # == 3 (2 features plus the bias term)\nn_outputs = len(np.unique(y_train))  # == 3 (there are 3 iris classes)\n\nNow here comes the hardest part: training! Theoretically, it’s simple: it’s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it’s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it’s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won’t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what’s going on under the hood.\nSo the equations we will need are the cost function:\n\\(J(\\mathbf{\\Theta}) = - \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}\\)\nAnd the equation for the gradients:\n\\(\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}\\)\nNote that \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) may not be computable if \\(\\hat{p}_k^{(i)} = 0\\). So we will add a tiny value \\(\\epsilon\\) to \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) to avoid getting nan values.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        print(epoch, xentropy_losses.sum(axis=1).mean())\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    Theta = Theta - eta * gradients\n\n0 3.7085808486476917\n1000 0.14519367480830647\n2000 0.13013095755040877\n3000 0.12009639326384532\n4000 0.11372961364786878\n5000 0.11002459532472424\n\n\nAnd that’s it! The Softmax model is trained. Let’s look at the model parameters:\n\nTheta\n\narray([[ 0.41931626,  6.11112089, -5.52429876],\n       [-6.53054533, -0.74608616,  8.33137102],\n       [-5.28115784,  0.25152675,  6.90680425]])\n\n\nLet’s make predictions for the validation set and check the accuracy score:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nWell, this model looks pretty ok. For the sake of the exercise, let’s add a bit of \\(\\ell_2\\) regularization. The following training code is similar to the one above, but the loss now has an additional \\(\\ell_2\\) penalty, and the gradients have the proper additional term (note that we don’t regularize the first element of Theta since this corresponds to the bias term). Also, let’s try increasing the learning rate eta.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\nalpha = 0.01  # regularization hyperparameter\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n        total_loss = xentropy_losses.sum(axis=1).mean() + alpha * l2_loss\n        print(epoch, total_loss.round(4))\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n1000 0.3259\n2000 0.3259\n3000 0.3259\n4000 0.3259\n5000 0.3259\n\n\nBecause of the additional \\(\\ell_2\\) penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let’s find out:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nIn this case, the \\(\\ell_2\\) penalty did not change the test accuracy. Perhaps try fine-tuning alpha?\nNow let’s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing.\n\neta = 0.5\nn_epochs = 50_001\nm = len(X_train)\nepsilon = 1e-5\nC = 100  # regularization hyperparameter\nbest_loss = np.infty\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    Y_proba_valid = softmax(X_valid @ Theta)\n    xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n    l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n    total_loss = xentropy_losses.sum(axis=1).mean() + 1 / C * l2_loss\n    if epoch % 1000 == 0:\n        print(epoch, total_loss.round(4))\n    if total_loss &lt; best_loss:\n        best_loss = total_loss\n    else:\n        print(epoch - 1, best_loss.round(4))\n        print(epoch, total_loss.round(4), \"early stopping!\")\n        break\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), 1 / C * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n281 0.3256\n282 0.3256 early stopping!\n\n\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nOh well, still no change in validation accuracy, but at least early stopping shortened training a bit.\nNow let’s plot the model’s predictions on the whole dataset (remember to scale all features fed to the model):\n\ncustom_cmap = mpl.colors.ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new = (X_new - mean) / std\nX_new_with_bias = np.c_[np.ones(len(X_new)), X_new]\n\nlogits = X_new_with_bias @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"upper left\")\nplt.axis([0, 7, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\nAnd now let’s measure the final model’s accuracy on the test set:\n\nlogits = X_test @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_test).mean()\naccuracy_score\n\n0.9666666666666667\n\n\nWell we get even better performance on the test set. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary."
  },
  {
    "objectID": "blog3.html",
    "href": "blog3.html",
    "title": "Linear and Non-Linear Regression",
    "section": "",
    "text": "Regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. The goal is to understand and quantify the impact of the independent variables on the dependent variable. Regression analysis is widely employed in various fields, including economics, finance, biology, and machine learning, to make predictions, infer relationships, and understand patterns within data. The most fundamental form of regression is linear regression, where the relationship between variables is assumed to be linear. However, when the relationship is more complex and cannot be adequately captured by a straight line, non-linear regression models are employed.\nLinear regression assumes a linear relationship between the independent and dependent variables. The model equation is represented as a linear combination of the independent variables and an ε which is the error term. The coefficients represent the slope or impact of each independent variable on the dependent variable. Linear regression is straightforward, interpretable, and computationally efficient, making it a commonly used method. However, it may not capture complex, non-linear relationships effectively.\nNon-linear regression allows for more flexibility in modeling relationships that are not linear. The model equation is more complex and may involve non-linear functions, such as exponentials, logarithms, polynomials, or trigonometric functions. This flexibility enables non-linear regression to better represent curved or intricate patterns in the data. Non-linear regression models are particularly useful when the relationship between variables is better described by a curve, wave, or other non-linear shapes. While non-linear regression introduces more complexity, it requires careful consideration of model selection, and the interpretation of parameters may not be as intuitive as in linear regression. Various techniques, such as gradient descent or optimization algorithms, are employed to estimate the parameters of non-linear models from data."
  },
  {
    "objectID": "blog3.html#the-normal-equation",
    "href": "blog3.html#the-normal-equation",
    "title": "Linear and Non-Linear Regression",
    "section": "The Normal Equation",
    "text": "The Normal Equation\n\nimport numpy as np\n\nnp.random.seed(42)  # to make this code example reproducible\nm = 100  # number of instances\nX = 2 * np.random.rand(m, 1)  # column vector\ny = 4 + 3 * X + np.random.randn(m, 1)  # column vector\n\n\n# extra code – generates and saves Figure 4–1\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"generated_data_plot\")\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import add_dummy_feature\n\nX_b = add_dummy_feature(X)  # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n\n\ntheta_best\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\nX_new = np.array([[0], [2]])\nX_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\ny_predict = X_new_b @ theta_best\ny_predict\n\narray([[4.21509616],\n       [9.75532293]])\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.plot(X, y, \"b.\")\n\n# extra code – beautifies and saves Figure 4–2\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.legend(loc=\"upper left\")\nsave_fig(\"linear_model_predictions_plot\")\n\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([4.21509616]), array([[2.77011339]]))\n\n\n\nlin_reg.predict(X_new)\n\narray([[4.21509616],\n       [9.75532293]])\n\n\nThe LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:\n\ntheta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\ntheta_best_svd\n\narray([[4.21509616],\n       [2.77011339]])\n\n\nThis function computes \\(\\mathbf{X}^+\\mathbf{y}\\), where \\(\\mathbf{X}^{+}\\) is the pseudoinverse of \\(\\mathbf{X}\\) (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly:\n\nnp.linalg.pinv(X_b) @ y\n\narray([[4.21509616],\n       [2.77011339]])"
  },
  {
    "objectID": "blog3.html#batch-gradient-descent",
    "href": "blog3.html#batch-gradient-descent",
    "title": "Linear and Non-Linear Regression",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\n\neta = 0.1  # learning rate\nn_epochs = 1000\nm = len(X_b)  # number of instances\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # randomly initialized model parameters\n\nfor epoch in range(n_epochs):\n    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n    theta = theta - eta * gradients\n\nThe trained model parameters:\n\ntheta\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\n# extra code – generates and saves Figure 4–8\n\nimport matplotlib as mpl\n\ndef plot_gradient_descent(theta, eta):\n    m = len(X_b)\n    plt.plot(X, y, \"b.\")\n    n_epochs = 1000\n    n_shown = 20\n    theta_path = []\n    for epoch in range(n_epochs):\n        if epoch &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(epoch / n_shown + 0.15))\n            plt.plot(X_new, y_predict, linestyle=\"solid\", color=color)\n        gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n        theta = theta - eta * gradients\n        theta_path.append(theta)\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 2, 0, 15])\n    plt.grid()\n    plt.title(fr\"$\\eta = {eta}$\")\n    return theta_path\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nplt.figure(figsize=(10, 4))\nplt.subplot(131)\nplot_gradient_descent(theta, eta=0.02)\nplt.ylabel(\"$y$\", rotation=0)\nplt.subplot(132)\ntheta_path_bgd = plot_gradient_descent(theta, eta=0.1)\nplt.gca().axes.yaxis.set_ticklabels([])\nplt.subplot(133)\nplt.gca().axes.yaxis.set_ticklabels([])\nplot_gradient_descent(theta, eta=0.5)\nsave_fig(\"gradient_descent_plot\")\nplt.show()"
  },
  {
    "objectID": "blog3.html#stochastic-gradient-descent",
    "href": "blog3.html#stochastic-gradient-descent",
    "title": "Linear and Non-Linear Regression",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\ntheta_path_sgd = []  # extra code – we need to store the path of theta in the\n                     #              parameter space to plot the next figure\n\n\nn_epochs = 50\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nn_shown = 20  # extra code – just needed to generate the figure below\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n\nfor epoch in range(n_epochs):\n    for iteration in range(m):\n\n        # extra code – these 4 lines are used to generate the figure\n        if epoch == 0 and iteration &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(iteration / n_shown + 0.15))\n            plt.plot(X_new, y_predict, color=color)\n\n        random_index = np.random.randint(m)\n        xi = X_b[random_index : random_index + 1]\n        yi = y[random_index : random_index + 1]\n        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n        eta = learning_schedule(epoch * m + iteration)\n        theta = theta - eta * gradients\n        theta_path_sgd.append(theta)  # extra code – to generate the figure\n\n# extra code – this section beautifies and saves Figure 4–10\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"sgd_plot\")\nplt.show()\n\n\n\n\n\ntheta\n\narray([[4.21076011],\n       [2.74856079]])\n\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n                       n_iter_no_change=100, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n\nSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)\n\n\n\nsgd_reg.intercept_, sgd_reg.coef_\n\n(array([4.21278812]), array([2.77270267]))"
  },
  {
    "objectID": "blog3.html#mini-batch-gradient-descent",
    "href": "blog3.html#mini-batch-gradient-descent",
    "title": "Linear and Non-Linear Regression",
    "section": "Mini-batch gradient descent",
    "text": "Mini-batch gradient descent\nThe code in this section is used to generate the next figure, it is not in the book.\n\n# extra code – this cell generates and saves Figure 4–11\n\nfrom math import ceil\n\nn_epochs = 50\nminibatch_size = 20\nn_batches_per_epoch = ceil(m / minibatch_size)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nt0, t1 = 200, 1000  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\ntheta_path_mgd = []\nfor epoch in range(n_epochs):\n    shuffled_indices = np.random.permutation(m)\n    X_b_shuffled = X_b[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    for iteration in range(0, n_batches_per_epoch):\n        idx = iteration * minibatch_size\n        xi = X_b_shuffled[idx : idx + minibatch_size]\n        yi = y_shuffled[idx : idx + minibatch_size]\n        gradients = 2 / minibatch_size * xi.T @ (xi @ theta - yi)\n        eta = learning_schedule(iteration)\n        theta = theta - eta * gradients\n        theta_path_mgd.append(theta)\n\ntheta_path_bgd = np.array(theta_path_bgd)\ntheta_path_sgd = np.array(theta_path_sgd)\ntheta_path_mgd = np.array(theta_path_mgd)\n\nplt.figure(figsize=(7, 4))\nplt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1,\n         label=\"Stochastic\")\nplt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2,\n         label=\"Mini-batch\")\nplt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3,\n         label=\"Batch\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$   \", rotation=0)\nplt.axis([2.6, 4.6, 2.3, 3.4])\nplt.grid()\nsave_fig(\"gradient_descent_paths_plot\")\nplt.show()"
  },
  {
    "objectID": "blog3.html#ridge-regression",
    "href": "blog3.html#ridge-regression",
    "title": "Linear and Non-Linear Regression",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nLet’s generate a very small and noisy linear dataset:\n\n# extra code – we've done this type of generation several times before\nnp.random.seed(42)\nm = 20\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\nX_new = np.linspace(0, 3, 100).reshape(100, 1)\n\n\n# extra code – a quick peek at the dataset we just generated\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \".\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$  \", rotation=0)\nplt.axis([0, 3, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55325833]])\n\n\n\n# extra code – this cell generates and saves Figure 4–17\n\ndef plot_model(model_class, polynomial, alphas, **model_kwargs):\n    plt.plot(X, y, \"b.\", linewidth=3)\n    for alpha, style in zip(alphas, (\"b:\", \"g--\", \"r-\")):\n        if alpha &gt; 0:\n            model = model_class(alpha, **model_kwargs)\n        else:\n            model = LinearRegression()\n        if polynomial:\n            model = make_pipeline(\n                PolynomialFeatures(degree=10, include_bias=False),\n                StandardScaler(),\n                model)\n        model.fit(X, y)\n        y_new_regul = model.predict(X_new)\n        plt.plot(X_new, y_new_regul, style, linewidth=2,\n                 label=fr\"$\\alpha = {alpha}$\")\n    plt.legend(loc=\"upper left\")\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 3, 0, 3.5])\n    plt.grid()\n\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"ridge_regression_plot\")\nplt.show()\n\n\n\n\n\nsgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n                       max_iter=1000, eta0=0.01, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\nsgd_reg.predict([[1.5]])\n\narray([1.55302613])\n\n\n\n# extra code – show that we get roughly the same solution as earlier when\n#              we use Stochastic Average GD (solver=\"sag\")\nridge_reg = Ridge(alpha=0.1, solver=\"sag\", random_state=42)\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55326019]])\n\n\n\n# extra code – shows the closed form solution of Ridge regression,\n#              compare with the next Ridge model's learned parameters below\nalpha = 0.1\nA = np.array([[0., 0.], [0., 1.]])\nX_b = np.c_[np.ones(m), X]\nnp.linalg.inv(X_b.T @ X_b + alpha * A) @ X_b.T @ y\n\narray([[0.97898394],\n       [0.3828496 ]])\n\n\n\nridge_reg.intercept_, ridge_reg.coef_  # extra code\n\n(array([0.97896386]), array([[0.38286422]]))"
  },
  {
    "objectID": "blog3.html#lasso-regression",
    "href": "blog3.html#lasso-regression",
    "title": "Linear and Non-Linear Regression",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nfrom sklearn.linear_model import Lasso\n\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X, y)\nlasso_reg.predict([[1.5]])\n\narray([1.53788174])\n\n\n\n# extra code – this cell generates and saves Figure 4–18\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"lasso_regression_plot\")\nplt.show()\n\n\n\n\n\n# extra code – this BIG cell generates and saves Figure 4–19\n\nt1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n\nt1s = np.linspace(t1a, t1b, 500)\nt2s = np.linspace(t2a, t2b, 500)\nt1, t2 = np.meshgrid(t1s, t2s)\nT = np.c_[t1.ravel(), t2.ravel()]\nXr = np.array([[1, 1], [1, -1], [1, 0.5]])\nyr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n\nJ = (1 / len(Xr) * ((T @ Xr.T - yr.T) ** 2).sum(axis=1)).reshape(t1.shape)\n\nN1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\nN2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n\nt_min_idx = np.unravel_index(J.argmin(), J.shape)\nt1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n\nt_init = np.array([[0.25], [-1]])\n\ndef bgd_path(theta, X, y, l1, l2, core=1, eta=0.05, n_iterations=200):\n    path = [theta]\n    for iteration in range(n_iterations):\n        gradients = (core * 2 / len(X) * X.T @ (X @ theta - y)\n                     + l1 * np.sign(theta) + l2 * theta)\n        theta = theta - eta * gradients\n        path.append(theta)\n    return np.array(path)\n\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n\nfor i, N, l1, l2, title in ((0, N1, 2.0, 0, \"Lasso\"), (1, N2, 0, 2.0, \"Ridge\")):\n    JR = J + l1 * N1 + l2 * 0.5 * N2 ** 2\n\n    tr_min_idx = np.unravel_index(JR.argmin(), JR.shape)\n    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n\n    levels = np.exp(np.linspace(0, 1, 20)) - 1\n    levelsJ = levels * (J.max() - J.min()) + J.min()\n    levelsJR = levels * (JR.max() - JR.min()) + JR.min()\n    levelsN = np.linspace(0, N.max(), 10)\n\n    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n    path_N = bgd_path(theta=np.array([[2.0], [0.5]]), X=Xr, y=yr,\n                      l1=np.sign(l1) / 3, l2=np.sign(l2), core=0)\n    ax = axes[i, 0]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, N / 2.0, levels=levelsN)\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.set_title(fr\"$\\ell_{i + 1}$ penalty\")\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n    ax.set_ylabel(r\"$\\theta_2$\", rotation=0)\n\n    ax = axes[i, 1]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.plot(t1r_min, t2r_min, \"rs\")\n    ax.set_title(title)\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n\nsave_fig(\"lasso_vs_ridge_plot\")\nplt.show()"
  },
  {
    "objectID": "blog3.html#elastic-net",
    "href": "blog3.html#elastic-net",
    "title": "Linear and Non-Linear Regression",
    "section": "Elastic Net",
    "text": "Elastic Net\n\nfrom sklearn.linear_model import ElasticNet\n\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic_net.fit(X, y)\nelastic_net.predict([[1.5]])\n\narray([1.54333232])"
  },
  {
    "objectID": "blog3.html#early-stopping",
    "href": "blog3.html#early-stopping",
    "title": "Linear and Non-Linear Regression",
    "section": "Early Stopping",
    "text": "Early Stopping\nLet’s go back to the quadratic dataset we used earlier:\n\nfrom copy import deepcopy\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n# extra code – creates the same quadratic dataset as earlier and splits it\nnp.random.seed(42)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\nX_train, y_train = X[: m // 2], y[: m // 2, 0]\nX_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n                              StandardScaler())\nX_train_prep = preprocessing.fit_transform(X_train)\nX_valid_prep = preprocessing.transform(X_valid)\nsgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\nn_epochs = 500\nbest_valid_rmse = float('inf')\ntrain_errors, val_errors = [], []  # extra code – it's for the figure below\n\nfor epoch in range(n_epochs):\n    sgd_reg.partial_fit(X_train_prep, y_train)\n    y_valid_predict = sgd_reg.predict(X_valid_prep)\n    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n    if val_error &lt; best_valid_rmse:\n        best_valid_rmse = val_error\n        best_model = deepcopy(sgd_reg)\n\n    # extra code – we evaluate the train error and save it for the figure\n    y_train_predict = sgd_reg.predict(X_train_prep)\n    train_error = mean_squared_error(y_train, y_train_predict, squared=False)\n    val_errors.append(val_error)\n    train_errors.append(train_error)\n\n# extra code – this section generates and saves Figure 4–20\nbest_epoch = np.argmin(val_errors)\nplt.figure(figsize=(6, 4))\nplt.annotate('Best model',\n             xy=(best_epoch, best_valid_rmse),\n             xytext=(best_epoch, best_valid_rmse + 0.5),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], \"k:\", linewidth=2)\nplt.plot(val_errors, \"b-\", linewidth=3, label=\"Validation set\")\nplt.plot(best_epoch, best_valid_rmse, \"bo\")\nplt.plot(train_errors, \"r--\", linewidth=2, label=\"Training set\")\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"RMSE\")\nplt.axis([0, n_epochs, 0, 3.5])\nplt.grid()\nsave_fig(\"early_stopping_plot\")\nplt.show()"
  },
  {
    "objectID": "blog3.html#estimating-probabilities",
    "href": "blog3.html#estimating-probabilities",
    "title": "Linear and Non-Linear Regression",
    "section": "Estimating Probabilities",
    "text": "Estimating Probabilities\n\n# extra code – generates and saves Figure 4–21\n\nlim = 6\nt = np.linspace(-lim, lim, 100)\nsig = 1 / (1 + np.exp(-t))\n\nplt.figure(figsize=(8, 3))\nplt.plot([-lim, lim], [0, 0], \"k-\")\nplt.plot([-lim, lim], [0.5, 0.5], \"k:\")\nplt.plot([-lim, lim], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\")\nplt.axis([-lim, lim, -0.1, 1.1])\nplt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\nplt.grid()\nsave_fig(\"logistic_function_plot\")\nplt.show()"
  },
  {
    "objectID": "blog3.html#decision-boundaries",
    "href": "blog3.html#decision-boundaries",
    "title": "Linear and Non-Linear Regression",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\nlist(iris)\n\n['data',\n 'target',\n 'frame',\n 'target_names',\n 'DESCR',\n 'feature_names',\n 'filename',\n 'data_module']\n\n\n\nprint(iris.DESCR)  # extra code – it's a bit too long\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n|details-start|\n**References**\n|details-split|\n\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n  Mathematical Statistics\" (John Wiley, NY, 1950).\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n  Structure and Classification Rule for Recognition in Partially Exposed\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n  on Information Theory, May 1972, 431-433.\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n  conceptual clustering system finds 3 classes in the data.\n- Many, many more ...\n\n|details-end|\n\n\n\niris.data.head(3)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n\n\n\n\n\n\niris.target.head(3)  # note that the instances are not shuffled\n\n0    0\n1    0\n2    0\nName: target, dtype: int64\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\n\nLogisticRegression(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=42)\n\n\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\ny_proba = log_reg.predict_proba(X_new)\ndecision_boundary = X_new[y_proba[:, 1] &gt;= 0.5][0, 0]\n\nplt.figure(figsize=(8, 3))  # extra code – not needed, just formatting\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n         label=\"Not Iris virginica proba\")\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\nplt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n         label=\"Decision boundary\")\n\n# extra code – this section beautifies and saves Figure 4–23\nplt.arrow(x=decision_boundary, y=0.08, dx=-0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\nplt.arrow(x=decision_boundary, y=0.92, dx=0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\nplt.plot(X_train[y_train == 0], y_train[y_train == 0], \"bs\")\nplt.plot(X_train[y_train == 1], y_train[y_train == 1], \"g^\")\nplt.xlabel(\"Petal width (cm)\")\nplt.ylabel(\"Probability\")\nplt.legend(loc=\"center left\")\nplt.axis([0, 3, -0.02, 1.02])\nplt.grid()\nsave_fig(\"logistic_regression_plot\")\n\nplt.show()\n\n\n\n\n\ndecision_boundary\n\n1.6516516516516517\n\n\n\nlog_reg.predict([[1.7], [1.5]])\n\narray([ True, False])\n\n\n\n# extra code – this cell generates and saves Figure 4–24\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(C=2, random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# for the contour plot\nx0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1),\n                     np.linspace(0.8, 2.7, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]  # one instance per point on the figure\ny_proba = log_reg.predict_proba(X_new)\nzz = y_proba[:, 1].reshape(x0.shape)\n\n# for the decision boundary\nleft_right = np.array([2.9, 7])\nboundary = -((log_reg.coef_[0, 0] * left_right + log_reg.intercept_[0])\n             / log_reg.coef_[0, 1])\n\nplt.figure(figsize=(10, 4))\nplt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \"bs\")\nplt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \"g^\")\ncontour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1)\nplt.plot(left_right, boundary, \"k--\", linewidth=3)\nplt.text(3.5, 1.27, \"Not Iris virginica\", color=\"b\", ha=\"center\")\nplt.text(6.5, 2.3, \"Iris virginica\", color=\"g\", ha=\"center\")\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.axis([2.9, 7, 0.8, 2.7])\nplt.grid()\nsave_fig(\"logistic_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "blog3.html#softmax-regression",
    "href": "blog3.html#softmax-regression",
    "title": "Linear and Non-Linear Regression",
    "section": "Softmax Regression",
    "text": "Softmax Regression\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax_reg = LogisticRegression(C=30, random_state=42)\nsoftmax_reg.fit(X_train, y_train)\n\nLogisticRegression(C=30, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=30, random_state=42)\n\n\n\nsoftmax_reg.predict([[5, 2]])\n\narray([2])\n\n\n\nsoftmax_reg.predict_proba([[5, 2]]).round(2)\n\narray([[0.  , 0.04, 0.96]])\n\n\n\n# extra code – this cell generates and saves Figure 4–25\n\nfrom matplotlib.colors import ListedColormap\n\ncustom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\n\ny_proba = softmax_reg.predict_proba(X_new)\ny_predict = softmax_reg.predict(X_new)\n\nzz1 = y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"center left\")\nplt.axis([0.5, 7, 0, 3.5])\nplt.grid()\nsave_fig(\"softmax_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "blog3.html#to-11.",
    "href": "blog3.html#to-11.",
    "title": "Linear and Non-Linear Regression",
    "section": "1. to 11.",
    "text": "1. to 11.\n\nIf you have a training set with millions of features you can use Stochastic Gradient Descent or Mini-batch Gradient Descent, and perhaps Batch Gradient Descent if the training set fits in memory. But you cannot use the Normal Equation or the SVD approach because the computational complexity grows quickly (more than quadratically) with the number of features.\nIf the features in your training set have very different scales, the cost function will have the shape of an elongated bowl, so the Gradient Descent algorithms will take a long time to converge. To solve this you should scale the data before training the model. Note that the Normal Equation or SVD approach will work just fine without scaling. Moreover, regularized models may converge to a suboptimal solution if the features are not scaled: since regularization penalizes large weights, features with smaller values will tend to be ignored compared to features with larger values.\nGradient Descent cannot get stuck in a local minimum when training a Logistic Regression model because the cost function is convex. Convex means that if you draw a straight line between any two points on the curve, the line never crosses the curve.\nIf the optimization problem is convex (such as Linear Regression or Logistic Regression), and assuming the learning rate is not too high, then all Gradient Descent algorithms will approach the global optimum and end up producing fairly similar models. However, unless you gradually reduce the learning rate, Stochastic GD and Mini-batch GD will never truly converge; instead, they will keep jumping back and forth around the global optimum. This means that even if you let them run for a very long time, these Gradient Descent algorithms will produce slightly different models.\nIf the validation error consistently goes up after every epoch, then one possibility is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However, if the training error is not going up, then your model is overfitting the training set and you should stop training.\nDue to their random nature, neither Stochastic Gradient Descent nor Mini-batch Gradient Descent is guaranteed to make progress at every single training iteration. So if you immediately stop training when the validation error goes up, you may stop much too early, before the optimum is reached. A better option is to save the model at regular intervals; then, when it has not improved for a long time (meaning it will probably never beat the record), you can revert to the best saved model.\nStochastic Gradient Descent has the fastest training iteration since it considers only one training instance at a time, so it is generally the first to reach the vicinity of the global optimum (or Mini-batch GD with a very small mini-batch size). However, only Batch Gradient Descent will actually converge, given enough training time. As mentioned, Stochastic GD and Mini-batch GD will bounce around the optimum, unless you gradually reduce the learning rate.\nIf the validation error is much higher than the training error, this is likely because your model is overfitting the training set. One way to try to fix this is to reduce the polynomial degree: a model with fewer degrees of freedom is less likely to overfit. Another thing you can try is to regularize the model—for example, by adding an ℓ₂ penalty (Ridge) or an ℓ₁ penalty (Lasso) to the cost function. This will also reduce the degrees of freedom of the model. Lastly, you can try to increase the size of the training set.\nIf both the training error and the validation error are almost equal and fairly high, the model is likely underfitting the training set, which means it has a high bias. You should try reducing the regularization hyperparameter α.\nLet’s see:\n\n\nA model with some regularization typically performs better than a model without any regularization, so you should generally prefer Ridge Regression over plain Linear Regression.\nLasso Regression uses an ℓ₁ penalty, which tends to push the weights down to exactly zero. This leads to sparse models, where all weights are zero except for the most important weights. This is a way to perform feature selection automatically, which is good if you suspect that only a few features actually matter. When you are not sure, you should prefer Ridge Regression.\nElastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training instances). However, it does add an extra hyperparameter to tune. If you want Lasso without the erratic behavior, you can just use Elastic Net with an l1_ratio close to 1.\n\n\nIf you want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not exclusive classes (i.e., all four combinations are possible) you should train two Logistic Regression classifiers."
  },
  {
    "objectID": "blog3.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "href": "blog3.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "title": "Linear and Non-Linear Regression",
    "section": "12. Batch Gradient Descent with early stopping for Softmax Regression",
    "text": "12. Batch Gradient Descent with early stopping for Softmax Regression\nExercise: Implement Batch Gradient Descent with early stopping for Softmax Regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset.\nLet’s start by loading the data. We will just reuse the Iris dataset we loaded earlier.\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"].values\n\nWe need to add the bias term for every instance (\\(x_0 = 1\\)). The easiest option to do this would be to use Scikit-Learn’s add_dummy_feature() function, but the point of this exercise is to get a better understanding of the algorithms by implementing them manually. So here is one possible implementation:\n\nX_with_bias = np.c_[np.ones(len(X)), X]\n\nThe easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn’s train_test_split() function, but again, we want to do it manually:\n\ntest_ratio = 0.2\nvalidation_ratio = 0.2\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nnp.random.seed(42)\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\nThe targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for any given instance is a one-hot vector). Let’s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance. To understand this code, you need to know that np.diag(np.ones(n)) creates an n×n matrix full of 0s except for 1s on the main diagonal. Moreover, if a is a NumPy array, then a[[1, 3, 2]] returns an array with 3 rows equal to a[1], a[3] and a[2] (this is advanced NumPy indexing).\n\ndef to_one_hot(y):\n    return np.diag(np.ones(y.max() + 1))[y]\n\nLet’s test this function on the first 10 instances:\n\ny_train[:10]\n\narray([1, 0, 2, 1, 1, 0, 1, 2, 1, 1])\n\n\n\nto_one_hot(y_train[:10])\n\narray([[0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.]])\n\n\nLooks good, so let’s create the target class probabilities matrix for the training set and the test set:\n\nY_train_one_hot = to_one_hot(y_train)\nY_valid_one_hot = to_one_hot(y_valid)\nY_test_one_hot = to_one_hot(y_test)\n\nNow let’s scale the inputs. We compute the mean and standard deviation of each feature on the training set (except for the bias feature), then we center and scale each feature in the training set, the validation set, and the test set:\n\nmean = X_train[:, 1:].mean(axis=0)\nstd = X_train[:, 1:].std(axis=0)\nX_train[:, 1:] = (X_train[:, 1:] - mean) / std\nX_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\nX_test[:, 1:] = (X_test[:, 1:] - mean) / std\n\nNow let’s implement the Softmax function. Recall that it is defined by the following equation:\n\\(\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}\\)\n\ndef softmax(logits):\n    exps = np.exp(logits)\n    exp_sums = exps.sum(axis=1, keepdims=True)\n    return exps / exp_sums\n\nWe are almost ready to start training. Let’s define the number of inputs and outputs:\n\nn_inputs = X_train.shape[1]  # == 3 (2 features plus the bias term)\nn_outputs = len(np.unique(y_train))  # == 3 (there are 3 iris classes)\n\nNow here comes the hardest part: training! Theoretically, it’s simple: it’s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it’s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it’s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won’t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what’s going on under the hood.\nSo the equations we will need are the cost function:\n\\(J(\\mathbf{\\Theta}) = - \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}\\)\nAnd the equation for the gradients:\n\\(\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}\\)\nNote that \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) may not be computable if \\(\\hat{p}_k^{(i)} = 0\\). So we will add a tiny value \\(\\epsilon\\) to \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) to avoid getting nan values.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        print(epoch, xentropy_losses.sum(axis=1).mean())\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    Theta = Theta - eta * gradients\n\n0 3.7085808486476917\n1000 0.14519367480830647\n2000 0.13013095755040877\n3000 0.12009639326384532\n4000 0.11372961364786878\n5000 0.11002459532472424\n\n\nAnd that’s it! The Softmax model is trained. Let’s look at the model parameters:\n\nTheta\n\narray([[ 0.41931626,  6.11112089, -5.52429876],\n       [-6.53054533, -0.74608616,  8.33137102],\n       [-5.28115784,  0.25152675,  6.90680425]])\n\n\nLet’s make predictions for the validation set and check the accuracy score:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nWell, this model looks pretty ok. For the sake of the exercise, let’s add a bit of \\(\\ell_2\\) regularization. The following training code is similar to the one above, but the loss now has an additional \\(\\ell_2\\) penalty, and the gradients have the proper additional term (note that we don’t regularize the first element of Theta since this corresponds to the bias term). Also, let’s try increasing the learning rate eta.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\nalpha = 0.01  # regularization hyperparameter\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n        total_loss = xentropy_losses.sum(axis=1).mean() + alpha * l2_loss\n        print(epoch, total_loss.round(4))\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n1000 0.3259\n2000 0.3259\n3000 0.3259\n4000 0.3259\n5000 0.3259\n\n\nBecause of the additional \\(\\ell_2\\) penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let’s find out:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nIn this case, the \\(\\ell_2\\) penalty did not change the test accuracy. Perhaps try fine-tuning alpha?\nNow let’s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing.\n\neta = 0.5\nn_epochs = 50_001\nm = len(X_train)\nepsilon = 1e-5\nC = 100  # regularization hyperparameter\nbest_loss = np.infty\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    Y_proba_valid = softmax(X_valid @ Theta)\n    xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n    l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n    total_loss = xentropy_losses.sum(axis=1).mean() + 1 / C * l2_loss\n    if epoch % 1000 == 0:\n        print(epoch, total_loss.round(4))\n    if total_loss &lt; best_loss:\n        best_loss = total_loss\n    else:\n        print(epoch - 1, best_loss.round(4))\n        print(epoch, total_loss.round(4), \"early stopping!\")\n        break\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), 1 / C * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n281 0.3256\n282 0.3256 early stopping!\n\n\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nOh well, still no change in validation accuracy, but at least early stopping shortened training a bit.\nNow let’s plot the model’s predictions on the whole dataset (remember to scale all features fed to the model):\n\ncustom_cmap = mpl.colors.ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new = (X_new - mean) / std\nX_new_with_bias = np.c_[np.ones(len(X_new)), X_new]\n\nlogits = X_new_with_bias @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"upper left\")\nplt.axis([0, 7, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\nAnd now let’s measure the final model’s accuracy on the test set:\n\nlogits = X_test @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_test).mean()\naccuracy_score\n\n0.9666666666666667\n\n\nWell we get even better performance on the test set. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary."
  },
  {
    "objectID": "blog4.html",
    "href": "blog4.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is a fundamental concept in machine learning that involves assigning predefined labels to input data points based on their features. There are several types of classification algorithms, each suited to different scenarios. Multi-label Classification allows instances to be assigned to multiple classes simultaneously. For instance, a news article might belong to categories like “Politics,” “Science,” and “Technology” simultaneously. The mathematical formulation involves extending binary classification concepts to multiple classes.\nBinary Classification is the simplest form, where the task involves distinguishing between two classes. For instance, predicting whether an email is spam or not. The mathematical formulation often includes a decision boundary, dividing the feature space into regions associated with each class.\nMulticlass Classification extends the concept to scenarios with more than two classes. A classic example is handwritten digit recognition, where the goal is to classify digits from 0 to 9. Mathematical formulations usually involve multiple decision boundaries, each separating one class from the rest.\nMulti-label Classification allows instances to be assigned to multiple classes simultaneously. For instance, a news article might belong to categories like “Politics,” “Science,” and “Technology” simultaneously. The mathematical formulation involves extending binary classification concepts to multiple classes."
  },
  {
    "objectID": "blog4.html#the-normal-equation",
    "href": "blog4.html#the-normal-equation",
    "title": "Classification",
    "section": "The Normal Equation",
    "text": "The Normal Equation\n\nimport numpy as np\n\nnp.random.seed(42)  # to make this code example reproducible\nm = 100  # number of instances\nX = 2 * np.random.rand(m, 1)  # column vector\ny = 4 + 3 * X + np.random.randn(m, 1)  # column vector\n\n\n# extra code – generates and saves Figure 4–1\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"generated_data_plot\")\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import add_dummy_feature\n\nX_b = add_dummy_feature(X)  # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n\n\ntheta_best\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\nX_new = np.array([[0], [2]])\nX_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\ny_predict = X_new_b @ theta_best\ny_predict\n\narray([[4.21509616],\n       [9.75532293]])\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.plot(X, y, \"b.\")\n\n# extra code – beautifies and saves Figure 4–2\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.legend(loc=\"upper left\")\nsave_fig(\"linear_model_predictions_plot\")\n\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([4.21509616]), array([[2.77011339]]))\n\n\n\nlin_reg.predict(X_new)\n\narray([[4.21509616],\n       [9.75532293]])\n\n\nThe LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:\n\ntheta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\ntheta_best_svd\n\narray([[4.21509616],\n       [2.77011339]])\n\n\nThis function computes \\(\\mathbf{X}^+\\mathbf{y}\\), where \\(\\mathbf{X}^{+}\\) is the pseudoinverse of \\(\\mathbf{X}\\) (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly:\n\nnp.linalg.pinv(X_b) @ y\n\narray([[4.21509616],\n       [2.77011339]])"
  },
  {
    "objectID": "blog4.html#batch-gradient-descent",
    "href": "blog4.html#batch-gradient-descent",
    "title": "Classification",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\n\neta = 0.1  # learning rate\nn_epochs = 1000\nm = len(X_b)  # number of instances\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # randomly initialized model parameters\n\nfor epoch in range(n_epochs):\n    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n    theta = theta - eta * gradients\n\nThe trained model parameters:\n\ntheta\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\n# extra code – generates and saves Figure 4–8\n\nimport matplotlib as mpl\n\ndef plot_gradient_descent(theta, eta):\n    m = len(X_b)\n    plt.plot(X, y, \"b.\")\n    n_epochs = 1000\n    n_shown = 20\n    theta_path = []\n    for epoch in range(n_epochs):\n        if epoch &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(epoch / n_shown + 0.15))\n            plt.plot(X_new, y_predict, linestyle=\"solid\", color=color)\n        gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n        theta = theta - eta * gradients\n        theta_path.append(theta)\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 2, 0, 15])\n    plt.grid()\n    plt.title(fr\"$\\eta = {eta}$\")\n    return theta_path\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nplt.figure(figsize=(10, 4))\nplt.subplot(131)\nplot_gradient_descent(theta, eta=0.02)\nplt.ylabel(\"$y$\", rotation=0)\nplt.subplot(132)\ntheta_path_bgd = plot_gradient_descent(theta, eta=0.1)\nplt.gca().axes.yaxis.set_ticklabels([])\nplt.subplot(133)\nplt.gca().axes.yaxis.set_ticklabels([])\nplot_gradient_descent(theta, eta=0.5)\nsave_fig(\"gradient_descent_plot\")\nplt.show()"
  },
  {
    "objectID": "blog4.html#stochastic-gradient-descent",
    "href": "blog4.html#stochastic-gradient-descent",
    "title": "Classification",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\ntheta_path_sgd = []  # extra code – we need to store the path of theta in the\n                     #              parameter space to plot the next figure\n\n\nn_epochs = 50\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nn_shown = 20  # extra code – just needed to generate the figure below\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n\nfor epoch in range(n_epochs):\n    for iteration in range(m):\n\n        # extra code – these 4 lines are used to generate the figure\n        if epoch == 0 and iteration &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(iteration / n_shown + 0.15))\n            plt.plot(X_new, y_predict, color=color)\n\n        random_index = np.random.randint(m)\n        xi = X_b[random_index : random_index + 1]\n        yi = y[random_index : random_index + 1]\n        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n        eta = learning_schedule(epoch * m + iteration)\n        theta = theta - eta * gradients\n        theta_path_sgd.append(theta)  # extra code – to generate the figure\n\n# extra code – this section beautifies and saves Figure 4–10\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"sgd_plot\")\nplt.show()\n\n\n\n\n\ntheta\n\narray([[4.21076011],\n       [2.74856079]])\n\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n                       n_iter_no_change=100, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n\nSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)\n\n\n\nsgd_reg.intercept_, sgd_reg.coef_\n\n(array([4.21278812]), array([2.77270267]))"
  },
  {
    "objectID": "blog4.html#mini-batch-gradient-descent",
    "href": "blog4.html#mini-batch-gradient-descent",
    "title": "Classification",
    "section": "Mini-batch gradient descent",
    "text": "Mini-batch gradient descent\nThe code in this section is used to generate the next figure, it is not in the book.\n\n# extra code – this cell generates and saves Figure 4–11\n\nfrom math import ceil\n\nn_epochs = 50\nminibatch_size = 20\nn_batches_per_epoch = ceil(m / minibatch_size)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nt0, t1 = 200, 1000  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\ntheta_path_mgd = []\nfor epoch in range(n_epochs):\n    shuffled_indices = np.random.permutation(m)\n    X_b_shuffled = X_b[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    for iteration in range(0, n_batches_per_epoch):\n        idx = iteration * minibatch_size\n        xi = X_b_shuffled[idx : idx + minibatch_size]\n        yi = y_shuffled[idx : idx + minibatch_size]\n        gradients = 2 / minibatch_size * xi.T @ (xi @ theta - yi)\n        eta = learning_schedule(iteration)\n        theta = theta - eta * gradients\n        theta_path_mgd.append(theta)\n\ntheta_path_bgd = np.array(theta_path_bgd)\ntheta_path_sgd = np.array(theta_path_sgd)\ntheta_path_mgd = np.array(theta_path_mgd)\n\nplt.figure(figsize=(7, 4))\nplt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1,\n         label=\"Stochastic\")\nplt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2,\n         label=\"Mini-batch\")\nplt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3,\n         label=\"Batch\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$   \", rotation=0)\nplt.axis([2.6, 4.6, 2.3, 3.4])\nplt.grid()\nsave_fig(\"gradient_descent_paths_plot\")\nplt.show()"
  },
  {
    "objectID": "blog4.html#ridge-regression",
    "href": "blog4.html#ridge-regression",
    "title": "Classification",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nLet’s generate a very small and noisy linear dataset:\n\n# extra code – we've done this type of generation several times before\nnp.random.seed(42)\nm = 20\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\nX_new = np.linspace(0, 3, 100).reshape(100, 1)\n\n\n# extra code – a quick peek at the dataset we just generated\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \".\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$  \", rotation=0)\nplt.axis([0, 3, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55325833]])\n\n\n\n# extra code – this cell generates and saves Figure 4–17\n\ndef plot_model(model_class, polynomial, alphas, **model_kwargs):\n    plt.plot(X, y, \"b.\", linewidth=3)\n    for alpha, style in zip(alphas, (\"b:\", \"g--\", \"r-\")):\n        if alpha &gt; 0:\n            model = model_class(alpha, **model_kwargs)\n        else:\n            model = LinearRegression()\n        if polynomial:\n            model = make_pipeline(\n                PolynomialFeatures(degree=10, include_bias=False),\n                StandardScaler(),\n                model)\n        model.fit(X, y)\n        y_new_regul = model.predict(X_new)\n        plt.plot(X_new, y_new_regul, style, linewidth=2,\n                 label=fr\"$\\alpha = {alpha}$\")\n    plt.legend(loc=\"upper left\")\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 3, 0, 3.5])\n    plt.grid()\n\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"ridge_regression_plot\")\nplt.show()\n\n\n\n\n\nsgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n                       max_iter=1000, eta0=0.01, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\nsgd_reg.predict([[1.5]])\n\narray([1.55302613])\n\n\n\n# extra code – show that we get roughly the same solution as earlier when\n#              we use Stochastic Average GD (solver=\"sag\")\nridge_reg = Ridge(alpha=0.1, solver=\"sag\", random_state=42)\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55326019]])\n\n\n\n# extra code – shows the closed form solution of Ridge regression,\n#              compare with the next Ridge model's learned parameters below\nalpha = 0.1\nA = np.array([[0., 0.], [0., 1.]])\nX_b = np.c_[np.ones(m), X]\nnp.linalg.inv(X_b.T @ X_b + alpha * A) @ X_b.T @ y\n\narray([[0.97898394],\n       [0.3828496 ]])\n\n\n\nridge_reg.intercept_, ridge_reg.coef_  # extra code\n\n(array([0.97896386]), array([[0.38286422]]))"
  },
  {
    "objectID": "blog4.html#lasso-regression",
    "href": "blog4.html#lasso-regression",
    "title": "Classification",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nfrom sklearn.linear_model import Lasso\n\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X, y)\nlasso_reg.predict([[1.5]])\n\narray([1.53788174])\n\n\n\n# extra code – this cell generates and saves Figure 4–18\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"lasso_regression_plot\")\nplt.show()\n\n\n\n\n\n# extra code – this BIG cell generates and saves Figure 4–19\n\nt1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n\nt1s = np.linspace(t1a, t1b, 500)\nt2s = np.linspace(t2a, t2b, 500)\nt1, t2 = np.meshgrid(t1s, t2s)\nT = np.c_[t1.ravel(), t2.ravel()]\nXr = np.array([[1, 1], [1, -1], [1, 0.5]])\nyr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n\nJ = (1 / len(Xr) * ((T @ Xr.T - yr.T) ** 2).sum(axis=1)).reshape(t1.shape)\n\nN1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\nN2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n\nt_min_idx = np.unravel_index(J.argmin(), J.shape)\nt1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n\nt_init = np.array([[0.25], [-1]])\n\ndef bgd_path(theta, X, y, l1, l2, core=1, eta=0.05, n_iterations=200):\n    path = [theta]\n    for iteration in range(n_iterations):\n        gradients = (core * 2 / len(X) * X.T @ (X @ theta - y)\n                     + l1 * np.sign(theta) + l2 * theta)\n        theta = theta - eta * gradients\n        path.append(theta)\n    return np.array(path)\n\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n\nfor i, N, l1, l2, title in ((0, N1, 2.0, 0, \"Lasso\"), (1, N2, 0, 2.0, \"Ridge\")):\n    JR = J + l1 * N1 + l2 * 0.5 * N2 ** 2\n\n    tr_min_idx = np.unravel_index(JR.argmin(), JR.shape)\n    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n\n    levels = np.exp(np.linspace(0, 1, 20)) - 1\n    levelsJ = levels * (J.max() - J.min()) + J.min()\n    levelsJR = levels * (JR.max() - JR.min()) + JR.min()\n    levelsN = np.linspace(0, N.max(), 10)\n\n    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n    path_N = bgd_path(theta=np.array([[2.0], [0.5]]), X=Xr, y=yr,\n                      l1=np.sign(l1) / 3, l2=np.sign(l2), core=0)\n    ax = axes[i, 0]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, N / 2.0, levels=levelsN)\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.set_title(fr\"$\\ell_{i + 1}$ penalty\")\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n    ax.set_ylabel(r\"$\\theta_2$\", rotation=0)\n\n    ax = axes[i, 1]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.plot(t1r_min, t2r_min, \"rs\")\n    ax.set_title(title)\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n\nsave_fig(\"lasso_vs_ridge_plot\")\nplt.show()"
  },
  {
    "objectID": "blog4.html#elastic-net",
    "href": "blog4.html#elastic-net",
    "title": "Classification",
    "section": "Elastic Net",
    "text": "Elastic Net\n\nfrom sklearn.linear_model import ElasticNet\n\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic_net.fit(X, y)\nelastic_net.predict([[1.5]])\n\narray([1.54333232])"
  },
  {
    "objectID": "blog4.html#early-stopping",
    "href": "blog4.html#early-stopping",
    "title": "Classification",
    "section": "Early Stopping",
    "text": "Early Stopping\nLet’s go back to the quadratic dataset we used earlier:\n\nfrom copy import deepcopy\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n# extra code – creates the same quadratic dataset as earlier and splits it\nnp.random.seed(42)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\nX_train, y_train = X[: m // 2], y[: m // 2, 0]\nX_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n                              StandardScaler())\nX_train_prep = preprocessing.fit_transform(X_train)\nX_valid_prep = preprocessing.transform(X_valid)\nsgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\nn_epochs = 500\nbest_valid_rmse = float('inf')\ntrain_errors, val_errors = [], []  # extra code – it's for the figure below\n\nfor epoch in range(n_epochs):\n    sgd_reg.partial_fit(X_train_prep, y_train)\n    y_valid_predict = sgd_reg.predict(X_valid_prep)\n    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n    if val_error &lt; best_valid_rmse:\n        best_valid_rmse = val_error\n        best_model = deepcopy(sgd_reg)\n\n    # extra code – we evaluate the train error and save it for the figure\n    y_train_predict = sgd_reg.predict(X_train_prep)\n    train_error = mean_squared_error(y_train, y_train_predict, squared=False)\n    val_errors.append(val_error)\n    train_errors.append(train_error)\n\n# extra code – this section generates and saves Figure 4–20\nbest_epoch = np.argmin(val_errors)\nplt.figure(figsize=(6, 4))\nplt.annotate('Best model',\n             xy=(best_epoch, best_valid_rmse),\n             xytext=(best_epoch, best_valid_rmse + 0.5),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], \"k:\", linewidth=2)\nplt.plot(val_errors, \"b-\", linewidth=3, label=\"Validation set\")\nplt.plot(best_epoch, best_valid_rmse, \"bo\")\nplt.plot(train_errors, \"r--\", linewidth=2, label=\"Training set\")\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"RMSE\")\nplt.axis([0, n_epochs, 0, 3.5])\nplt.grid()\nsave_fig(\"early_stopping_plot\")\nplt.show()"
  },
  {
    "objectID": "blog4.html#estimating-probabilities",
    "href": "blog4.html#estimating-probabilities",
    "title": "Classification",
    "section": "Estimating Probabilities",
    "text": "Estimating Probabilities\n\n# extra code – generates and saves Figure 4–21\n\nlim = 6\nt = np.linspace(-lim, lim, 100)\nsig = 1 / (1 + np.exp(-t))\n\nplt.figure(figsize=(8, 3))\nplt.plot([-lim, lim], [0, 0], \"k-\")\nplt.plot([-lim, lim], [0.5, 0.5], \"k:\")\nplt.plot([-lim, lim], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\")\nplt.axis([-lim, lim, -0.1, 1.1])\nplt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\nplt.grid()\nsave_fig(\"logistic_function_plot\")\nplt.show()"
  },
  {
    "objectID": "blog4.html#decision-boundaries",
    "href": "blog4.html#decision-boundaries",
    "title": "Classification",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\nlist(iris)\n\n['data',\n 'target',\n 'frame',\n 'target_names',\n 'DESCR',\n 'feature_names',\n 'filename',\n 'data_module']\n\n\n\nprint(iris.DESCR)  # extra code – it's a bit too long\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n|details-start|\n**References**\n|details-split|\n\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n  Mathematical Statistics\" (John Wiley, NY, 1950).\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n  Structure and Classification Rule for Recognition in Partially Exposed\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n  on Information Theory, May 1972, 431-433.\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n  conceptual clustering system finds 3 classes in the data.\n- Many, many more ...\n\n|details-end|\n\n\n\niris.data.head(3)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n\n\n\n\n\n\niris.target.head(3)  # note that the instances are not shuffled\n\n0    0\n1    0\n2    0\nName: target, dtype: int64\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\n\nLogisticRegression(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=42)\n\n\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\ny_proba = log_reg.predict_proba(X_new)\ndecision_boundary = X_new[y_proba[:, 1] &gt;= 0.5][0, 0]\n\nplt.figure(figsize=(8, 3))  # extra code – not needed, just formatting\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n         label=\"Not Iris virginica proba\")\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\nplt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n         label=\"Decision boundary\")\n\n# extra code – this section beautifies and saves Figure 4–23\nplt.arrow(x=decision_boundary, y=0.08, dx=-0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\nplt.arrow(x=decision_boundary, y=0.92, dx=0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\nplt.plot(X_train[y_train == 0], y_train[y_train == 0], \"bs\")\nplt.plot(X_train[y_train == 1], y_train[y_train == 1], \"g^\")\nplt.xlabel(\"Petal width (cm)\")\nplt.ylabel(\"Probability\")\nplt.legend(loc=\"center left\")\nplt.axis([0, 3, -0.02, 1.02])\nplt.grid()\nsave_fig(\"logistic_regression_plot\")\n\nplt.show()\n\n\n\n\n\ndecision_boundary\n\n1.6516516516516517\n\n\n\nlog_reg.predict([[1.7], [1.5]])\n\narray([ True, False])\n\n\n\n# extra code – this cell generates and saves Figure 4–24\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(C=2, random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# for the contour plot\nx0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1),\n                     np.linspace(0.8, 2.7, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]  # one instance per point on the figure\ny_proba = log_reg.predict_proba(X_new)\nzz = y_proba[:, 1].reshape(x0.shape)\n\n# for the decision boundary\nleft_right = np.array([2.9, 7])\nboundary = -((log_reg.coef_[0, 0] * left_right + log_reg.intercept_[0])\n             / log_reg.coef_[0, 1])\n\nplt.figure(figsize=(10, 4))\nplt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \"bs\")\nplt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \"g^\")\ncontour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1)\nplt.plot(left_right, boundary, \"k--\", linewidth=3)\nplt.text(3.5, 1.27, \"Not Iris virginica\", color=\"b\", ha=\"center\")\nplt.text(6.5, 2.3, \"Iris virginica\", color=\"g\", ha=\"center\")\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.axis([2.9, 7, 0.8, 2.7])\nplt.grid()\nsave_fig(\"logistic_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "blog4.html#softmax-regression",
    "href": "blog4.html#softmax-regression",
    "title": "Classification",
    "section": "Softmax Regression",
    "text": "Softmax Regression\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax_reg = LogisticRegression(C=30, random_state=42)\nsoftmax_reg.fit(X_train, y_train)\n\nLogisticRegression(C=30, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=30, random_state=42)\n\n\n\nsoftmax_reg.predict([[5, 2]])\n\narray([2])\n\n\n\nsoftmax_reg.predict_proba([[5, 2]]).round(2)\n\narray([[0.  , 0.04, 0.96]])\n\n\n\n# extra code – this cell generates and saves Figure 4–25\n\nfrom matplotlib.colors import ListedColormap\n\ncustom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\n\ny_proba = softmax_reg.predict_proba(X_new)\ny_predict = softmax_reg.predict(X_new)\n\nzz1 = y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"center left\")\nplt.axis([0.5, 7, 0, 3.5])\nplt.grid()\nsave_fig(\"softmax_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "blog4.html#to-11.",
    "href": "blog4.html#to-11.",
    "title": "Classification",
    "section": "1. to 11.",
    "text": "1. to 11.\n\nIf you have a training set with millions of features you can use Stochastic Gradient Descent or Mini-batch Gradient Descent, and perhaps Batch Gradient Descent if the training set fits in memory. But you cannot use the Normal Equation or the SVD approach because the computational complexity grows quickly (more than quadratically) with the number of features.\nIf the features in your training set have very different scales, the cost function will have the shape of an elongated bowl, so the Gradient Descent algorithms will take a long time to converge. To solve this you should scale the data before training the model. Note that the Normal Equation or SVD approach will work just fine without scaling. Moreover, regularized models may converge to a suboptimal solution if the features are not scaled: since regularization penalizes large weights, features with smaller values will tend to be ignored compared to features with larger values.\nGradient Descent cannot get stuck in a local minimum when training a Logistic Regression model because the cost function is convex. Convex means that if you draw a straight line between any two points on the curve, the line never crosses the curve.\nIf the optimization problem is convex (such as Linear Regression or Logistic Regression), and assuming the learning rate is not too high, then all Gradient Descent algorithms will approach the global optimum and end up producing fairly similar models. However, unless you gradually reduce the learning rate, Stochastic GD and Mini-batch GD will never truly converge; instead, they will keep jumping back and forth around the global optimum. This means that even if you let them run for a very long time, these Gradient Descent algorithms will produce slightly different models.\nIf the validation error consistently goes up after every epoch, then one possibility is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However, if the training error is not going up, then your model is overfitting the training set and you should stop training.\nDue to their random nature, neither Stochastic Gradient Descent nor Mini-batch Gradient Descent is guaranteed to make progress at every single training iteration. So if you immediately stop training when the validation error goes up, you may stop much too early, before the optimum is reached. A better option is to save the model at regular intervals; then, when it has not improved for a long time (meaning it will probably never beat the record), you can revert to the best saved model.\nStochastic Gradient Descent has the fastest training iteration since it considers only one training instance at a time, so it is generally the first to reach the vicinity of the global optimum (or Mini-batch GD with a very small mini-batch size). However, only Batch Gradient Descent will actually converge, given enough training time. As mentioned, Stochastic GD and Mini-batch GD will bounce around the optimum, unless you gradually reduce the learning rate.\nIf the validation error is much higher than the training error, this is likely because your model is overfitting the training set. One way to try to fix this is to reduce the polynomial degree: a model with fewer degrees of freedom is less likely to overfit. Another thing you can try is to regularize the model—for example, by adding an ℓ₂ penalty (Ridge) or an ℓ₁ penalty (Lasso) to the cost function. This will also reduce the degrees of freedom of the model. Lastly, you can try to increase the size of the training set.\nIf both the training error and the validation error are almost equal and fairly high, the model is likely underfitting the training set, which means it has a high bias. You should try reducing the regularization hyperparameter α.\nLet’s see:\n\n\nA model with some regularization typically performs better than a model without any regularization, so you should generally prefer Ridge Regression over plain Linear Regression.\nLasso Regression uses an ℓ₁ penalty, which tends to push the weights down to exactly zero. This leads to sparse models, where all weights are zero except for the most important weights. This is a way to perform feature selection automatically, which is good if you suspect that only a few features actually matter. When you are not sure, you should prefer Ridge Regression.\nElastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training instances). However, it does add an extra hyperparameter to tune. If you want Lasso without the erratic behavior, you can just use Elastic Net with an l1_ratio close to 1.\n\n\nIf you want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not exclusive classes (i.e., all four combinations are possible) you should train two Logistic Regression classifiers."
  },
  {
    "objectID": "blog4.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "href": "blog4.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "title": "Classification",
    "section": "12. Batch Gradient Descent with early stopping for Softmax Regression",
    "text": "12. Batch Gradient Descent with early stopping for Softmax Regression\nExercise: Implement Batch Gradient Descent with early stopping for Softmax Regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset.\nLet’s start by loading the data. We will just reuse the Iris dataset we loaded earlier.\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"].values\n\nWe need to add the bias term for every instance (\\(x_0 = 1\\)). The easiest option to do this would be to use Scikit-Learn’s add_dummy_feature() function, but the point of this exercise is to get a better understanding of the algorithms by implementing them manually. So here is one possible implementation:\n\nX_with_bias = np.c_[np.ones(len(X)), X]\n\nThe easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn’s train_test_split() function, but again, we want to do it manually:\n\ntest_ratio = 0.2\nvalidation_ratio = 0.2\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nnp.random.seed(42)\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\nThe targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for any given instance is a one-hot vector). Let’s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance. To understand this code, you need to know that np.diag(np.ones(n)) creates an n×n matrix full of 0s except for 1s on the main diagonal. Moreover, if a is a NumPy array, then a[[1, 3, 2]] returns an array with 3 rows equal to a[1], a[3] and a[2] (this is advanced NumPy indexing).\n\ndef to_one_hot(y):\n    return np.diag(np.ones(y.max() + 1))[y]\n\nLet’s test this function on the first 10 instances:\n\ny_train[:10]\n\narray([1, 0, 2, 1, 1, 0, 1, 2, 1, 1])\n\n\n\nto_one_hot(y_train[:10])\n\narray([[0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.]])\n\n\nLooks good, so let’s create the target class probabilities matrix for the training set and the test set:\n\nY_train_one_hot = to_one_hot(y_train)\nY_valid_one_hot = to_one_hot(y_valid)\nY_test_one_hot = to_one_hot(y_test)\n\nNow let’s scale the inputs. We compute the mean and standard deviation of each feature on the training set (except for the bias feature), then we center and scale each feature in the training set, the validation set, and the test set:\n\nmean = X_train[:, 1:].mean(axis=0)\nstd = X_train[:, 1:].std(axis=0)\nX_train[:, 1:] = (X_train[:, 1:] - mean) / std\nX_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\nX_test[:, 1:] = (X_test[:, 1:] - mean) / std\n\nNow let’s implement the Softmax function. Recall that it is defined by the following equation:\n\\(\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}\\)\n\ndef softmax(logits):\n    exps = np.exp(logits)\n    exp_sums = exps.sum(axis=1, keepdims=True)\n    return exps / exp_sums\n\nWe are almost ready to start training. Let’s define the number of inputs and outputs:\n\nn_inputs = X_train.shape[1]  # == 3 (2 features plus the bias term)\nn_outputs = len(np.unique(y_train))  # == 3 (there are 3 iris classes)\n\nNow here comes the hardest part: training! Theoretically, it’s simple: it’s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it’s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it’s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won’t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what’s going on under the hood.\nSo the equations we will need are the cost function:\n\\(J(\\mathbf{\\Theta}) = - \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}\\)\nAnd the equation for the gradients:\n\\(\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}\\)\nNote that \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) may not be computable if \\(\\hat{p}_k^{(i)} = 0\\). So we will add a tiny value \\(\\epsilon\\) to \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) to avoid getting nan values.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        print(epoch, xentropy_losses.sum(axis=1).mean())\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    Theta = Theta - eta * gradients\n\n0 3.7085808486476917\n1000 0.14519367480830647\n2000 0.13013095755040877\n3000 0.12009639326384532\n4000 0.11372961364786878\n5000 0.11002459532472424\n\n\nAnd that’s it! The Softmax model is trained. Let’s look at the model parameters:\n\nTheta\n\narray([[ 0.41931626,  6.11112089, -5.52429876],\n       [-6.53054533, -0.74608616,  8.33137102],\n       [-5.28115784,  0.25152675,  6.90680425]])\n\n\nLet’s make predictions for the validation set and check the accuracy score:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nWell, this model looks pretty ok. For the sake of the exercise, let’s add a bit of \\(\\ell_2\\) regularization. The following training code is similar to the one above, but the loss now has an additional \\(\\ell_2\\) penalty, and the gradients have the proper additional term (note that we don’t regularize the first element of Theta since this corresponds to the bias term). Also, let’s try increasing the learning rate eta.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\nalpha = 0.01  # regularization hyperparameter\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n        total_loss = xentropy_losses.sum(axis=1).mean() + alpha * l2_loss\n        print(epoch, total_loss.round(4))\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n1000 0.3259\n2000 0.3259\n3000 0.3259\n4000 0.3259\n5000 0.3259\n\n\nBecause of the additional \\(\\ell_2\\) penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let’s find out:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nIn this case, the \\(\\ell_2\\) penalty did not change the test accuracy. Perhaps try fine-tuning alpha?\nNow let’s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing.\n\neta = 0.5\nn_epochs = 50_001\nm = len(X_train)\nepsilon = 1e-5\nC = 100  # regularization hyperparameter\nbest_loss = np.infty\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    Y_proba_valid = softmax(X_valid @ Theta)\n    xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n    l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n    total_loss = xentropy_losses.sum(axis=1).mean() + 1 / C * l2_loss\n    if epoch % 1000 == 0:\n        print(epoch, total_loss.round(4))\n    if total_loss &lt; best_loss:\n        best_loss = total_loss\n    else:\n        print(epoch - 1, best_loss.round(4))\n        print(epoch, total_loss.round(4), \"early stopping!\")\n        break\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), 1 / C * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n281 0.3256\n282 0.3256 early stopping!\n\n\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nOh well, still no change in validation accuracy, but at least early stopping shortened training a bit.\nNow let’s plot the model’s predictions on the whole dataset (remember to scale all features fed to the model):\n\ncustom_cmap = mpl.colors.ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new = (X_new - mean) / std\nX_new_with_bias = np.c_[np.ones(len(X_new)), X_new]\n\nlogits = X_new_with_bias @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"upper left\")\nplt.axis([0, 7, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\nAnd now let’s measure the final model’s accuracy on the test set:\n\nlogits = X_test @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_test).mean()\naccuracy_score\n\n0.9666666666666667\n\n\nWell we get even better performance on the test set. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary."
  },
  {
    "objectID": "blog5.html",
    "href": "blog5.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly detection, also known as outlier detection, is a machine learning technique focused on identifying patterns or instances in a dataset that deviate significantly from the majority of the data. These anomalies can represent rare events, errors, or abnormal behavior that could be indicative of problems or interesting insights. Anomaly detection plays a crucial role in various domains, including fraud detection, network security, manufacturing quality control, and healthcare monitoring."
  },
  {
    "objectID": "blog5.html#the-normal-equation",
    "href": "blog5.html#the-normal-equation",
    "title": "Anomaly Detection",
    "section": "The Normal Equation",
    "text": "The Normal Equation\n\nimport numpy as np\n\nnp.random.seed(42)  # to make this code example reproducible\nm = 100  # number of instances\nX = 2 * np.random.rand(m, 1)  # column vector\ny = 4 + 3 * X + np.random.randn(m, 1)  # column vector\n\n\n# extra code – generates and saves Figure 4–1\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"generated_data_plot\")\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import add_dummy_feature\n\nX_b = add_dummy_feature(X)  # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n\n\ntheta_best\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\nX_new = np.array([[0], [2]])\nX_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\ny_predict = X_new_b @ theta_best\ny_predict\n\narray([[4.21509616],\n       [9.75532293]])\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.plot(X, y, \"b.\")\n\n# extra code – beautifies and saves Figure 4–2\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.legend(loc=\"upper left\")\nsave_fig(\"linear_model_predictions_plot\")\n\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([4.21509616]), array([[2.77011339]]))\n\n\n\nlin_reg.predict(X_new)\n\narray([[4.21509616],\n       [9.75532293]])\n\n\nThe LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:\n\ntheta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\ntheta_best_svd\n\narray([[4.21509616],\n       [2.77011339]])\n\n\nThis function computes \\(\\mathbf{X}^+\\mathbf{y}\\), where \\(\\mathbf{X}^{+}\\) is the pseudoinverse of \\(\\mathbf{X}\\) (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly:\n\nnp.linalg.pinv(X_b) @ y\n\narray([[4.21509616],\n       [2.77011339]])"
  },
  {
    "objectID": "blog5.html#batch-gradient-descent",
    "href": "blog5.html#batch-gradient-descent",
    "title": "Anomaly Detection",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\n\neta = 0.1  # learning rate\nn_epochs = 1000\nm = len(X_b)  # number of instances\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # randomly initialized model parameters\n\nfor epoch in range(n_epochs):\n    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n    theta = theta - eta * gradients\n\nThe trained model parameters:\n\ntheta\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\n# extra code – generates and saves Figure 4–8\n\nimport matplotlib as mpl\n\ndef plot_gradient_descent(theta, eta):\n    m = len(X_b)\n    plt.plot(X, y, \"b.\")\n    n_epochs = 1000\n    n_shown = 20\n    theta_path = []\n    for epoch in range(n_epochs):\n        if epoch &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(epoch / n_shown + 0.15))\n            plt.plot(X_new, y_predict, linestyle=\"solid\", color=color)\n        gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n        theta = theta - eta * gradients\n        theta_path.append(theta)\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 2, 0, 15])\n    plt.grid()\n    plt.title(fr\"$\\eta = {eta}$\")\n    return theta_path\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nplt.figure(figsize=(10, 4))\nplt.subplot(131)\nplot_gradient_descent(theta, eta=0.02)\nplt.ylabel(\"$y$\", rotation=0)\nplt.subplot(132)\ntheta_path_bgd = plot_gradient_descent(theta, eta=0.1)\nplt.gca().axes.yaxis.set_ticklabels([])\nplt.subplot(133)\nplt.gca().axes.yaxis.set_ticklabels([])\nplot_gradient_descent(theta, eta=0.5)\nsave_fig(\"gradient_descent_plot\")\nplt.show()"
  },
  {
    "objectID": "blog5.html#stochastic-gradient-descent",
    "href": "blog5.html#stochastic-gradient-descent",
    "title": "Anomaly Detection",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\ntheta_path_sgd = []  # extra code – we need to store the path of theta in the\n                     #              parameter space to plot the next figure\n\n\nn_epochs = 50\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nn_shown = 20  # extra code – just needed to generate the figure below\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n\nfor epoch in range(n_epochs):\n    for iteration in range(m):\n\n        # extra code – these 4 lines are used to generate the figure\n        if epoch == 0 and iteration &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(iteration / n_shown + 0.15))\n            plt.plot(X_new, y_predict, color=color)\n\n        random_index = np.random.randint(m)\n        xi = X_b[random_index : random_index + 1]\n        yi = y[random_index : random_index + 1]\n        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n        eta = learning_schedule(epoch * m + iteration)\n        theta = theta - eta * gradients\n        theta_path_sgd.append(theta)  # extra code – to generate the figure\n\n# extra code – this section beautifies and saves Figure 4–10\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"sgd_plot\")\nplt.show()\n\n\n\n\n\ntheta\n\narray([[4.21076011],\n       [2.74856079]])\n\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n                       n_iter_no_change=100, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n\nSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)\n\n\n\nsgd_reg.intercept_, sgd_reg.coef_\n\n(array([4.21278812]), array([2.77270267]))"
  },
  {
    "objectID": "blog5.html#mini-batch-gradient-descent",
    "href": "blog5.html#mini-batch-gradient-descent",
    "title": "Anomaly Detection",
    "section": "Mini-batch gradient descent",
    "text": "Mini-batch gradient descent\nThe code in this section is used to generate the next figure, it is not in the book.\n\n# extra code – this cell generates and saves Figure 4–11\n\nfrom math import ceil\n\nn_epochs = 50\nminibatch_size = 20\nn_batches_per_epoch = ceil(m / minibatch_size)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nt0, t1 = 200, 1000  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\ntheta_path_mgd = []\nfor epoch in range(n_epochs):\n    shuffled_indices = np.random.permutation(m)\n    X_b_shuffled = X_b[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    for iteration in range(0, n_batches_per_epoch):\n        idx = iteration * minibatch_size\n        xi = X_b_shuffled[idx : idx + minibatch_size]\n        yi = y_shuffled[idx : idx + minibatch_size]\n        gradients = 2 / minibatch_size * xi.T @ (xi @ theta - yi)\n        eta = learning_schedule(iteration)\n        theta = theta - eta * gradients\n        theta_path_mgd.append(theta)\n\ntheta_path_bgd = np.array(theta_path_bgd)\ntheta_path_sgd = np.array(theta_path_sgd)\ntheta_path_mgd = np.array(theta_path_mgd)\n\nplt.figure(figsize=(7, 4))\nplt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1,\n         label=\"Stochastic\")\nplt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2,\n         label=\"Mini-batch\")\nplt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3,\n         label=\"Batch\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$   \", rotation=0)\nplt.axis([2.6, 4.6, 2.3, 3.4])\nplt.grid()\nsave_fig(\"gradient_descent_paths_plot\")\nplt.show()"
  },
  {
    "objectID": "blog5.html#ridge-regression",
    "href": "blog5.html#ridge-regression",
    "title": "Anomaly Detection",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nLet’s generate a very small and noisy linear dataset:\n\n# extra code – we've done this type of generation several times before\nnp.random.seed(42)\nm = 20\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\nX_new = np.linspace(0, 3, 100).reshape(100, 1)\n\n\n# extra code – a quick peek at the dataset we just generated\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \".\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$  \", rotation=0)\nplt.axis([0, 3, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55325833]])\n\n\n\n# extra code – this cell generates and saves Figure 4–17\n\ndef plot_model(model_class, polynomial, alphas, **model_kwargs):\n    plt.plot(X, y, \"b.\", linewidth=3)\n    for alpha, style in zip(alphas, (\"b:\", \"g--\", \"r-\")):\n        if alpha &gt; 0:\n            model = model_class(alpha, **model_kwargs)\n        else:\n            model = LinearRegression()\n        if polynomial:\n            model = make_pipeline(\n                PolynomialFeatures(degree=10, include_bias=False),\n                StandardScaler(),\n                model)\n        model.fit(X, y)\n        y_new_regul = model.predict(X_new)\n        plt.plot(X_new, y_new_regul, style, linewidth=2,\n                 label=fr\"$\\alpha = {alpha}$\")\n    plt.legend(loc=\"upper left\")\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 3, 0, 3.5])\n    plt.grid()\n\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"ridge_regression_plot\")\nplt.show()\n\n\n\n\n\nsgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n                       max_iter=1000, eta0=0.01, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\nsgd_reg.predict([[1.5]])\n\narray([1.55302613])\n\n\n\n# extra code – show that we get roughly the same solution as earlier when\n#              we use Stochastic Average GD (solver=\"sag\")\nridge_reg = Ridge(alpha=0.1, solver=\"sag\", random_state=42)\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55326019]])\n\n\n\n# extra code – shows the closed form solution of Ridge regression,\n#              compare with the next Ridge model's learned parameters below\nalpha = 0.1\nA = np.array([[0., 0.], [0., 1.]])\nX_b = np.c_[np.ones(m), X]\nnp.linalg.inv(X_b.T @ X_b + alpha * A) @ X_b.T @ y\n\narray([[0.97898394],\n       [0.3828496 ]])\n\n\n\nridge_reg.intercept_, ridge_reg.coef_  # extra code\n\n(array([0.97896386]), array([[0.38286422]]))"
  },
  {
    "objectID": "blog5.html#lasso-regression",
    "href": "blog5.html#lasso-regression",
    "title": "Anomaly Detection",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nfrom sklearn.linear_model import Lasso\n\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X, y)\nlasso_reg.predict([[1.5]])\n\narray([1.53788174])\n\n\n\n# extra code – this cell generates and saves Figure 4–18\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"lasso_regression_plot\")\nplt.show()\n\n\n\n\n\n# extra code – this BIG cell generates and saves Figure 4–19\n\nt1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n\nt1s = np.linspace(t1a, t1b, 500)\nt2s = np.linspace(t2a, t2b, 500)\nt1, t2 = np.meshgrid(t1s, t2s)\nT = np.c_[t1.ravel(), t2.ravel()]\nXr = np.array([[1, 1], [1, -1], [1, 0.5]])\nyr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n\nJ = (1 / len(Xr) * ((T @ Xr.T - yr.T) ** 2).sum(axis=1)).reshape(t1.shape)\n\nN1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\nN2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n\nt_min_idx = np.unravel_index(J.argmin(), J.shape)\nt1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n\nt_init = np.array([[0.25], [-1]])\n\ndef bgd_path(theta, X, y, l1, l2, core=1, eta=0.05, n_iterations=200):\n    path = [theta]\n    for iteration in range(n_iterations):\n        gradients = (core * 2 / len(X) * X.T @ (X @ theta - y)\n                     + l1 * np.sign(theta) + l2 * theta)\n        theta = theta - eta * gradients\n        path.append(theta)\n    return np.array(path)\n\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n\nfor i, N, l1, l2, title in ((0, N1, 2.0, 0, \"Lasso\"), (1, N2, 0, 2.0, \"Ridge\")):\n    JR = J + l1 * N1 + l2 * 0.5 * N2 ** 2\n\n    tr_min_idx = np.unravel_index(JR.argmin(), JR.shape)\n    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n\n    levels = np.exp(np.linspace(0, 1, 20)) - 1\n    levelsJ = levels * (J.max() - J.min()) + J.min()\n    levelsJR = levels * (JR.max() - JR.min()) + JR.min()\n    levelsN = np.linspace(0, N.max(), 10)\n\n    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n    path_N = bgd_path(theta=np.array([[2.0], [0.5]]), X=Xr, y=yr,\n                      l1=np.sign(l1) / 3, l2=np.sign(l2), core=0)\n    ax = axes[i, 0]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, N / 2.0, levels=levelsN)\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.set_title(fr\"$\\ell_{i + 1}$ penalty\")\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n    ax.set_ylabel(r\"$\\theta_2$\", rotation=0)\n\n    ax = axes[i, 1]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.plot(t1r_min, t2r_min, \"rs\")\n    ax.set_title(title)\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n\nsave_fig(\"lasso_vs_ridge_plot\")\nplt.show()"
  },
  {
    "objectID": "blog5.html#elastic-net",
    "href": "blog5.html#elastic-net",
    "title": "Anomaly Detection",
    "section": "Elastic Net",
    "text": "Elastic Net\n\nfrom sklearn.linear_model import ElasticNet\n\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic_net.fit(X, y)\nelastic_net.predict([[1.5]])\n\narray([1.54333232])"
  },
  {
    "objectID": "blog5.html#early-stopping",
    "href": "blog5.html#early-stopping",
    "title": "Anomaly Detection",
    "section": "Early Stopping",
    "text": "Early Stopping\nLet’s go back to the quadratic dataset we used earlier:\n\nfrom copy import deepcopy\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n# extra code – creates the same quadratic dataset as earlier and splits it\nnp.random.seed(42)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\nX_train, y_train = X[: m // 2], y[: m // 2, 0]\nX_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n                              StandardScaler())\nX_train_prep = preprocessing.fit_transform(X_train)\nX_valid_prep = preprocessing.transform(X_valid)\nsgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\nn_epochs = 500\nbest_valid_rmse = float('inf')\ntrain_errors, val_errors = [], []  # extra code – it's for the figure below\n\nfor epoch in range(n_epochs):\n    sgd_reg.partial_fit(X_train_prep, y_train)\n    y_valid_predict = sgd_reg.predict(X_valid_prep)\n    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n    if val_error &lt; best_valid_rmse:\n        best_valid_rmse = val_error\n        best_model = deepcopy(sgd_reg)\n\n    # extra code – we evaluate the train error and save it for the figure\n    y_train_predict = sgd_reg.predict(X_train_prep)\n    train_error = mean_squared_error(y_train, y_train_predict, squared=False)\n    val_errors.append(val_error)\n    train_errors.append(train_error)\n\n# extra code – this section generates and saves Figure 4–20\nbest_epoch = np.argmin(val_errors)\nplt.figure(figsize=(6, 4))\nplt.annotate('Best model',\n             xy=(best_epoch, best_valid_rmse),\n             xytext=(best_epoch, best_valid_rmse + 0.5),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], \"k:\", linewidth=2)\nplt.plot(val_errors, \"b-\", linewidth=3, label=\"Validation set\")\nplt.plot(best_epoch, best_valid_rmse, \"bo\")\nplt.plot(train_errors, \"r--\", linewidth=2, label=\"Training set\")\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"RMSE\")\nplt.axis([0, n_epochs, 0, 3.5])\nplt.grid()\nsave_fig(\"early_stopping_plot\")\nplt.show()"
  },
  {
    "objectID": "blog5.html#estimating-probabilities",
    "href": "blog5.html#estimating-probabilities",
    "title": "Anomaly Detection",
    "section": "Estimating Probabilities",
    "text": "Estimating Probabilities\n\n# extra code – generates and saves Figure 4–21\n\nlim = 6\nt = np.linspace(-lim, lim, 100)\nsig = 1 / (1 + np.exp(-t))\n\nplt.figure(figsize=(8, 3))\nplt.plot([-lim, lim], [0, 0], \"k-\")\nplt.plot([-lim, lim], [0.5, 0.5], \"k:\")\nplt.plot([-lim, lim], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\")\nplt.axis([-lim, lim, -0.1, 1.1])\nplt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\nplt.grid()\nsave_fig(\"logistic_function_plot\")\nplt.show()"
  },
  {
    "objectID": "blog5.html#decision-boundaries",
    "href": "blog5.html#decision-boundaries",
    "title": "Anomaly Detection",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\nlist(iris)\n\n['data',\n 'target',\n 'frame',\n 'target_names',\n 'DESCR',\n 'feature_names',\n 'filename',\n 'data_module']\n\n\n\nprint(iris.DESCR)  # extra code – it's a bit too long\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n|details-start|\n**References**\n|details-split|\n\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n  Mathematical Statistics\" (John Wiley, NY, 1950).\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n  Structure and Classification Rule for Recognition in Partially Exposed\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n  on Information Theory, May 1972, 431-433.\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n  conceptual clustering system finds 3 classes in the data.\n- Many, many more ...\n\n|details-end|\n\n\n\niris.data.head(3)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n\n\n\n\n\n\niris.target.head(3)  # note that the instances are not shuffled\n\n0    0\n1    0\n2    0\nName: target, dtype: int64\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\n\nLogisticRegression(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=42)\n\n\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\ny_proba = log_reg.predict_proba(X_new)\ndecision_boundary = X_new[y_proba[:, 1] &gt;= 0.5][0, 0]\n\nplt.figure(figsize=(8, 3))  # extra code – not needed, just formatting\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n         label=\"Not Iris virginica proba\")\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\nplt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n         label=\"Decision boundary\")\n\n# extra code – this section beautifies and saves Figure 4–23\nplt.arrow(x=decision_boundary, y=0.08, dx=-0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\nplt.arrow(x=decision_boundary, y=0.92, dx=0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\nplt.plot(X_train[y_train == 0], y_train[y_train == 0], \"bs\")\nplt.plot(X_train[y_train == 1], y_train[y_train == 1], \"g^\")\nplt.xlabel(\"Petal width (cm)\")\nplt.ylabel(\"Probability\")\nplt.legend(loc=\"center left\")\nplt.axis([0, 3, -0.02, 1.02])\nplt.grid()\nsave_fig(\"logistic_regression_plot\")\n\nplt.show()\n\n\n\n\n\ndecision_boundary\n\n1.6516516516516517\n\n\n\nlog_reg.predict([[1.7], [1.5]])\n\narray([ True, False])\n\n\n\n# extra code – this cell generates and saves Figure 4–24\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(C=2, random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# for the contour plot\nx0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1),\n                     np.linspace(0.8, 2.7, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]  # one instance per point on the figure\ny_proba = log_reg.predict_proba(X_new)\nzz = y_proba[:, 1].reshape(x0.shape)\n\n# for the decision boundary\nleft_right = np.array([2.9, 7])\nboundary = -((log_reg.coef_[0, 0] * left_right + log_reg.intercept_[0])\n             / log_reg.coef_[0, 1])\n\nplt.figure(figsize=(10, 4))\nplt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \"bs\")\nplt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \"g^\")\ncontour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1)\nplt.plot(left_right, boundary, \"k--\", linewidth=3)\nplt.text(3.5, 1.27, \"Not Iris virginica\", color=\"b\", ha=\"center\")\nplt.text(6.5, 2.3, \"Iris virginica\", color=\"g\", ha=\"center\")\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.axis([2.9, 7, 0.8, 2.7])\nplt.grid()\nsave_fig(\"logistic_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "blog5.html#softmax-regression",
    "href": "blog5.html#softmax-regression",
    "title": "Anomaly Detection",
    "section": "Softmax Regression",
    "text": "Softmax Regression\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax_reg = LogisticRegression(C=30, random_state=42)\nsoftmax_reg.fit(X_train, y_train)\n\nLogisticRegression(C=30, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=30, random_state=42)\n\n\n\nsoftmax_reg.predict([[5, 2]])\n\narray([2])\n\n\n\nsoftmax_reg.predict_proba([[5, 2]]).round(2)\n\narray([[0.  , 0.04, 0.96]])\n\n\n\n# extra code – this cell generates and saves Figure 4–25\n\nfrom matplotlib.colors import ListedColormap\n\ncustom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\n\ny_proba = softmax_reg.predict_proba(X_new)\ny_predict = softmax_reg.predict(X_new)\n\nzz1 = y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"center left\")\nplt.axis([0.5, 7, 0, 3.5])\nplt.grid()\nsave_fig(\"softmax_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "blog5.html#to-11.",
    "href": "blog5.html#to-11.",
    "title": "Anomaly Detection",
    "section": "1. to 11.",
    "text": "1. to 11.\n\nIf you have a training set with millions of features you can use Stochastic Gradient Descent or Mini-batch Gradient Descent, and perhaps Batch Gradient Descent if the training set fits in memory. But you cannot use the Normal Equation or the SVD approach because the computational complexity grows quickly (more than quadratically) with the number of features.\nIf the features in your training set have very different scales, the cost function will have the shape of an elongated bowl, so the Gradient Descent algorithms will take a long time to converge. To solve this you should scale the data before training the model. Note that the Normal Equation or SVD approach will work just fine without scaling. Moreover, regularized models may converge to a suboptimal solution if the features are not scaled: since regularization penalizes large weights, features with smaller values will tend to be ignored compared to features with larger values.\nGradient Descent cannot get stuck in a local minimum when training a Logistic Regression model because the cost function is convex. Convex means that if you draw a straight line between any two points on the curve, the line never crosses the curve.\nIf the optimization problem is convex (such as Linear Regression or Logistic Regression), and assuming the learning rate is not too high, then all Gradient Descent algorithms will approach the global optimum and end up producing fairly similar models. However, unless you gradually reduce the learning rate, Stochastic GD and Mini-batch GD will never truly converge; instead, they will keep jumping back and forth around the global optimum. This means that even if you let them run for a very long time, these Gradient Descent algorithms will produce slightly different models.\nIf the validation error consistently goes up after every epoch, then one possibility is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However, if the training error is not going up, then your model is overfitting the training set and you should stop training.\nDue to their random nature, neither Stochastic Gradient Descent nor Mini-batch Gradient Descent is guaranteed to make progress at every single training iteration. So if you immediately stop training when the validation error goes up, you may stop much too early, before the optimum is reached. A better option is to save the model at regular intervals; then, when it has not improved for a long time (meaning it will probably never beat the record), you can revert to the best saved model.\nStochastic Gradient Descent has the fastest training iteration since it considers only one training instance at a time, so it is generally the first to reach the vicinity of the global optimum (or Mini-batch GD with a very small mini-batch size). However, only Batch Gradient Descent will actually converge, given enough training time. As mentioned, Stochastic GD and Mini-batch GD will bounce around the optimum, unless you gradually reduce the learning rate.\nIf the validation error is much higher than the training error, this is likely because your model is overfitting the training set. One way to try to fix this is to reduce the polynomial degree: a model with fewer degrees of freedom is less likely to overfit. Another thing you can try is to regularize the model—for example, by adding an ℓ₂ penalty (Ridge) or an ℓ₁ penalty (Lasso) to the cost function. This will also reduce the degrees of freedom of the model. Lastly, you can try to increase the size of the training set.\nIf both the training error and the validation error are almost equal and fairly high, the model is likely underfitting the training set, which means it has a high bias. You should try reducing the regularization hyperparameter α.\nLet’s see:\n\n\nA model with some regularization typically performs better than a model without any regularization, so you should generally prefer Ridge Regression over plain Linear Regression.\nLasso Regression uses an ℓ₁ penalty, which tends to push the weights down to exactly zero. This leads to sparse models, where all weights are zero except for the most important weights. This is a way to perform feature selection automatically, which is good if you suspect that only a few features actually matter. When you are not sure, you should prefer Ridge Regression.\nElastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training instances). However, it does add an extra hyperparameter to tune. If you want Lasso without the erratic behavior, you can just use Elastic Net with an l1_ratio close to 1.\n\n\nIf you want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not exclusive classes (i.e., all four combinations are possible) you should train two Logistic Regression classifiers."
  },
  {
    "objectID": "blog5.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "href": "blog5.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "title": "Anomaly Detection",
    "section": "12. Batch Gradient Descent with early stopping for Softmax Regression",
    "text": "12. Batch Gradient Descent with early stopping for Softmax Regression\nExercise: Implement Batch Gradient Descent with early stopping for Softmax Regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset.\nLet’s start by loading the data. We will just reuse the Iris dataset we loaded earlier.\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"].values\n\nWe need to add the bias term for every instance (\\(x_0 = 1\\)). The easiest option to do this would be to use Scikit-Learn’s add_dummy_feature() function, but the point of this exercise is to get a better understanding of the algorithms by implementing them manually. So here is one possible implementation:\n\nX_with_bias = np.c_[np.ones(len(X)), X]\n\nThe easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn’s train_test_split() function, but again, we want to do it manually:\n\ntest_ratio = 0.2\nvalidation_ratio = 0.2\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nnp.random.seed(42)\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\nThe targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for any given instance is a one-hot vector). Let’s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance. To understand this code, you need to know that np.diag(np.ones(n)) creates an n×n matrix full of 0s except for 1s on the main diagonal. Moreover, if a is a NumPy array, then a[[1, 3, 2]] returns an array with 3 rows equal to a[1], a[3] and a[2] (this is advanced NumPy indexing).\n\ndef to_one_hot(y):\n    return np.diag(np.ones(y.max() + 1))[y]\n\nLet’s test this function on the first 10 instances:\n\ny_train[:10]\n\narray([1, 0, 2, 1, 1, 0, 1, 2, 1, 1])\n\n\n\nto_one_hot(y_train[:10])\n\narray([[0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.]])\n\n\nLooks good, so let’s create the target class probabilities matrix for the training set and the test set:\n\nY_train_one_hot = to_one_hot(y_train)\nY_valid_one_hot = to_one_hot(y_valid)\nY_test_one_hot = to_one_hot(y_test)\n\nNow let’s scale the inputs. We compute the mean and standard deviation of each feature on the training set (except for the bias feature), then we center and scale each feature in the training set, the validation set, and the test set:\n\nmean = X_train[:, 1:].mean(axis=0)\nstd = X_train[:, 1:].std(axis=0)\nX_train[:, 1:] = (X_train[:, 1:] - mean) / std\nX_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\nX_test[:, 1:] = (X_test[:, 1:] - mean) / std\n\nNow let’s implement the Softmax function. Recall that it is defined by the following equation:\n\\(\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}\\)\n\ndef softmax(logits):\n    exps = np.exp(logits)\n    exp_sums = exps.sum(axis=1, keepdims=True)\n    return exps / exp_sums\n\nWe are almost ready to start training. Let’s define the number of inputs and outputs:\n\nn_inputs = X_train.shape[1]  # == 3 (2 features plus the bias term)\nn_outputs = len(np.unique(y_train))  # == 3 (there are 3 iris classes)\n\nNow here comes the hardest part: training! Theoretically, it’s simple: it’s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it’s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it’s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won’t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what’s going on under the hood.\nSo the equations we will need are the cost function:\n\\(J(\\mathbf{\\Theta}) = - \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}\\)\nAnd the equation for the gradients:\n\\(\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}\\)\nNote that \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) may not be computable if \\(\\hat{p}_k^{(i)} = 0\\). So we will add a tiny value \\(\\epsilon\\) to \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) to avoid getting nan values.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        print(epoch, xentropy_losses.sum(axis=1).mean())\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    Theta = Theta - eta * gradients\n\n0 3.7085808486476917\n1000 0.14519367480830647\n2000 0.13013095755040877\n3000 0.12009639326384532\n4000 0.11372961364786878\n5000 0.11002459532472424\n\n\nAnd that’s it! The Softmax model is trained. Let’s look at the model parameters:\n\nTheta\n\narray([[ 0.41931626,  6.11112089, -5.52429876],\n       [-6.53054533, -0.74608616,  8.33137102],\n       [-5.28115784,  0.25152675,  6.90680425]])\n\n\nLet’s make predictions for the validation set and check the accuracy score:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nWell, this model looks pretty ok. For the sake of the exercise, let’s add a bit of \\(\\ell_2\\) regularization. The following training code is similar to the one above, but the loss now has an additional \\(\\ell_2\\) penalty, and the gradients have the proper additional term (note that we don’t regularize the first element of Theta since this corresponds to the bias term). Also, let’s try increasing the learning rate eta.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\nalpha = 0.01  # regularization hyperparameter\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n        total_loss = xentropy_losses.sum(axis=1).mean() + alpha * l2_loss\n        print(epoch, total_loss.round(4))\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n1000 0.3259\n2000 0.3259\n3000 0.3259\n4000 0.3259\n5000 0.3259\n\n\nBecause of the additional \\(\\ell_2\\) penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let’s find out:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nIn this case, the \\(\\ell_2\\) penalty did not change the test accuracy. Perhaps try fine-tuning alpha?\nNow let’s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing.\n\neta = 0.5\nn_epochs = 50_001\nm = len(X_train)\nepsilon = 1e-5\nC = 100  # regularization hyperparameter\nbest_loss = np.infty\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    Y_proba_valid = softmax(X_valid @ Theta)\n    xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n    l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n    total_loss = xentropy_losses.sum(axis=1).mean() + 1 / C * l2_loss\n    if epoch % 1000 == 0:\n        print(epoch, total_loss.round(4))\n    if total_loss &lt; best_loss:\n        best_loss = total_loss\n    else:\n        print(epoch - 1, best_loss.round(4))\n        print(epoch, total_loss.round(4), \"early stopping!\")\n        break\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), 1 / C * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n281 0.3256\n282 0.3256 early stopping!\n\n\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nOh well, still no change in validation accuracy, but at least early stopping shortened training a bit.\nNow let’s plot the model’s predictions on the whole dataset (remember to scale all features fed to the model):\n\ncustom_cmap = mpl.colors.ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new = (X_new - mean) / std\nX_new_with_bias = np.c_[np.ones(len(X_new)), X_new]\n\nlogits = X_new_with_bias @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"upper left\")\nplt.axis([0, 7, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\nAnd now let’s measure the final model’s accuracy on the test set:\n\nlogits = X_test @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_test).mean()\naccuracy_score\n\n0.9666666666666667\n\n\nWell we get even better performance on the test set. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary."
  },
  {
    "objectID": "blog0.html",
    "href": "blog0.html",
    "title": "Clustering",
    "section": "",
    "text": "Chapter 4 – Training Models\nThis notebook contains all the sample code and solutions to the exercises in chapter 4.\nThis project requires Python 3.7 or above:\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nIt also requires Scikit-Learn ≥ 1.0.1:\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\nAs we did in previous chapters, let’s define the default font sizes to make the figures prettier:\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\nAnd let’s create the images/training_linear_models folder (if it doesn’t already exist), and define the save_fig() function which is used through this notebook to save the figures in high-res for the book:\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"training_linear_models\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)"
  },
  {
    "objectID": "blog0.html#the-normal-equation",
    "href": "blog0.html#the-normal-equation",
    "title": "Clustering",
    "section": "The Normal Equation",
    "text": "The Normal Equation\n\nimport numpy as np\n\nnp.random.seed(42)  # to make this code example reproducible\nm = 100  # number of instances\nX = 2 * np.random.rand(m, 1)  # column vector\ny = 4 + 3 * X + np.random.randn(m, 1)  # column vector\n\n\n# extra code – generates and saves Figure 4–1\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"generated_data_plot\")\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import add_dummy_feature\n\nX_b = add_dummy_feature(X)  # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n\n\ntheta_best\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\nX_new = np.array([[0], [2]])\nX_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\ny_predict = X_new_b @ theta_best\ny_predict\n\narray([[4.21509616],\n       [9.75532293]])\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.plot(X, y, \"b.\")\n\n# extra code – beautifies and saves Figure 4–2\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.legend(loc=\"upper left\")\nsave_fig(\"linear_model_predictions_plot\")\n\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([4.21509616]), array([[2.77011339]]))\n\n\n\nlin_reg.predict(X_new)\n\narray([[4.21509616],\n       [9.75532293]])\n\n\nThe LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:\n\ntheta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\ntheta_best_svd\n\narray([[4.21509616],\n       [2.77011339]])\n\n\nThis function computes \\(\\mathbf{X}^+\\mathbf{y}\\), where \\(\\mathbf{X}^{+}\\) is the pseudoinverse of \\(\\mathbf{X}\\) (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly:\n\nnp.linalg.pinv(X_b) @ y\n\narray([[4.21509616],\n       [2.77011339]])"
  },
  {
    "objectID": "blog0.html#batch-gradient-descent",
    "href": "blog0.html#batch-gradient-descent",
    "title": "Clustering",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\n\neta = 0.1  # learning rate\nn_epochs = 1000\nm = len(X_b)  # number of instances\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # randomly initialized model parameters\n\nfor epoch in range(n_epochs):\n    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n    theta = theta - eta * gradients\n\nThe trained model parameters:\n\ntheta\n\narray([[4.21509616],\n       [2.77011339]])\n\n\n\n# extra code – generates and saves Figure 4–8\n\nimport matplotlib as mpl\n\ndef plot_gradient_descent(theta, eta):\n    m = len(X_b)\n    plt.plot(X, y, \"b.\")\n    n_epochs = 1000\n    n_shown = 20\n    theta_path = []\n    for epoch in range(n_epochs):\n        if epoch &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(epoch / n_shown + 0.15))\n            plt.plot(X_new, y_predict, linestyle=\"solid\", color=color)\n        gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n        theta = theta - eta * gradients\n        theta_path.append(theta)\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 2, 0, 15])\n    plt.grid()\n    plt.title(fr\"$\\eta = {eta}$\")\n    return theta_path\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nplt.figure(figsize=(10, 4))\nplt.subplot(131)\nplot_gradient_descent(theta, eta=0.02)\nplt.ylabel(\"$y$\", rotation=0)\nplt.subplot(132)\ntheta_path_bgd = plot_gradient_descent(theta, eta=0.1)\nplt.gca().axes.yaxis.set_ticklabels([])\nplt.subplot(133)\nplt.gca().axes.yaxis.set_ticklabels([])\nplot_gradient_descent(theta, eta=0.5)\nsave_fig(\"gradient_descent_plot\")\nplt.show()"
  },
  {
    "objectID": "blog0.html#stochastic-gradient-descent",
    "href": "blog0.html#stochastic-gradient-descent",
    "title": "Clustering",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\ntheta_path_sgd = []  # extra code – we need to store the path of theta in the\n                     #              parameter space to plot the next figure\n\n\nn_epochs = 50\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nn_shown = 20  # extra code – just needed to generate the figure below\nplt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n\nfor epoch in range(n_epochs):\n    for iteration in range(m):\n\n        # extra code – these 4 lines are used to generate the figure\n        if epoch == 0 and iteration &lt; n_shown:\n            y_predict = X_new_b @ theta\n            color = mpl.colors.rgb2hex(plt.cm.OrRd(iteration / n_shown + 0.15))\n            plt.plot(X_new, y_predict, color=color)\n\n        random_index = np.random.randint(m)\n        xi = X_b[random_index : random_index + 1]\n        yi = y[random_index : random_index + 1]\n        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n        eta = learning_schedule(epoch * m + iteration)\n        theta = theta - eta * gradients\n        theta_path_sgd.append(theta)  # extra code – to generate the figure\n\n# extra code – this section beautifies and saves Figure 4–10\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nsave_fig(\"sgd_plot\")\nplt.show()\n\n\n\n\n\ntheta\n\narray([[4.21076011],\n       [2.74856079]])\n\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n                       n_iter_no_change=100, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n\nSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)\n\n\n\nsgd_reg.intercept_, sgd_reg.coef_\n\n(array([4.21278812]), array([2.77270267]))"
  },
  {
    "objectID": "blog0.html#mini-batch-gradient-descent",
    "href": "blog0.html#mini-batch-gradient-descent",
    "title": "Clustering",
    "section": "Mini-batch gradient descent",
    "text": "Mini-batch gradient descent\nThe code in this section is used to generate the next figure, it is not in the book.\n\n# extra code – this cell generates and saves Figure 4–11\n\nfrom math import ceil\n\nn_epochs = 50\nminibatch_size = 20\nn_batches_per_epoch = ceil(m / minibatch_size)\n\nnp.random.seed(42)\ntheta = np.random.randn(2, 1)  # random initialization\n\nt0, t1 = 200, 1000  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\ntheta_path_mgd = []\nfor epoch in range(n_epochs):\n    shuffled_indices = np.random.permutation(m)\n    X_b_shuffled = X_b[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    for iteration in range(0, n_batches_per_epoch):\n        idx = iteration * minibatch_size\n        xi = X_b_shuffled[idx : idx + minibatch_size]\n        yi = y_shuffled[idx : idx + minibatch_size]\n        gradients = 2 / minibatch_size * xi.T @ (xi @ theta - yi)\n        eta = learning_schedule(iteration)\n        theta = theta - eta * gradients\n        theta_path_mgd.append(theta)\n\ntheta_path_bgd = np.array(theta_path_bgd)\ntheta_path_sgd = np.array(theta_path_sgd)\ntheta_path_mgd = np.array(theta_path_mgd)\n\nplt.figure(figsize=(7, 4))\nplt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1,\n         label=\"Stochastic\")\nplt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2,\n         label=\"Mini-batch\")\nplt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3,\n         label=\"Batch\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$   \", rotation=0)\nplt.axis([2.6, 4.6, 2.3, 3.4])\nplt.grid()\nsave_fig(\"gradient_descent_paths_plot\")\nplt.show()"
  },
  {
    "objectID": "blog0.html#ridge-regression",
    "href": "blog0.html#ridge-regression",
    "title": "Clustering",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nLet’s generate a very small and noisy linear dataset:\n\n# extra code – we've done this type of generation several times before\nnp.random.seed(42)\nm = 20\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\nX_new = np.linspace(0, 3, 100).reshape(100, 1)\n\n\n# extra code – a quick peek at the dataset we just generated\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \".\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$  \", rotation=0)\nplt.axis([0, 3, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55325833]])\n\n\n\n# extra code – this cell generates and saves Figure 4–17\n\ndef plot_model(model_class, polynomial, alphas, **model_kwargs):\n    plt.plot(X, y, \"b.\", linewidth=3)\n    for alpha, style in zip(alphas, (\"b:\", \"g--\", \"r-\")):\n        if alpha &gt; 0:\n            model = model_class(alpha, **model_kwargs)\n        else:\n            model = LinearRegression()\n        if polynomial:\n            model = make_pipeline(\n                PolynomialFeatures(degree=10, include_bias=False),\n                StandardScaler(),\n                model)\n        model.fit(X, y)\n        y_new_regul = model.predict(X_new)\n        plt.plot(X_new, y_new_regul, style, linewidth=2,\n                 label=fr\"$\\alpha = {alpha}$\")\n    plt.legend(loc=\"upper left\")\n    plt.xlabel(\"$x_1$\")\n    plt.axis([0, 3, 0, 3.5])\n    plt.grid()\n\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"ridge_regression_plot\")\nplt.show()\n\n\n\n\n\nsgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n                       max_iter=1000, eta0=0.01, random_state=42)\nsgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\nsgd_reg.predict([[1.5]])\n\narray([1.55302613])\n\n\n\n# extra code – show that we get roughly the same solution as earlier when\n#              we use Stochastic Average GD (solver=\"sag\")\nridge_reg = Ridge(alpha=0.1, solver=\"sag\", random_state=42)\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])\n\narray([[1.55326019]])\n\n\n\n# extra code – shows the closed form solution of Ridge regression,\n#              compare with the next Ridge model's learned parameters below\nalpha = 0.1\nA = np.array([[0., 0.], [0., 1.]])\nX_b = np.c_[np.ones(m), X]\nnp.linalg.inv(X_b.T @ X_b + alpha * A) @ X_b.T @ y\n\narray([[0.97898394],\n       [0.3828496 ]])\n\n\n\nridge_reg.intercept_, ridge_reg.coef_  # extra code\n\n(array([0.97896386]), array([[0.38286422]]))"
  },
  {
    "objectID": "blog0.html#lasso-regression",
    "href": "blog0.html#lasso-regression",
    "title": "Clustering",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nfrom sklearn.linear_model import Lasso\n\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X, y)\nlasso_reg.predict([[1.5]])\n\narray([1.53788174])\n\n\n\n# extra code – this cell generates and saves Figure 4–18\nplt.figure(figsize=(9, 3.5))\nplt.subplot(121)\nplot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\nplt.ylabel(\"$y$  \", rotation=0)\nplt.subplot(122)\nplot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=42)\nplt.gca().axes.yaxis.set_ticklabels([])\nsave_fig(\"lasso_regression_plot\")\nplt.show()\n\n\n\n\n\n# extra code – this BIG cell generates and saves Figure 4–19\n\nt1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n\nt1s = np.linspace(t1a, t1b, 500)\nt2s = np.linspace(t2a, t2b, 500)\nt1, t2 = np.meshgrid(t1s, t2s)\nT = np.c_[t1.ravel(), t2.ravel()]\nXr = np.array([[1, 1], [1, -1], [1, 0.5]])\nyr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n\nJ = (1 / len(Xr) * ((T @ Xr.T - yr.T) ** 2).sum(axis=1)).reshape(t1.shape)\n\nN1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\nN2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n\nt_min_idx = np.unravel_index(J.argmin(), J.shape)\nt1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n\nt_init = np.array([[0.25], [-1]])\n\ndef bgd_path(theta, X, y, l1, l2, core=1, eta=0.05, n_iterations=200):\n    path = [theta]\n    for iteration in range(n_iterations):\n        gradients = (core * 2 / len(X) * X.T @ (X @ theta - y)\n                     + l1 * np.sign(theta) + l2 * theta)\n        theta = theta - eta * gradients\n        path.append(theta)\n    return np.array(path)\n\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n\nfor i, N, l1, l2, title in ((0, N1, 2.0, 0, \"Lasso\"), (1, N2, 0, 2.0, \"Ridge\")):\n    JR = J + l1 * N1 + l2 * 0.5 * N2 ** 2\n\n    tr_min_idx = np.unravel_index(JR.argmin(), JR.shape)\n    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n\n    levels = np.exp(np.linspace(0, 1, 20)) - 1\n    levelsJ = levels * (J.max() - J.min()) + J.min()\n    levelsJR = levels * (JR.max() - JR.min()) + JR.min()\n    levelsN = np.linspace(0, N.max(), 10)\n\n    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n    path_N = bgd_path(theta=np.array([[2.0], [0.5]]), X=Xr, y=yr,\n                      l1=np.sign(l1) / 3, l2=np.sign(l2), core=0)\n    ax = axes[i, 0]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, N / 2.0, levels=levelsN)\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.set_title(fr\"$\\ell_{i + 1}$ penalty\")\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n    ax.set_ylabel(r\"$\\theta_2$\", rotation=0)\n\n    ax = axes[i, 1]\n    ax.grid()\n    ax.axhline(y=0, color=\"k\")\n    ax.axvline(x=0, color=\"k\")\n    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n    ax.plot(0, 0, \"ys\")\n    ax.plot(t1_min, t2_min, \"ys\")\n    ax.plot(t1r_min, t2r_min, \"rs\")\n    ax.set_title(title)\n    ax.axis([t1a, t1b, t2a, t2b])\n    if i == 1:\n        ax.set_xlabel(r\"$\\theta_1$\")\n\nsave_fig(\"lasso_vs_ridge_plot\")\nplt.show()"
  },
  {
    "objectID": "blog0.html#elastic-net",
    "href": "blog0.html#elastic-net",
    "title": "Clustering",
    "section": "Elastic Net",
    "text": "Elastic Net\n\nfrom sklearn.linear_model import ElasticNet\n\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic_net.fit(X, y)\nelastic_net.predict([[1.5]])\n\narray([1.54333232])"
  },
  {
    "objectID": "blog0.html#early-stopping",
    "href": "blog0.html#early-stopping",
    "title": "Clustering",
    "section": "Early Stopping",
    "text": "Early Stopping\nLet’s go back to the quadratic dataset we used earlier:\n\nfrom copy import deepcopy\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n# extra code – creates the same quadratic dataset as earlier and splits it\nnp.random.seed(42)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\nX_train, y_train = X[: m // 2], y[: m // 2, 0]\nX_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n                              StandardScaler())\nX_train_prep = preprocessing.fit_transform(X_train)\nX_valid_prep = preprocessing.transform(X_valid)\nsgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\nn_epochs = 500\nbest_valid_rmse = float('inf')\ntrain_errors, val_errors = [], []  # extra code – it's for the figure below\n\nfor epoch in range(n_epochs):\n    sgd_reg.partial_fit(X_train_prep, y_train)\n    y_valid_predict = sgd_reg.predict(X_valid_prep)\n    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n    if val_error &lt; best_valid_rmse:\n        best_valid_rmse = val_error\n        best_model = deepcopy(sgd_reg)\n\n    # extra code – we evaluate the train error and save it for the figure\n    y_train_predict = sgd_reg.predict(X_train_prep)\n    train_error = mean_squared_error(y_train, y_train_predict, squared=False)\n    val_errors.append(val_error)\n    train_errors.append(train_error)\n\n# extra code – this section generates and saves Figure 4–20\nbest_epoch = np.argmin(val_errors)\nplt.figure(figsize=(6, 4))\nplt.annotate('Best model',\n             xy=(best_epoch, best_valid_rmse),\n             xytext=(best_epoch, best_valid_rmse + 0.5),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], \"k:\", linewidth=2)\nplt.plot(val_errors, \"b-\", linewidth=3, label=\"Validation set\")\nplt.plot(best_epoch, best_valid_rmse, \"bo\")\nplt.plot(train_errors, \"r--\", linewidth=2, label=\"Training set\")\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"RMSE\")\nplt.axis([0, n_epochs, 0, 3.5])\nplt.grid()\nsave_fig(\"early_stopping_plot\")\nplt.show()"
  },
  {
    "objectID": "blog0.html#estimating-probabilities",
    "href": "blog0.html#estimating-probabilities",
    "title": "Clustering",
    "section": "Estimating Probabilities",
    "text": "Estimating Probabilities\n\n# extra code – generates and saves Figure 4–21\n\nlim = 6\nt = np.linspace(-lim, lim, 100)\nsig = 1 / (1 + np.exp(-t))\n\nplt.figure(figsize=(8, 3))\nplt.plot([-lim, lim], [0, 0], \"k-\")\nplt.plot([-lim, lim], [0.5, 0.5], \"k:\")\nplt.plot([-lim, lim], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\")\nplt.axis([-lim, lim, -0.1, 1.1])\nplt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\nplt.grid()\nsave_fig(\"logistic_function_plot\")\nplt.show()"
  },
  {
    "objectID": "blog0.html#decision-boundaries",
    "href": "blog0.html#decision-boundaries",
    "title": "Clustering",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\nlist(iris)\n\n['data',\n 'target',\n 'frame',\n 'target_names',\n 'DESCR',\n 'feature_names',\n 'filename',\n 'data_module']\n\n\n\nprint(iris.DESCR)  # extra code – it's a bit too long\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n|details-start|\n**References**\n|details-split|\n\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n  Mathematical Statistics\" (John Wiley, NY, 1950).\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n  Structure and Classification Rule for Recognition in Partially Exposed\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n  on Information Theory, May 1972, 431-433.\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n  conceptual clustering system finds 3 classes in the data.\n- Many, many more ...\n\n|details-end|\n\n\n\niris.data.head(3)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n\n\n\n\n\n\niris.target.head(3)  # note that the instances are not shuffled\n\n0    0\n1    0\n2    0\nName: target, dtype: int64\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\n\nLogisticRegression(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=42)\n\n\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\ny_proba = log_reg.predict_proba(X_new)\ndecision_boundary = X_new[y_proba[:, 1] &gt;= 0.5][0, 0]\n\nplt.figure(figsize=(8, 3))  # extra code – not needed, just formatting\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n         label=\"Not Iris virginica proba\")\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\nplt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n         label=\"Decision boundary\")\n\n# extra code – this section beautifies and saves Figure 4–23\nplt.arrow(x=decision_boundary, y=0.08, dx=-0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\nplt.arrow(x=decision_boundary, y=0.92, dx=0.3, dy=0,\n          head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\nplt.plot(X_train[y_train == 0], y_train[y_train == 0], \"bs\")\nplt.plot(X_train[y_train == 1], y_train[y_train == 1], \"g^\")\nplt.xlabel(\"Petal width (cm)\")\nplt.ylabel(\"Probability\")\nplt.legend(loc=\"center left\")\nplt.axis([0, 3, -0.02, 1.02])\nplt.grid()\nsave_fig(\"logistic_regression_plot\")\n\nplt.show()\n\n\n\n\n\ndecision_boundary\n\n1.6516516516516517\n\n\n\nlog_reg.predict([[1.7], [1.5]])\n\narray([ True, False])\n\n\n\n# extra code – this cell generates and saves Figure 4–24\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris.target_names[iris.target] == 'virginica'\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlog_reg = LogisticRegression(C=2, random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# for the contour plot\nx0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1),\n                     np.linspace(0.8, 2.7, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]  # one instance per point on the figure\ny_proba = log_reg.predict_proba(X_new)\nzz = y_proba[:, 1].reshape(x0.shape)\n\n# for the decision boundary\nleft_right = np.array([2.9, 7])\nboundary = -((log_reg.coef_[0, 0] * left_right + log_reg.intercept_[0])\n             / log_reg.coef_[0, 1])\n\nplt.figure(figsize=(10, 4))\nplt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \"bs\")\nplt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \"g^\")\ncontour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1)\nplt.plot(left_right, boundary, \"k--\", linewidth=3)\nplt.text(3.5, 1.27, \"Not Iris virginica\", color=\"b\", ha=\"center\")\nplt.text(6.5, 2.3, \"Iris virginica\", color=\"g\", ha=\"center\")\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.axis([2.9, 7, 0.8, 2.7])\nplt.grid()\nsave_fig(\"logistic_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "blog0.html#softmax-regression",
    "href": "blog0.html#softmax-regression",
    "title": "Clustering",
    "section": "Softmax Regression",
    "text": "Softmax Regression\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax_reg = LogisticRegression(C=30, random_state=42)\nsoftmax_reg.fit(X_train, y_train)\n\nLogisticRegression(C=30, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=30, random_state=42)\n\n\n\nsoftmax_reg.predict([[5, 2]])\n\narray([2])\n\n\n\nsoftmax_reg.predict_proba([[5, 2]]).round(2)\n\narray([[0.  , 0.04, 0.96]])\n\n\n\n# extra code – this cell generates and saves Figure 4–25\n\nfrom matplotlib.colors import ListedColormap\n\ncustom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\n\ny_proba = softmax_reg.predict_proba(X_new)\ny_predict = softmax_reg.predict(X_new)\n\nzz1 = y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"center left\")\nplt.axis([0.5, 7, 0, 3.5])\nplt.grid()\nsave_fig(\"softmax_regression_contour_plot\")\nplt.show()"
  },
  {
    "objectID": "blog0.html#to-11.",
    "href": "blog0.html#to-11.",
    "title": "Clustering",
    "section": "1. to 11.",
    "text": "1. to 11.\n\nIf you have a training set with millions of features you can use Stochastic Gradient Descent or Mini-batch Gradient Descent, and perhaps Batch Gradient Descent if the training set fits in memory. But you cannot use the Normal Equation or the SVD approach because the computational complexity grows quickly (more than quadratically) with the number of features.\nIf the features in your training set have very different scales, the cost function will have the shape of an elongated bowl, so the Gradient Descent algorithms will take a long time to converge. To solve this you should scale the data before training the model. Note that the Normal Equation or SVD approach will work just fine without scaling. Moreover, regularized models may converge to a suboptimal solution if the features are not scaled: since regularization penalizes large weights, features with smaller values will tend to be ignored compared to features with larger values.\nGradient Descent cannot get stuck in a local minimum when training a Logistic Regression model because the cost function is convex. Convex means that if you draw a straight line between any two points on the curve, the line never crosses the curve.\nIf the optimization problem is convex (such as Linear Regression or Logistic Regression), and assuming the learning rate is not too high, then all Gradient Descent algorithms will approach the global optimum and end up producing fairly similar models. However, unless you gradually reduce the learning rate, Stochastic GD and Mini-batch GD will never truly converge; instead, they will keep jumping back and forth around the global optimum. This means that even if you let them run for a very long time, these Gradient Descent algorithms will produce slightly different models.\nIf the validation error consistently goes up after every epoch, then one possibility is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However, if the training error is not going up, then your model is overfitting the training set and you should stop training.\nDue to their random nature, neither Stochastic Gradient Descent nor Mini-batch Gradient Descent is guaranteed to make progress at every single training iteration. So if you immediately stop training when the validation error goes up, you may stop much too early, before the optimum is reached. A better option is to save the model at regular intervals; then, when it has not improved for a long time (meaning it will probably never beat the record), you can revert to the best saved model.\nStochastic Gradient Descent has the fastest training iteration since it considers only one training instance at a time, so it is generally the first to reach the vicinity of the global optimum (or Mini-batch GD with a very small mini-batch size). However, only Batch Gradient Descent will actually converge, given enough training time. As mentioned, Stochastic GD and Mini-batch GD will bounce around the optimum, unless you gradually reduce the learning rate.\nIf the validation error is much higher than the training error, this is likely because your model is overfitting the training set. One way to try to fix this is to reduce the polynomial degree: a model with fewer degrees of freedom is less likely to overfit. Another thing you can try is to regularize the model—for example, by adding an ℓ₂ penalty (Ridge) or an ℓ₁ penalty (Lasso) to the cost function. This will also reduce the degrees of freedom of the model. Lastly, you can try to increase the size of the training set.\nIf both the training error and the validation error are almost equal and fairly high, the model is likely underfitting the training set, which means it has a high bias. You should try reducing the regularization hyperparameter α.\nLet’s see:\n\n\nA model with some regularization typically performs better than a model without any regularization, so you should generally prefer Ridge Regression over plain Linear Regression.\nLasso Regression uses an ℓ₁ penalty, which tends to push the weights down to exactly zero. This leads to sparse models, where all weights are zero except for the most important weights. This is a way to perform feature selection automatically, which is good if you suspect that only a few features actually matter. When you are not sure, you should prefer Ridge Regression.\nElastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training instances). However, it does add an extra hyperparameter to tune. If you want Lasso without the erratic behavior, you can just use Elastic Net with an l1_ratio close to 1.\n\n\nIf you want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not exclusive classes (i.e., all four combinations are possible) you should train two Logistic Regression classifiers."
  },
  {
    "objectID": "blog0.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "href": "blog0.html#batch-gradient-descent-with-early-stopping-for-softmax-regression",
    "title": "Clustering",
    "section": "12. Batch Gradient Descent with early stopping for Softmax Regression",
    "text": "12. Batch Gradient Descent with early stopping for Softmax Regression\nExercise: Implement Batch Gradient Descent with early stopping for Softmax Regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset.\nLet’s start by loading the data. We will just reuse the Iris dataset we loaded earlier.\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"].values\n\nWe need to add the bias term for every instance (\\(x_0 = 1\\)). The easiest option to do this would be to use Scikit-Learn’s add_dummy_feature() function, but the point of this exercise is to get a better understanding of the algorithms by implementing them manually. So here is one possible implementation:\n\nX_with_bias = np.c_[np.ones(len(X)), X]\n\nThe easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn’s train_test_split() function, but again, we want to do it manually:\n\ntest_ratio = 0.2\nvalidation_ratio = 0.2\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nnp.random.seed(42)\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\nThe targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for any given instance is a one-hot vector). Let’s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance. To understand this code, you need to know that np.diag(np.ones(n)) creates an n×n matrix full of 0s except for 1s on the main diagonal. Moreover, if a is a NumPy array, then a[[1, 3, 2]] returns an array with 3 rows equal to a[1], a[3] and a[2] (this is advanced NumPy indexing).\n\ndef to_one_hot(y):\n    return np.diag(np.ones(y.max() + 1))[y]\n\nLet’s test this function on the first 10 instances:\n\ny_train[:10]\n\narray([1, 0, 2, 1, 1, 0, 1, 2, 1, 1])\n\n\n\nto_one_hot(y_train[:10])\n\narray([[0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.]])\n\n\nLooks good, so let’s create the target class probabilities matrix for the training set and the test set:\n\nY_train_one_hot = to_one_hot(y_train)\nY_valid_one_hot = to_one_hot(y_valid)\nY_test_one_hot = to_one_hot(y_test)\n\nNow let’s scale the inputs. We compute the mean and standard deviation of each feature on the training set (except for the bias feature), then we center and scale each feature in the training set, the validation set, and the test set:\n\nmean = X_train[:, 1:].mean(axis=0)\nstd = X_train[:, 1:].std(axis=0)\nX_train[:, 1:] = (X_train[:, 1:] - mean) / std\nX_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\nX_test[:, 1:] = (X_test[:, 1:] - mean) / std\n\nNow let’s implement the Softmax function. Recall that it is defined by the following equation:\n\\(\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}\\)\n\ndef softmax(logits):\n    exps = np.exp(logits)\n    exp_sums = exps.sum(axis=1, keepdims=True)\n    return exps / exp_sums\n\nWe are almost ready to start training. Let’s define the number of inputs and outputs:\n\nn_inputs = X_train.shape[1]  # == 3 (2 features plus the bias term)\nn_outputs = len(np.unique(y_train))  # == 3 (there are 3 iris classes)\n\nNow here comes the hardest part: training! Theoretically, it’s simple: it’s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it’s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it’s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won’t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what’s going on under the hood.\nSo the equations we will need are the cost function:\n\\(J(\\mathbf{\\Theta}) = - \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}\\)\nAnd the equation for the gradients:\n\\(\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}\\)\nNote that \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) may not be computable if \\(\\hat{p}_k^{(i)} = 0\\). So we will add a tiny value \\(\\epsilon\\) to \\(\\log\\left(\\hat{p}_k^{(i)}\\right)\\) to avoid getting nan values.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        print(epoch, xentropy_losses.sum(axis=1).mean())\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    Theta = Theta - eta * gradients\n\n0 3.7085808486476917\n1000 0.14519367480830647\n2000 0.13013095755040877\n3000 0.12009639326384532\n4000 0.11372961364786878\n5000 0.11002459532472424\n\n\nAnd that’s it! The Softmax model is trained. Let’s look at the model parameters:\n\nTheta\n\narray([[ 0.41931626,  6.11112089, -5.52429876],\n       [-6.53054533, -0.74608616,  8.33137102],\n       [-5.28115784,  0.25152675,  6.90680425]])\n\n\nLet’s make predictions for the validation set and check the accuracy score:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nWell, this model looks pretty ok. For the sake of the exercise, let’s add a bit of \\(\\ell_2\\) regularization. The following training code is similar to the one above, but the loss now has an additional \\(\\ell_2\\) penalty, and the gradients have the proper additional term (note that we don’t regularize the first element of Theta since this corresponds to the bias term). Also, let’s try increasing the learning rate eta.\n\neta = 0.5\nn_epochs = 5001\nm = len(X_train)\nepsilon = 1e-5\nalpha = 0.01  # regularization hyperparameter\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    if epoch % 1000 == 0:\n        Y_proba_valid = softmax(X_valid @ Theta)\n        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n        l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n        total_loss = xentropy_losses.sum(axis=1).mean() + alpha * l2_loss\n        print(epoch, total_loss.round(4))\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n1000 0.3259\n2000 0.3259\n3000 0.3259\n4000 0.3259\n5000 0.3259\n\n\nBecause of the additional \\(\\ell_2\\) penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let’s find out:\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nIn this case, the \\(\\ell_2\\) penalty did not change the test accuracy. Perhaps try fine-tuning alpha?\nNow let’s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing.\n\neta = 0.5\nn_epochs = 50_001\nm = len(X_train)\nepsilon = 1e-5\nC = 100  # regularization hyperparameter\nbest_loss = np.infty\n\nnp.random.seed(42)\nTheta = np.random.randn(n_inputs, n_outputs)\n\nfor epoch in range(n_epochs):\n    logits = X_train @ Theta\n    Y_proba = softmax(logits)\n    Y_proba_valid = softmax(X_valid @ Theta)\n    xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n    l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n    total_loss = xentropy_losses.sum(axis=1).mean() + 1 / C * l2_loss\n    if epoch % 1000 == 0:\n        print(epoch, total_loss.round(4))\n    if total_loss &lt; best_loss:\n        best_loss = total_loss\n    else:\n        print(epoch - 1, best_loss.round(4))\n        print(epoch, total_loss.round(4), \"early stopping!\")\n        break\n    error = Y_proba - Y_train_one_hot\n    gradients = 1 / m * X_train.T @ error\n    gradients += np.r_[np.zeros([1, n_outputs]), 1 / C * Theta[1:]]\n    Theta = Theta - eta * gradients\n\n0 3.7372\n281 0.3256\n282 0.3256 early stopping!\n\n\n\nlogits = X_valid @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_valid).mean()\naccuracy_score\n\n0.9333333333333333\n\n\nOh well, still no change in validation accuracy, but at least early stopping shortened training a bit.\nNow let’s plot the model’s predictions on the whole dataset (remember to scale all features fed to the model):\n\ncustom_cmap = mpl.colors.ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n\nx0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n                     np.linspace(0, 3.5, 200).reshape(-1, 1))\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new = (X_new - mean) / std\nX_new_with_bias = np.c_[np.ones(len(X_new)), X_new]\n\nlogits = X_new_with_bias @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=\"hot\")\nplt.clabel(contour, inline=1)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"upper left\")\nplt.axis([0, 7, 0, 3.5])\nplt.grid()\nplt.show()\n\n\n\n\nAnd now let’s measure the final model’s accuracy on the test set:\n\nlogits = X_test @ Theta\nY_proba = softmax(logits)\ny_predict = Y_proba.argmax(axis=1)\n\naccuracy_score = (y_predict == y_test).mean()\naccuracy_score\n\n0.9666666666666667\n\n\nWell we get even better performance on the test set. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary."
  },
  {
    "objectID": "about.html#key-probability-rules",
    "href": "about.html#key-probability-rules",
    "title": "Probability Theory and Random Variables",
    "section": "Key Probability Rules",
    "text": "Key Probability Rules\n1. Probability of any Event: The probability of an event A in the sample space S, P(A) cannot be negative or more than 1. The probability of all the events in the sample space S adds to 1.\n        P(A) ≥ 0\n\n        P(A) ≤ 1\n\n        P(S) = 1\n2. Addition Rule: The addition rule is a fundamental concept that deals with the probability of the union of two or more events. For two events A and B, the probability of their union (A ∪ B) is given by:\n        P(A ∪ B) = P(A) + P(B) − P(A ∩ B)\nThis formula accounts for the overlap between events A and B to avoid double-counting. The probability of the intersection P(A∩B) is subtracted to ensure accuracy.\n3. Bayes Rule: The multiplication rule governs the probability of the intersection of two events. For two events A and B, the probability of their intersection (A∩B) is given by:\n        P(A ∩ B) = P(A)⋅P(B∣A)\nHere, P(B∣A) represents the conditional probability of event B occurring given that event A has occurred. It expresses the probability of B within the context of A.\n4. Independence Rule: Two events, A and B, are considered independent if the occurrence (or non-occurrence) of one event has no influence on the probability of the other event.\n        P(A ∩ B) = P(A)⋅P(B)\n\n        P(A∣B) = P(A) and P(B∣A) = P(B)\nThese fundamental probability rules lay the groundwork for more complex probability calculations, enabling us to analyze and predict the likelihood of various outcomes in uncertain scenarios. They are essential tools for decision-making and risk assessment in diverse fields, from statistics to finance and beyond."
  },
  {
    "objectID": "about.html#types-of-probability",
    "href": "about.html#types-of-probability",
    "title": "Probability Theory and Random Variables",
    "section": "Types of Probability",
    "text": "Types of Probability\nProbability comes in various forms, each serving a specific purpose in different contexts. Here are three types of probability: classical, empirical, and subjective, along with real-world examples to illustrate each:\nClassical Probability\nClassical probability is based on equally likely outcomes in a sample space. It assumes that each outcome in the sample space is equally likely to occur.\nExample: Consider a fair coin. The sample space is {Heads, Tails}, and since the coin is fair, each outcome is equally likely. The probability of getting Heads or Tails is 0.5 or 50%.\nEmpircal Probability\nEmpirical probability is based on observed data. It involves calculating the probability of an event by analyzing data collected from actual experiments or observations.\nExample: Suppose you want to know the probability of rain in a particular city. Empirical probability would involve collecting data over time, noting the days with rain, and calculating the ratio of rainy days to the total number of days. If it rained on 30 out of 90 days, the empirical probability would be 0.333 or approximately 33.3%.\nSubjective Probability\nSubjective probability is based on personal judgment, beliefs, or opinions. It reflects an individual’s subjective assessment of the likelihood of an event.\nExample: A doctor might assign a subjective probability to the likelihood of a patient having a particular illness based on their experience, knowledge, and the patient’s symptoms. This probability is subjective and varies from one medical professional to another."
  },
  {
    "objectID": "about.html#probability-distributions",
    "href": "about.html#probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nIn probability theory, a probability distribution describes the likelihood of various outcomes in a sample space. It provides a way to model and analyze uncertainty by assigning probabilities to different events. Probability distributions are fundamental tools in probability theory, offering insights into the nature of random variables and helping us make predictions about their behavior. Probability distributions help quantify the uncertainty associated with different outcomes of a random variable. They provide a systematic way to express the likelihood of each possible value.\nProbability distributions form the basis for statistical inference. They allow us to make predictions about the population based on a sample and make informed decisions under uncertainty. Probability distributions are used to model various random processes in diverse fields such as physics, finance, biology, and engineering. They provide a mathematical framework to describe the probabilistic nature of real-world phenomena.\nUniform Distribution\nThe uniform distribution is characterized by all outcomes being equally likely. Each value within a specified range has the same probability of occurring. The probability density function (PDF) for a continuous uniform distribution over the interval [a,b] is given by:\n        f(x)= 1/b−a  for a ≤ x ≤ b\nConsider the example of rolling a fair six-sided die. Each face of the die has an equal chance of landing face up. If we assume the die is unbiased, the outcome of each roll follows a uniform distribution over the discrete values {1,2,3,4,5,6}. Each number has a probability of 1/6 of occurring.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import uniform\n\n# Define the range [a, b]\na = 1\nb = 6\n\n# Generate a sample of 1000 values from a uniform distribution\nsample = uniform.rvs(loc=a, scale=b-a, size=1000)\n\n# Plot the histogram of the sample\nplt.hist(sample, bins=20, density=True, alpha=0.7, color='blue')\n\n# Plot the probability density function (PDF)\nx = np.linspace(a, b, 100)\nplt.plot(x, uniform.pdf(x, loc=a, scale=b-a), 'r-', lw=2, label='Uniform PDF')\n\n# Add labels and a legend\nplt.xlabel('Outcome')\nplt.ylabel('Probability Density')\nplt.title('Uniform Distribution Example')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nBinomial Distribution\nThe binomial distribution models the number of successes in a fixed number of independent and identically distributed Bernoulli trials, where each trial has only two possible outcomes (usually termed as “success” and “failure”). The probability mass function (PMF) of a binomial distribution is given by:\n         P(X=k)=( n_p_k ) p^k (1-p)^(n-k)\n​ where: n is the number of trials, k is the number of successes, p is the probability of success on a single trial, and (1−p) is the probability of failure on a single trial.\nConsider the example of flipping a biased coin. Let’s say you have a coin that has a 60% chance of landing heads (success) and a 40% chance of landing tails (failure). If you flip this coin 5 times, you can model the number of heads obtained using a binomial distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\n# Define parameters\nn_trials = 5  # Number of coin flips\np_success = 0.6  # Probability of getting heads\n\n# Generate possible outcomes (0 to n_trials)\nx = np.arange(0, n_trials+1)\n\n# Calculate binomial probabilities for each outcome\nbinomial_pmf = binom.pmf(x, n_trials, p_success)\n\n# Plot the probability mass function (PMF)\nplt.stem(x, binomial_pmf, basefmt='b-', linefmt='b-', markerfmt='bo')\nplt.title('Binomial Distribution Example')\nplt.xlabel('Number of Heads')\nplt.ylabel('Probability')\nplt.show()\n\n\n\n\nGaussian Distribution\nThe Gaussian distribution, also known as the normal distribution, is a continuous probability distribution that is symmetric around its mean. It is characterized by its bell-shaped curve. The probability density function (PDF) of a normal distribution is given by:\nf(x;μ,σ)= 1/σroot(2π) e -1/3(x-mu)/sigma^2\nwhere: μ is the mean, σ is the standard deviation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Define parameters\nmean_height = 170  # Mean height in centimeters\nstd_deviation = 10  # Standard deviation in centimeters\n\n# Generate a sample of 1000 values from a normal distribution\nsample = np.random.normal(mean_height, std_deviation, 1000)\n\n# Plot the histogram of the sample\nplt.hist(sample, bins=30, density=True, alpha=0.7, color='blue')\n\n# Plot the probability density function (PDF)\nx = np.linspace(mean_height - 4*std_deviation, mean_height + 4*std_deviation, 100)\nplt.plot(x, norm.pdf(x, mean_height, std_deviation), 'r-', lw=2, label='Normal PDF')\n\n# Add labels and a legend\nplt.xlabel('Height (cm)')\nplt.ylabel('Probability Density')\nplt.title('Gaussian Distribution Example')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nPoisson Distribution\nThe Poisson distribution models the number of events that occur in a fixed interval of time or space. It is often used for rare events with a known average rate of occurrence. The probability mass function (PMF) of a Poisson distribution is given by:\nInsert formula\nInsert defining elements\nConsider a scenario where you are observing the number of customer arrivals at a store in a given hour, and you know that, on average, 5 customers arrive per hour. The Poisson distribution can be used to model the probability of observing a specific number of customer arrivals in that hour.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson\n\n# Define parameter\naverage_rate = 5  # Average number of events per interval\n\n# Generate possible outcomes (0 to 20 events)\nx = np.arange(0, 21)\n\n# Calculate Poisson probabilities for each outcome\npoisson_pmf = poisson.pmf(x, average_rate)\n\n# Plot the probability mass function (PMF)\nplt.stem(x, poisson_pmf, basefmt='b-', linefmt='b-', markerfmt='bo')\nplt.title('Poisson Distribution Example')\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\nplt.show()\n\n\n\n\nExponential Distribution\nThe exponential distribution models the time between events in a Poisson process, where events occur continuously and independently at a constant average rate. It is characterized by its memoryless property, meaning that the probability of an event occurring in the next time unit is independent of the past. The probability density function (PDF) of an exponential distribution is given by:\nInsert formula\nDefine elements\nConsider a scenario where customers arrive at a service point, and the time between successive arrivals follows an exponential distribution. This can be applied to model the inter-arrival times in a queue, the time between calls at a call center, or the time between arrivals of buses at a bus stop.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import expon\n\n# Define parameter\nrate_parameter = 0.5  # Average number of events per unit time\n\n# Generate a sample of 1000 values from an exponential distribution\nsample = expon.rvs(scale=1/rate_parameter, size=1000)\n\n# Plot the histogram of the sample\nplt.hist(sample, bins=30, density=True, alpha=0.7, color='blue')\n\n# Plot the probability density function (PDF)\nx = np.linspace(0, 5, 100)\nplt.plot(x, expon.pdf(x, scale=1/rate_parameter), 'r-', lw=2, label='Exponential PDF')\n\n# Add labels and a legend\nplt.xlabel('Time Between Events')\nplt.ylabel('Probability Density')\nplt.title('Exponential Distribution Example')\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "about.html#expected-value-and-variance",
    "href": "about.html#expected-value-and-variance",
    "title": "Probability Theory and Random Variables",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\nThe expected value of a discrete random variable X is a measure of the central tendency and is denoted by E(X). It is calculated as the sum of each possible value of X multiplied by its probability.\nInsert Formula\nAdd an example in die setting\n\n# Define the sample space and probabilities for each outcome\noutcomes = [1, 2, 3, 4, 5, 6]\nprobabilities = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n\n# Calculate the expected value\nexpected_value = sum(x * p for x, p in zip(outcomes, probabilities))\nprint(\"Expected Value:\", expected_value)\n\nExpected Value: 3.5\n\n\nFor a continuous random variable X, the expected value is the integral of x times the probability density function (PDF).\nInsert Formula\nConsider a continuous random variable X with a uniform distribution over the interval [2,8].\n\nfrom scipy.integrate import quad\n\n# Define the PDF for the uniform distribution\ndef uniform_pdf(x):\n    return 1/6 if 2 &lt;= x &lt;= 8 else 0\n\n# Calculate the expected value\nexpected_value_continuous, _ = quad(lambda x: x * uniform_pdf(x), 2, 8)\nprint(\"Expected Value (Continuous):\", expected_value_continuous)\n\nExpected Value (Continuous): 5.0\n\n\nVariance measures the spread or variability of a random variable. For a discrete random variable X, the variance Var(X) is calculated as the sum of the squared differences between each value and the mean, weighted by their probabilities.\nInsert Formula\nAdd an example in a die setting\n\n# Calculate the variance\nvariance = sum((x - expected_value)**2 * p for x, p in zip(outcomes, probabilities))\nprint(\"Variance:\", variance)\n\nVariance: 2.9166666666666665\n\n\nFor a continuous random variable X, the variance is the integral of (x−E(X))^2 times the PDF.\nInsert Formula\nConsider a continuous random variable X with a uniform distribution over the interval [2,8].\n\n# Calculate the variance\nvariance_continuous, _ = quad(lambda x: (x - expected_value_continuous)**2 * uniform_pdf(x), 2, 8)\nprint(\"Variance (Continuous):\", variance_continuous)\n\nVariance (Continuous): 3.0"
  },
  {
    "objectID": "about.html#joint-probability-distributions",
    "href": "about.html#joint-probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Joint Probability Distributions",
    "text": "Joint Probability Distributions\nA joint probability distribution describes the simultaneous behavior of two or more random variables. It provides the probabilities for every possible combination of values that the random variables can take.\nConsider two six-sided dice, X and Y, representing the outcomes of two independent rolls. The joint probability distribution is a table indicating the probability of each possible pair of outcomes (x,y).\n\nimport numpy as np\n\n# Define the sample space and calculate joint probabilities\nsample_space_X = [1, 2, 3, 4, 5, 6]\nsample_space_Y = [1, 2, 3, 4, 5, 6]\n\njoint_probabilities = np.zeros((6, 6))\n\nfor i, x in enumerate(sample_space_X):\n    for j, y in enumerate(sample_space_Y):\n        joint_probabilities[i, j] = 1/36  # Since each outcome is equally likely\n\n# Display the joint probability distribution\nprint(\"Joint Probability Distribution:\")\nprint(joint_probabilities)\n\nJoint Probability Distribution:\n[[0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]]"
  },
  {
    "objectID": "about.html#marginal-probability-distributions",
    "href": "about.html#marginal-probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Marginal Probability Distributions",
    "text": "Marginal Probability Distributions\nMarginal probability distributions focus on the probabilities of individual random variables without considering the others. In the context of joint distributions, these are obtained by summing or integrating over the values of the other variables.\nConsider two six-sided dice, X and Y, representing the outcomes of two independent rolls. The joint probability distribution is a table indicating the probability of each possible pair of outcomes (x,y). Let’s calculate the marginal distributions of X and Y.\n\n# Calculate marginal probabilities\nmarginal_X = np.sum(joint_probabilities, axis=1)\nmarginal_Y = np.sum(joint_probabilities, axis=0)\n\n# Display the marginal probability distributions\nprint(\"\\nMarginal Probability Distribution of X:\")\nprint(marginal_X)\n\nprint(\"\\nMarginal Probability Distribution of Y:\")\nprint(marginal_Y)\n\n\nMarginal Probability Distribution of X:\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n\nMarginal Probability Distribution of Y:\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]"
  },
  {
    "objectID": "blog1.html",
    "href": "blog1.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "From deciding whether to carry an umbrella to predicting stock market trends, uncertainty surrounds us, shaping our decisions, actions, and perceptions. It is this pervasive uncertainty that beckons us to explore the realm of probability theory, a powerful tool that empowers us to navigate the unpredictable nature of the world.\nEnter probability theory, a mathematical framework designed to bring order to the chaos of uncertainty. Probability theory provides us with a systematic way to quantify and analyze uncertainty, offering a language to express the likelihood of different outcomes. It serves as a compass, guiding us through the fog of unpredictability and enabling us to make informed decisions in the face of ambiguity.\nAt its core, probability theory explores the likelihood of events occurring in various situations. It equips us with the means to assign numerical values to the uncertainty inherent in any scenario, allowing us to make reasoned predictions and choices. Whether predicting the outcome of a dice roll or estimating the probability of a rare disease occurrence, probability theory provides the analytical tools essential for decision-making in uncertain environments.\nIn the following exploration of probability theory and random variables, we will unravel the intricacies of this indispensable field, delving into its fundamental concepts, applications in real life, and the ways it shapes our understanding of uncertainty. Join us on this journey as we unveil the mathematical underpinnings that empower us to confront the unpredictable with confidence and insight."
  },
  {
    "objectID": "blog1.html#key-probability-rules",
    "href": "blog1.html#key-probability-rules",
    "title": "Probability Theory and Random Variables",
    "section": "Key Probability Rules",
    "text": "Key Probability Rules\n1. Probability of any Event: The probability of an event A in the sample space S, P(A) cannot be negative or more than 1. The probability of all the events in the sample space S adds to 1. \\[\n            P(A) ≥ 0\n\\] \\[\n            P(A) ≤ 1\n\\] \\[\n            P(S) = 1\n\\]\n2. Addition Rule: The addition rule is a fundamental concept that deals with the probability of the union of two or more events. For two events A and B, the probability of their union (A ∪ B) is given by: \\[\n            P(A ∪ B) = P(A) + P(B) − P(A ∩ B)\n\\] This formula accounts for the overlap between events A and B to avoid double-counting. The probability of the intersection P(A∩B) is subtracted to ensure accuracy.\n3. Bayes Rule: The multiplication rule governs the probability of the intersection of two events. For two events A and B, the probability of their intersection (A∩B) is given by: \\[\n            P(A ∩ B) = P(A)⋅P(B∣A)\n\\] Here, P(B∣A) represents the conditional probability of event B occurring given that event A has occurred. It expresses the probability of B within the context of A.\n4. Independence Rule: Two events, A and B, are considered independent if the occurrence (or non-occurrence) of one event has no influence on the probability of the other event.\n\\[            \n            P(A ∩ B) = P(A)⋅P(B)\n\\] \\[\n            P(A∣B) = P(A)\n\\] \\[\n            P(B∣A) = P(B)\n\\]\nThese fundamental probability rules lay the groundwork for more complex probability calculations, enabling us to analyze and predict the likelihood of various outcomes in uncertain scenarios. They are essential tools for decision-making and risk assessment in diverse fields, from statistics to finance and beyond."
  },
  {
    "objectID": "blog1.html#types-of-probability",
    "href": "blog1.html#types-of-probability",
    "title": "Probability Theory and Random Variables",
    "section": "Types of Probability",
    "text": "Types of Probability\nProbability comes in various forms, each serving a specific purpose in different contexts. Here are three types of probability: classical, empirical, and subjective, along with real-world examples to illustrate each:\nClassical Probability\nClassical probability is based on equally likely outcomes in a sample space. It assumes that each outcome in the sample space is equally likely to occur.\nExample: Consider a fair coin. The sample space is {Heads, Tails}, and since the coin is fair, each outcome is equally likely. The probability of getting Heads or Tails is 0.5 or 50%.\nEmpircal Probability\nEmpirical probability is based on observed data. It involves calculating the probability of an event by analyzing data collected from actual experiments or observations.\nExample: Suppose you want to know the probability of rain in a particular city. Empirical probability would involve collecting data over time, noting the days with rain, and calculating the ratio of rainy days to the total number of days. If it rained on 30 out of 90 days, the empirical probability would be 0.333 or approximately 33.3%.\nSubjective Probability\nSubjective probability is based on personal judgment, beliefs, or opinions. It reflects an individual’s subjective assessment of the likelihood of an event.\nExample: A doctor might assign a subjective probability to the likelihood of a patient having a particular illness based on their experience, knowledge, and the patient’s symptoms. This probability is subjective and varies from one medical professional to another."
  },
  {
    "objectID": "blog1.html#probability-distributions",
    "href": "blog1.html#probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nIn probability theory, a probability distribution describes the likelihood of various outcomes in a sample space. It provides a way to model and analyze uncertainty by assigning probabilities to different events. Probability distributions are fundamental tools in probability theory, offering insights into the nature of random variables and helping us make predictions about their behavior. Probability distributions help quantify the uncertainty associated with different outcomes of a random variable. They provide a systematic way to express the likelihood of each possible value.\nProbability distributions form the basis for statistical inference. They allow us to make predictions about the population based on a sample and make informed decisions under uncertainty. Probability distributions are used to model various random processes in diverse fields such as physics, finance, biology, and engineering. They provide a mathematical framework to describe the probabilistic nature of real-world phenomena.\nUniform Distribution\nThe uniform distribution is characterized by all outcomes being equally likely. Each value within a specified range has the same probability of occurring. The probability density function (PDF) for a continuous uniform distribution over the interval [a,b] is given by:\n\\[\n        f(x) = 1/(b−a)\n\\] \\[\n        a ≤ x ≤ b\n\\]\nConsider the example of rolling a fair six-sided die. Each face of the die has an equal chance of landing face up. If we assume the die is unbiased, the outcome of each roll follows a uniform distribution over the discrete values {1,2,3,4,5,6}. Each number has a probability of 1/6 of occurring.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import uniform\n\n# Define the range [a, b]\na = 1\nb = 6\n\n# Generate a sample of 1000 values from a uniform distribution\nsample = uniform.rvs(loc=a, scale=b-a, size=1000)\n\n# Plot the histogram of the sample\nplt.hist(sample, bins=20, density=True, alpha=0.7, color='blue')\n\n# Plot the probability density function (PDF)\nx = np.linspace(a, b, 100)\nplt.plot(x, uniform.pdf(x, loc=a, scale=b-a), 'r-', lw=2, label='Uniform PDF')\n\n# Add labels and a legend\nplt.xlabel('Outcome')\nplt.ylabel('Probability Density')\nplt.title('Uniform Distribution Example')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nBinomial Distribution\nThe binomial distribution models the number of successes in a fixed number of independent and identically distributed Bernoulli trials, where each trial has only two possible outcomes (usually termed as “success” and “failure”). The probability mass function (PMF) of a binomial distribution is given by:\n\\[          \n             P(X=k )={n \\choose k} p^{k} (1-p)^{(n-k)}\n​\\]\nwhere: \\(n\\) is the number of trials, \\(k\\) is the number of successes, \\(p\\) is the probability of success on a single trial, and \\((1−p)\\) is the probability of failure on a single trial.\nConsider the example of flipping a biased coin. Let’s say you have a coin that has a 60% chance of landing heads (success) and a 40% chance of landing tails (failure). If you flip this coin 5 times, you can model the number of heads obtained using a binomial distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\n# Define parameters\nn_trials = 5  # Number of coin flips\np_success = 0.6  # Probability of getting heads\n\n# Generate possible outcomes (0 to n_trials)\nx = np.arange(0, n_trials+1)\n\n# Calculate binomial probabilities for each outcome\nbinomial_pmf = binom.pmf(x, n_trials, p_success)\n\n# Plot the probability mass function (PMF)\nplt.stem(x, binomial_pmf, basefmt='b-', linefmt='b-', markerfmt='bo')\nplt.title('Binomial Distribution Example')\nplt.xlabel('Number of Heads')\nplt.ylabel('Probability')\nplt.show()\n\n\n\n\nGaussian Distribution\nThe Gaussian distribution, also known as the normal distribution, is a continuous probability distribution that is symmetric around its mean. It is characterized by its bell-shaped curve. The probability density function (PDF) of a normal distribution is given by:\n\\[\nf(x |\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\, e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\\]\nwhere: \\(\\mu\\) is the mean, \\(\\sigma\\) is the standard deviation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Define parameters\nmean_height = 170  # Mean height in centimeters\nstd_deviation = 10  # Standard deviation in centimeters\n\n# Generate a sample of 1000 values from a normal distribution\nsample = np.random.normal(mean_height, std_deviation, 1000)\n\n# Plot the histogram of the sample\nplt.hist(sample, bins=30, density=True, alpha=0.7, color='blue')\n\n# Plot the probability density function (PDF)\nx = np.linspace(mean_height - 4*std_deviation, mean_height + 4*std_deviation, 100)\nplt.plot(x, norm.pdf(x, mean_height, std_deviation), 'r-', lw=2, label='Normal PDF')\n\n# Add labels and a legend\nplt.xlabel('Height (cm)')\nplt.ylabel('Probability Density')\nplt.title('Gaussian Distribution Example')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nPoisson Distribution\nThe Poisson distribution models the number of events that occur in a fixed interval of time or space. It is often used for rare events with a known average rate of occurrence. The probability mass function (PMF) of a Poisson distribution is given by:\n\\[          \n             f(x) = \\lambda^{x}* \\exp(-\\lambda)/x!\n\\]\nThe Poisson distribution is characterized by a single parameter, often denoted as λ (lambda), which represents the average rate of occurrence of the event per unit of time or space. Consider a scenario where you are observing the number of customer arrivals at a store in a given hour, and you know that, on average, 5 customers arrive per hour. The Poisson distribution can be used to model the probability of observing a specific number of customer arrivals in that hour.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson\n\n# Define parameter\naverage_rate = 5  # Average number of events per interval\n\n# Generate possible outcomes (0 to 20 events)\nx = np.arange(0, 21)\n\n# Calculate Poisson probabilities for each outcome\npoisson_pmf = poisson.pmf(x, average_rate)\n\n# Plot the probability mass function (PMF)\nplt.stem(x, poisson_pmf, basefmt='b-', linefmt='b-', markerfmt='bo')\nplt.title('Poisson Distribution Example')\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\nplt.show()\n\n\n\n\nExponential Distribution\nThe exponential distribution models the time between events in a Poisson process, where events occur continuously and independently at a constant average rate. It is characterized by its memoryless property, meaning that the probability of an event occurring in the next time unit is independent of the past. The probability density function (PDF) of an exponential distribution is given by:\n\\[\nf(x | \\lambda) = \\lambda \\, e^{-\\lambda x}\n\\]\nThe \\(\\lambda\\) is the rate parameter, and it determines the rate at which events occur. It is also equal to the reciprocal of the mean\nConsider a scenario where customers arrive at a service point, and the time between successive arrivals follows an exponential distribution. This can be applied to model the inter-arrival times in a queue, the time between calls at a call center, or the time between arrivals of buses at a bus stop.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import expon\n\n# Define parameter\nrate_parameter = 0.5  # Average number of events per unit time\n\n# Generate a sample of 1000 values from an exponential distribution\nsample = expon.rvs(scale=1/rate_parameter, size=1000)\n\n# Plot the histogram of the sample\nplt.hist(sample, bins=30, density=True, alpha=0.7, color='blue')\n\n# Plot the probability density function (PDF)\nx = np.linspace(0, 5, 100)\nplt.plot(x, expon.pdf(x, scale=1/rate_parameter), 'r-', lw=2, label='Exponential PDF')\n\n# Add labels and a legend\nplt.xlabel('Time Between Events')\nplt.ylabel('Probability Density')\nplt.title('Exponential Distribution Example')\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "blog1.html#expected-value-and-variance",
    "href": "blog1.html#expected-value-and-variance",
    "title": "Probability Theory and Random Variables",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\nThe expected value of a discrete random variable X is a measure of the central tendency and is denoted by E(X). It is calculated as the sum of each possible value of X multiplied by its probability.\n\\[\nE[X] = \\sum_{x} x \\cdot P(X=x)\n\\]\nLet’s consider the example of rolling a fair six-sided die. The possible outcomes are the numbers 1 through 6, each with a probability of 1/6 since the die is fair. The random variable X represents the outcome of a single roll of the die. The expectation \\(E[X]\\) is calculated as the sum of each possible outcome multiplied by its probability\n\\[\nE[X] = \\sum_{i=1}^{6} x_i \\cdot P(X=x_i) = \\frac{1}{6} \\cdot 1 + \\frac{1}{6} \\cdot 2 + \\frac{1}{6} \\cdot 3 + \\frac{1}{6} \\cdot 4 + \\frac{1}{6} \\cdot 5 + \\frac{1}{6} \\cdot 6\n\\]\n\n# Define the sample space and probabilities for each outcome\noutcomes = [1, 2, 3, 4, 5, 6]\nprobabilities = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n\n# Calculate the expected value\nexpected_value = sum(x * p for x, p in zip(outcomes, probabilities))\nprint(\"Expected Value:\", expected_value)\n\nExpected Value: 3.5\n\n\nFor a continuous random variable X, the expected value is the integral of x times the probability density function (PDF).\n\\[\nE[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nConsider a continuous random variable X with a uniform distribution over the interval [2,8].\n\nfrom scipy.integrate import quad\n\n# Define the PDF for the uniform distribution\ndef uniform_pdf(x):\n    return 1/6 if 2 &lt;= x &lt;= 8 else 0\n\n# Calculate the expected value\nexpected_value_continuous, _ = quad(lambda x: x * uniform_pdf(x), 2, 8)\nprint(\"Expected Value (Continuous):\", expected_value_continuous)\n\nExpected Value (Continuous): 5.0\n\n\nVariance measures the spread or variability of a random variable. For a discrete random variable X, the variance Var(X) is calculated as the sum of the squared differences between each value and the mean, weighted by their probabilities.\n\\[\n\\text{Var}(X) = \\sum_{i} (x_i - \\mu)^2 \\cdot P(X = x_i)\n\\]\n\n# Calculate the variance\nvariance = sum((x - expected_value)**2 * p for x, p in zip(outcomes, probabilities))\nprint(\"Variance:\", variance)\n\nVariance: 2.9166666666666665\n\n\nFor a continuous random variable X, the variance is the integral of (x−E(X))^2 times the PDF.\n\\[\n\\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 \\cdot f(x) \\, dx\n\\]\nConsider a continuous random variable X with a uniform distribution over the interval [2,8].\n\n# Calculate the variance\nvariance_continuous, _ = quad(lambda x: (x - expected_value_continuous)**2 * uniform_pdf(x), 2, 8)\nprint(\"Variance (Continuous):\", variance_continuous)\n\nVariance (Continuous): 3.0"
  },
  {
    "objectID": "blog1.html#joint-probability-distributions",
    "href": "blog1.html#joint-probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Joint Probability Distributions",
    "text": "Joint Probability Distributions\nA joint probability distribution describes the simultaneous behavior of two or more random variables. It provides the probabilities for every possible combination of values that the random variables can take.\nConsider two six-sided dice, X and Y, representing the outcomes of two independent rolls. The joint probability distribution is a table indicating the probability of each possible pair of outcomes (x,y).\n\nimport numpy as np\n\n# Define the sample space and calculate joint probabilities\nsample_space_X = [1, 2, 3, 4, 5, 6]\nsample_space_Y = [1, 2, 3, 4, 5, 6]\n\njoint_probabilities = np.zeros((6, 6))\n\nfor i, x in enumerate(sample_space_X):\n    for j, y in enumerate(sample_space_Y):\n        joint_probabilities[i, j] = 1/36  # Since each outcome is equally likely\n\n# Display the joint probability distribution\nprint(\"Joint Probability Distribution:\")\nprint(joint_probabilities)\n\nJoint Probability Distribution:\n[[0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]\n [0.02777778 0.02777778 0.02777778 0.02777778 0.02777778 0.02777778]]"
  },
  {
    "objectID": "blog1.html#marginal-probability-distributions",
    "href": "blog1.html#marginal-probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Marginal Probability Distributions",
    "text": "Marginal Probability Distributions\nMarginal probability distributions focus on the probabilities of individual random variables without considering the others. In the context of joint distributions, these are obtained by summing or integrating over the values of the other variables.\nConsider two six-sided dice, X and Y, representing the outcomes of two independent rolls. The joint probability distribution is a table indicating the probability of each possible pair of outcomes (x,y). Let’s calculate the marginal distributions of X and Y.\n\n# Calculate marginal probabilities\nmarginal_X = np.sum(joint_probabilities, axis=1)\nmarginal_Y = np.sum(joint_probabilities, axis=0)\n\n# Display the marginal probability distributions\nprint(\"\\nMarginal Probability Distribution of X:\")\nprint(marginal_X)\n\nprint(\"\\nMarginal Probability Distribution of Y:\")\nprint(marginal_Y)\n\n\nMarginal Probability Distribution of X:\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n\nMarginal Probability Distribution of Y:\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]"
  },
  {
    "objectID": "about.html#bayes-theorem",
    "href": "about.html#bayes-theorem",
    "title": "Probability Theory and Random Variables",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nBayes’ Theorem is a powerful tool derived from conditional probability. It allows us to update the probability of a hypothesis based on new evidence. The formula is as follows:\nInsert formula P(A∣B) = P(B∣A)⋅P(A)/P(B) Where: P(A∣B) is the posterior probability (probability of A given B). P(B∣A) is the likelihood (probability of B given A). P(A) is the prior probability (probability of A). P(B) is the marginal likelihood (probability of B).\nExample:\nLet’s consider a simple example where we want to predict the probability of a student passing an exam (event A) given that they attended a study session (event B). We have the following probabilities:\nP(A)=0.7 (prior probability of passing) P(B∣A)=0.8 (likelihood of attending a study session given passing) P(B∣¬A)=0.4 (likelihood of attending a study session given not passing)\n\n# Define probabilities\nP_A = 0.7  # Prior probability of passing\nP_B_given_A = 0.8  # Likelihood of attending a study session given passing\nP_B_given_not_A = 0.4  # Likelihood of attending a study session given not passing\n\n# Calculate marginal likelihood P(B)\nP_B = P_B_given_A * P_A + P_B_given_not_A * (1 - P_A)\n\n# Calculate posterior probability P(A|B) using Bayes' Theorem\nP_A_given_B = (P_B_given_A * P_A) / P_B\n\nprint(f\"The probability of passing given attending a study session is: {P_A_given_B:.2f}\")\n\nThe probability of passing given attending a study session is: 0.82\n\n\nIn this example, we use Bayes’ Theorem to update our prior belief (probability of passing) based on new evidence (attending a study session). The calculated posterior probability gives us an updated estimate of the likelihood of passing given the observed evidence.\nThis demonstrates how conditional probability and Bayes’ Theorem are applied in machine learning for making predictions and updating beliefs based on new information."
  },
  {
    "objectID": "blog3.html#linear-regression",
    "href": "blog3.html#linear-regression",
    "title": "Linear and Non-Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear Regression is a supervised machine learning algorithm used for predicting a continuous outcome variable (dependent variable) based on one or more predictor variables (independent variables). The basic idea is to find the best-fit straight line that minimizes the difference between the observed and predicted values.\n\nSimple Linear Regression\nFor a simple linear regression with one independent variable:\n\\[\n    y = \\beta_0 + \\beta_1x + \\epsilon\n\\]\n\\(y\\): Dependent variable (the variable we want to predict) \\(x\\): Independent variable (predictor variable) \\(\\beta_0\\): Intercept (y-intercept), the value of y when x=0 \\(\\beta_1\\): Slope (gradient), represents the change in y for a unit change in x \\(\\epsilon\\): Error term, represents the unobserved factors affecting y\nThe objective is to minimize the sum of squared differences between the observed (y) and predicted (y^) values:\n\\[\n\\text{Minimize: } J = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n\\]\n\\(m\\): Number of data points \\(\\hat{y}_i\\): Predicted value for the i-th data point \\(y_i\\): Observed value for the i-th data point\nGradient Descent is commonly used to find the values of β0 and β1 that minimize the cost function J. The update rule is:\n\\[\n\\beta_j := \\beta_j - \\alpha \\frac{1}{n} \\sum_{i=1}^{n} (h_\\theta(x_i) - y_i) \\cdot x_{ij}\n\\]\n\\(\\alpha\\) is the learning rate.\nWe choose the “Advertising” dataset from Kaggle where we want to analyze the relationship between “TV Advertising” and “Sales”.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Load the dataset\ndata = pd.read_csv('advertising.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n\n\n\n\n\nWe perform exploratory data analysis (EDA) on the data to understand the characteristics of the data, unveil patterns, detect anomalies, and gather insights that can guide subsequent analyses and modeling. We perform data wrangling, where the important steps include data cleaning by removal null-values and outliers.\n\ndata.describe()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n147.042500\n23.264000\n30.554000\n15.130500\n\n\nstd\n85.854236\n14.846809\n21.778621\n5.283892\n\n\nmin\n0.700000\n0.000000\n0.300000\n1.600000\n\n\n25%\n74.375000\n9.975000\n12.750000\n11.000000\n\n\n50%\n149.750000\n22.900000\n25.750000\n16.000000\n\n\n75%\n218.825000\n36.525000\n45.100000\n19.050000\n\n\nmax\n296.400000\n49.600000\n114.000000\n27.000000\n\n\n\n\n\n\n\nOutliers in a dataset are often identified as points that fall beyond a specified distance from the edges of the box and whiskers. This distance is typically determined by a multiplier of the IQR, and data points beyond this range are considered potential outliers.\n\nimport seaborn as sns\n# Checking for outliers\nsns.boxplot(data['Sales'])\nplt.show()\n\n\n\n\nThen we look at the feature interdependence using pairplots and the heatmap of the correlation between the different variables.\n\n# Sales relation with other variables using scatter plot.\nsns.pairplot(data, x_vars=['TV', 'Newspaper', 'Radio'], y_vars='Sales', height=4, aspect=1, kind='scatter')\nplt.show()\n# correlation between different variables.\nsns.heatmap(data.corr(), cmap=\"YlGnBu\", annot = True)\nplt.show()\n\n\n\n\n\n\n\nAs is visible from the pairplot and the heatmap, the variable TV seems to be most correlated with Sales. So let’s go ahead and perform simple linear regression using TV as our feature variable.\nWe need to split our variable into training and testing sets. We’ll accomplish this by utilizing the train_test_split function from the sklearn.model_selection library. It’s customary to allocate 70% of the data to our training dataset, leaving the remaining 30% for the test dataset. By default, we fit a line on the dataset that passes through the origin using the statsmodels library. However, to introduce an intercept, we must manually utilize the add_constant attribute of statsmodels. Once we’ve added the constant to our X_train dataset, we can proceed to fit a regression line using the OLS (Ordinary Least Squares) attribute of statsmodels, as demonstrated below.\n\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\n\nX = data['TV']\ny = data['Sales']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)\n\n# Adding a constant to get an intercept\nX_train_sm = sm.add_constant(X_train)\n\n# Fitting the resgression\nlr = sm.OLS(y_train, X_train_sm).fit()\n\nprint(lr.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.816\nModel:                            OLS   Adj. R-squared:                  0.814\nMethod:                 Least Squares   F-statistic:                     611.2\nDate:                Wed, 06 Dec 2023   Prob (F-statistic):           1.52e-52\nTime:                        07:17:21   Log-Likelihood:                -321.12\nNo. Observations:                 140   AIC:                             646.2\nDf Residuals:                     138   BIC:                             652.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          6.9487      0.385     18.068      0.000       6.188       7.709\nTV             0.0545      0.002     24.722      0.000       0.050       0.059\n==============================================================================\nOmnibus:                        0.027   Durbin-Watson:                   2.196\nProb(Omnibus):                  0.987   Jarque-Bera (JB):                0.150\nSkew:                          -0.006   Prob(JB):                        0.928\nKurtosis:                       2.840   Cond. No.                         328.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe key metrics in the summary that we note to see if the Linear Regression is a good fit are the following,\n\nR-squared: Indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. A higher R-squared value suggests a better fit.\np-value: The p-value associated with the t-statistic. The t-statistic tests the null hypothesis that the coefficient is equal to zero. A high absolute t-value and a low associated p-value suggest that the variable is significant. A low p-value (typically less than 0.05) suggests that the variable is statistically significant.\n\nWe can see from the above metrics that the fit is significant. So we visualize how well our model has fit the data by plotting the Line given by the fitted slope and intercept and see how it fits our data’s scatter plot.\n\nplt.scatter(X_train, y_train)\nplt.plot(X_train, 6.948 + 0.054*X_train, 'r',label=\"Predictions\")\nplt.xlabel('TV Advertising')\nplt.ylabel('Sales')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\nThe fit of the Linear Regression model can also be tested by seeing if the residual errors are normally distributed with zero mean and unit variance. We can see below that the residual distribution is normal as expected.\n\ny_train_pred = lr.predict(X_train_sm)\nres = (y_train - y_train_pred)\nfig = plt.figure()\nsns.displot(res, bins = 15, kde=True)                # Plot heading \nplt.xlabel('Residual')         # X-label\nplt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\nThese models need to be also evaluated on a test set that they were not exposed to while training, to understand if there is any overfitting or underfitting that has occurred due to bias and variance.\n\nX_test_sm = sm.add_constant(X_test)\n\n# Predict the y values corresponding to X_test_sm\ny_pred = lr.predict(X_test_sm)\n\nplt.scatter(X_test, y_test)\nplt.plot(X_test, 6.948 + 0.054 * X_test, 'r', label=\"Predictions\")\nplt.legend(loc=\"upper left\")\nplt.xlabel('TV Advertising')\nplt.ylabel('Sales')\nplt.show()\n\n\n\n\nWe can see that our model performs well on the test set as well, making it robust and generalizable to new data that it has not been exposed during training process.\n\n\nRegularized Linear Regression Models\nLasso Regression, or L1 regularization, is a linear regression technique that includes a penalty term in the cost function equivalent to the absolute values of the coefficients. This penalty encourages sparsity in the model, meaning it tends to force some of the coefficient estimates to be exactly zero. The regularization term is controlled by a hyperparameter, usually denoted as α. A higher α leads to a stronger regularization effect. Lasso regression is particularly useful when dealing with datasets with a large number of features, as it can automatically perform feature selection by setting some coefficients to zero, effectively ignoring less relevant predictors.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\n\nX = data['TV']\ny = data['Sales']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)\n\n# Ordinary Least Squares (OLS)\nols_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\nols_pred = ols_model.predict(sm.add_constant(X_test))\n\n# Lasso Regression with statsmodels\nlasso_model = sm.OLS(y_train, sm.add_constant(X_train)).fit_regularized(alpha=0.01, L1_wt=1)  # L1_wt=1 for Lasso\nlasso_pred = lasso_model.predict(sm.add_constant(X_test))\nprint(lasso_model.params)\n\nconst    6.913375\nTV       0.054717\ndtype: float64\n\n\nRidge Regression, or L2 regularization, is another variant of linear regression that includes a penalty term proportional to the squared values of the coefficients in the cost function. Similar to Lasso, Ridge introduces regularization controlled by a hyperparameter α. Ridge tends to shrink the coefficients towards zero but rarely sets them exactly to zero. It is effective in mitigating the issue of multicollinearity, where predictor variables are highly correlated. Ridge can stabilize the model and prevent it from being too sensitive to variations in the input data.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso, Ridge\n\nX = data['TV']\ny = data['Sales']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)\n\n# Ridge Regression with statsmodels\nridge_model = sm.OLS(y_train, sm.add_constant(X_train)).fit_regularized(alpha=0.01, L1_wt=0)  # L1_wt=0 for Ridge\nridge_pred = ridge_model.predict(sm.add_constant(X_test))\nprint(ridge_model.params)\n\n[6.71059051 0.05570333]\n\n\n\n# Compare performance using Mean Squared Error\nmse_ols = np.mean((y_test - ols_pred)**2)\nmse_lasso = np.mean((y_test - lasso_pred)**2)\nmse_ridge = np.mean((y_test - ridge_pred)**2)\n\nprint(f'Mean Squared Error (OLS): {mse_ols}')\nprint(f'Mean Squared Error (Lasso): {mse_lasso}')\nprint(f'Mean Squared Error (Ridge): {mse_ridge}')\n\n# Visualizing actual vs predicted values\nplt.figure(figsize=(12, 6))\n\n# Plot for OLS\nplt.subplot(1, 3, 1)\nplt.scatter(X_test, y_test)\nplt.plot(X_test, 6.948+ X_test*0.054, 'r', label=\"Predictions\")\nplt.title('OLS: Actual vs Predicted')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\n\n# Plot for Lasso\nplt.subplot(1, 3, 2)\nplt.scatter(X_test, y_test)\nplt.plot(X_test, 6.913 + X_test * 0.054, 'r', label=\"Predictions\")\nplt.title('Lasso: Actual vs Predicted')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\n\n# Plot for Ridge\nplt.subplot(1, 3, 3)\nplt.scatter(X_test, y_test)\nplt.plot(X_test, 6.710 + 0.055 * X_test, 'r', label=\"Predictions\")\nplt.title('Ridge: Actual vs Predicted')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\n\nplt.tight_layout()\nplt.show()\n\nMean Squared Error (OLS): 4.077556371826953\nMean Squared Error (Lasso): 4.08097078389324\nMean Squared Error (Ridge): 4.109361428273981\n\n\n\n\n\nIn this example, We’ve used the fit_regularized method from statsmodels with the L1 penalty (L1_wt=1) for Lasso regression and the L2 penalty (L1_wt=0) for Ridge regression. The performance is then compared using Mean Squared Error, and the scatter plots visualize the actual vs predicted values for each model. We can adjust the regularization strength (alpha) as needed, we are able to see the MSE is least for OLS case itself."
  },
  {
    "objectID": "blog3.html#non-linear-regression",
    "href": "blog3.html#non-linear-regression",
    "title": "Linear and Non-Linear Regression",
    "section": "Non-Linear Regression",
    "text": "Non-Linear Regression\nNon-linear regression allows for more flexibility in modeling relationships that are not linear. The model equation is more complex and may involve non-linear functions, such as exponentials, logarithms, polynomials, or trigonometric functions. This flexibility enables non-linear regression to better represent curved or intricate patterns in the data. Non-linear regression models are particularly useful when the relationship between variables is better described by a curve, wave, or other non-linear shapes.\n\nPolynomial Regression\nPolynomial regression is an extension of linear regression, allowing for the modeling of relationships that are not strictly linear. While linear regression assumes a linear relationship between the independent and dependent variables, polynomial regression accommodates curves and non-linear patterns. In polynomial regression, the relationship is represented by a polynomial equation, allowing for more flexibility in capturing complex patterns within the data.\nThe polynomial regression equation of degree n is given by: \\[\ny = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_n x^n + \\epsilon\n\\] Here, \\(y\\) is the dependent variable, \\(x\\) is the independent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1,\\beta_2,…,\\beta_n\\) are the coefficients, \\(x^n\\) represents the terms with increasing powers of and \\(\\epsilon\\) is the error term.\nThe coefficients are estimated from the data using methods like the method of least squares. We create a synthetic polynomial dataset to see how well the regressor is able to fit the polynomial function we have defined. We can see that the scatter plot below shows the data to be quadratic in nature as defined by our function y.\n\nnp.random.seed(42)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\n\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([-3, 3, 0, 10])\nplt.grid()\nplt.show()\n\n\n\n\nThis example generates a quadratic dataset and fits a second-degree polynomial using scikit-learn. We can adjust the degree parameter in PolynomialFeatures to experiment with different polynomial degrees. We need to expand the features by adding columns for X2,X3,…,Xn up to the desired degree n. We use a linear regression algorithm to fit the polynomial equation to the expanded polynomial features. This involves estimating the coefficients b1,b2,…,bn that minimize the sum of squared differences between the observed and predicted values.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([1.78134581]), array([[0.93366893, 0.56456263]]))\n\n\nOnce the model is trained, we use it to make predictions on new or unseen data. We can assess the performance of the model using appropriate metrics such as Mean Squared Error (MSE) or R-squared. We can visualize the fitted polynomial curve along with the data points to understand how well the model captures the underlying patterns.\n\nfrom sklearn.metrics import r2_score\n\nX_new = np.linspace(-3, 3, 100).reshape(100, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = lin_reg.predict(X_new_poly)\n\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \"b.\")\nplt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.legend(loc=\"upper left\")\nplt.axis([-3, 3, 0, 10])\nplt.grid()\nplt.show()\n\n\n\n\nThe choice of the degree n is crucial. A too high degree may lead to overfitting, capturing noise in the data rather than the actual trend. Here we test for three different degrees [1,2,100] and see how it changes the fit and how well its able to capture the noisy points and outliers.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nplt.figure(figsize=(6, 4))\n\nfor style, width, degree in ((\"r-+\", 2, 1), (\"b--\", 2, 2), (\"g-\", 1, 100)):\n    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n    std_scaler = StandardScaler()\n    lin_reg = LinearRegression()\n    polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\n    polynomial_regression.fit(X, y)\n    y_newbig = polynomial_regression.predict(X_new)\n    label = f\"{degree} degree{'s' if degree &gt; 1 else ''}\"\n    plt.plot(X_new, y_newbig, style, label=label, linewidth=width)\n\nplt.plot(X, y, \"b.\", linewidth=3)\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([-3, 3, 0, 10])\nplt.grid()\nplt.show()\n\n\n\n\nLinear regression is a fundamental tool for modeling relationships between variables through a linear equation. Lasso and Ridge regression, extensions of linear regression, introduce regularization to address issues like multicollinearity and overfitting. Lasso promotes sparsity, while Ridge penalizes large coefficients. Non-linear regression accommodates complex relationships. Linear regression is versatile, Lasso/Ridge are valuable for feature selection and regularization, and non-linear regression suits intricate data patterns. Applications include predicting house prices (linear), genomics (Lasso/Ridge), and modeling complex processes (non-linear). The choice depends on data characteristics and analysis goals."
  },
  {
    "objectID": "blog2.html",
    "href": "blog2.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering in machine learning is a technique used to group similar data points based on certain features or characteristics. The primary goal is to find patterns, structures, or natural groupings within the data without explicit labeling. Unlike supervised learning where the algorithm is trained on labeled data, clustering is an unsupervised learning approach that aims to discover inherent structures in the absence of predefined categories.\nClustering is crucial for identifying hidden patterns and structures within datasets. By organizing data points into clusters, the algorithm can reveal similarities and dissimilarities, making it easier to understand complex relationships within the data. This aids in pattern recognition, helping analysts and researchers make sense of large and unstructured datasets. Clustering plays a pivotal role in exploratory data analysis. It allows data scientists to gain insights into the distribution of data and discover natural groupings, facilitating the identification of trends, outliers, and anomalies. Clustering is particularly valuable when dealing with high-dimensional data, enabling the extraction of meaningful information from intricate datasets."
  },
  {
    "objectID": "blog2.html#partitioning-clustering",
    "href": "blog2.html#partitioning-clustering",
    "title": "Clustering",
    "section": "Partitioning Clustering:",
    "text": "Partitioning Clustering:\nPartitioning algorithms divide the dataset into distinct non-overlapping subsets or clusters. The most well-known example is K-Means. Partitioning methods are computationally efficient and suitable for large datasets, making them widely used in practice.\n\nKMeans\nKMeans is a popular unsupervised machine learning algorithm used for clustering. The goal of KMeans is to partition a dataset into K clusters, where each data point belongs to the cluster with the nearest mean. It’s an iterative algorithm that aims to minimize the sum of squared distances between data points and the centroid of their assigned cluster.\nGiven a dataset with n data points \\({x_1, x_2,…,x_n}\\) in d-dimensional space and \\(K\\) clusters, the goal is to minimize Inertia, also known as the within-cluster sum of squares, is the metric KMeans aims to minimize. It quantifies the compactness of the clusters. A lower inertia indicates that the data points in each cluster are closer to their centroid.\nIt is given by the following objective function:\n\\[\n\\text{Minimize: } J = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\lVert x_{ij} - \\mu_j \\rVert^2\n\\]\nWhere: \\(J\\) is the sum of squared distances (inertia) between data points and their assigned cluster centroids. \\(\\lVert x_{ij} - \\mu_j \\rVert^2\\) is the Inertia, the squared Euclidean distance between data point \\(x_i\\) and cluster centroid \\(μ_j\\).\nThe KMeans algorithm can be described as follows,\n\nInitialization: Randomly select K data points from the dataset as initial cluster centroids.\nAssignment: Assign each data point to the cluster whose centroid is closest.\nUpdate Centroids: Recalculate the centroids as the mean of all data points in the cluster.\nRepeat: Repeat steps 2 and 3 until convergence (when centroids no longer change significantly) or a specified number of iterations is reached.\n\nThe centroids can be initialized in different ways and can impact the performance of the algorithm, convergence and the final results. Common methods include,\n\nRandom Initialization: Randomly select K data points as initial centroids.\nKMeans++ Initialization: A smarter initialization method that spreads initial centroids apart to improve convergence speed.\n\nCentroids initialization is a crucial step, as poor initialization may lead to suboptimal solutions or slow convergence. KMeans++ is often preferred for better performance.\nWe will implement KMeans clustering on the Mall Customers dataset and discuss the different steps along with visualizations of the data.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\n# Load the Mall Customer Segmentation Data\nmall_data = pd.read_csv('mall_customers.csv')\n\n# Display the first few rows of the dataset\nprint(mall_data.head())\n\n   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n0           1    Male   19                  15                      39\n1           2    Male   21                  15                      81\n2           3  Female   20                  16                       6\n3           4  Female   23                  16                      77\n4           5  Female   31                  17                      40\n\n\nNow let’s perform exploratory data analysis (EDA) on the data to understand the underlying data distribution and outliers if any. We load the Mall Customer Segmentation dataset and conduct a preliminary exploration. The info() method provides information about the dataset, including the data types and missing values. The describe() method offers summary statistics, such as mean, standard deviation, minimum, and maximum values. The pairplot from Seaborn generates scatterplots for all pairs of features, differentiated by gender. This aids in understanding the relationships between different features and identifying potential patterns.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Display basic information about the dataset\nprint(mall_data.info())\n\n# Summary statistics\nprint(mall_data.describe())\n\n# Visualize the distribution of features\nsns.pairplot(mall_data, hue='Gender', palette='viridis')\nplt.show()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\nNone\n       CustomerID         Age  Annual Income (k$)  Spending Score (1-100)\ncount  200.000000  200.000000          200.000000              200.000000\nmean   100.500000   38.850000           60.560000               50.200000\nstd     57.879185   13.969007           26.264721               25.823522\nmin      1.000000   18.000000           15.000000                1.000000\n25%     50.750000   28.750000           41.500000               34.750000\n50%    100.500000   36.000000           61.500000               50.000000\n75%    150.250000   49.000000           78.000000               73.000000\nmax    200.000000   70.000000          137.000000               99.000000\n\n\n\n\n\nFeature scaling ensures that all features contribute equally to the clustering process. In this step, we select the ‘Annual Income (k$)’ and ‘Spending Score (1-100)’ columns, which are relevant for clustering. The StandardScaler is then used to standardize (normalize) these features, transforming them to have zero mean and unit variance. This step is crucial for KMeans, as it is sensitive to the scale of features.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Select relevant features\nX_mall = mall_data.iloc[:, [3, 4]]\n\n# Standardize the features\nscaler_mall = StandardScaler()\nX_mall_scaled = scaler_mall.fit_transform(X_mall)\n\nThe Elbow method helps us determine the optimal number of clusters (K). It involves running KMeans with different values of K and plotting the inertia (within-cluster sum of squares) against the number of clusters. The “elbow” in the plot represents a point where the rate of decrease of inertia slows down, suggesting an optimal value for K.\n\nfrom sklearn.cluster import KMeans\n\n# Finding the optimal number of clusters (k) using the Elbow method\ninertia_mall = []\nfor i in range(1, 11):\n    kmeans_mall = KMeans(n_clusters=i, random_state=42)\n    kmeans_mall.fit(X_mall_scaled)\n    inertia_mall.append(kmeans_mall.inertia_)\n\n# Plot the Elbow method\nplt.plot(range(1, 11), inertia_mall, marker='o')\nplt.title('Elbow Method for Optimal k (Mall Data)')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.show()\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\nHaving determined the optimal K, we apply the KMeans algorithm to the standardized features. The fit method calculates the clusters and assigns each data point to a cluster. The resulting cluster labels are added to the original dataset. To understand the impact of different initializations of centroids we visualize the clusters with ‘KMeans++’ initialization, random initiliazation and custom initialization.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the Mall Customer Segmentation Data\nmall_data = pd.read_csv('mall_customers.csv')\n\n# Select relevant features\nX_mall = mall_data.iloc[:, [3, 4]]\n\n# Standardize the features\nscaler_mall = StandardScaler()\nX_mall_scaled = scaler_mall.fit_transform(X_mall)\n\n# Number of clusters (you can choose the optimal number)\nn_clusters = 5\n\n# KMeans with default 'k-means++' initialization\nkmeans_default = KMeans(n_clusters=n_clusters, random_state=42)\nkmeans_default.fit(X_mall_scaled)\n\n# KMeans with 'random' initialization\nkmeans_random = KMeans(n_clusters=n_clusters, init='random', random_state=42)\nkmeans_random.fit(X_mall_scaled)\n\n# KMeans with custom initialization (you can provide your own array of centroids)\ncustom_initialization = [[-1, -1], [0, 0], [1, 1], [2, 2], [3, 3]]\nkmeans_custom = KMeans(n_clusters=n_clusters, init=custom_initialization, random_state=42)\nkmeans_custom.fit(X_mall_scaled)\n\n# Visualize the clusters using a scatter plot\nplt.figure(figsize=(12, 8))\n\n# Plot data points with cluster colors for default initialization\nplt.subplot(2, 2, 1)\nplt.scatter(X_mall_scaled[:, 0], X_mall_scaled[:, 1], c=kmeans_default.labels_, cmap='viridis')\nplt.scatter(kmeans_default.cluster_centers_[:, 0], kmeans_default.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title('KMeans with KMeans++ Initialization')\nplt.legend()\n\n# Plot data points with cluster colors for random initialization\nplt.subplot(2, 2, 2)\nplt.scatter(X_mall_scaled[:, 0], X_mall_scaled[:, 1], c=kmeans_random.labels_, cmap='viridis')\nplt.scatter(kmeans_random.cluster_centers_[:, 0], kmeans_random.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title('KMeans with Random Initialization')\nplt.legend()\n\n# Plot data points with cluster colors for custom initialization\nplt.subplot(2, 2, 3)\nplt.scatter(X_mall_scaled[:, 0], X_mall_scaled[:, 1], c=kmeans_custom.labels_, cmap='viridis')\nplt.scatter(kmeans_custom.cluster_centers_[:, 0], kmeans_custom.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title('KMeans with Custom Initialization')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: RuntimeWarning:\n\nExplicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n\n\n\n\n\n\nWe find that KMeans++ is the best way to initialize the centroids and retain that for our further analysis.\nHaving determined the optimal K, we apply the KMeans algorithm to the standardized features. The fit method calculates the clusters and assigns each data point to a cluster. The resulting cluster labels are added to the original dataset.\n\n# Apply KMeans with the optimal number of clusters\nkmeans_mall = KMeans(n_clusters=5, random_state=42)\nkmeans_mall.fit(X_mall_scaled)\n\n# Add cluster labels to the dataset\nmall_data['cluster'] = kmeans_mall.labels_\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nVisualizing the clusters aids in understanding the grouping of data points. The scatter plot depicts the ‘Annual Income’ against ‘Spending Score’, with points colored according to their assigned clusters. Additionally, the centroids of each cluster are marked in red, providing a central point of reference for each group.\nA Voronoi diagram is a geometric representation that divides a plane into regions based on the proximity to a set of seed points. Each region, known as a Voronoi cell, contains all points closer to a specific seed point than to any other point in the set. Voronoi diagrams offer a visually intuitive and spatially clear depiction of cluster boundaries, emphasizing the central points of clusters. They are particularly effective for revealing the geometric relationships between data points and cluster centroids, making them valuable for understanding the distribution of clusters in two-dimensional space.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\n# Load the Mall Customer Segmentation Data\nmall_data = pd.read_csv('mall_customers.csv')\n\n# Select relevant features\nX_mall = mall_data.iloc[:, [3, 4]]\n\n# Standardize the features\nscaler_mall = StandardScaler()\nX_mall_scaled = scaler_mall.fit_transform(X_mall)\n\n# Apply KMeans with the optimal number of clusters\nn_clusters = 5\nkmeans_mall = KMeans(n_clusters=n_clusters, random_state=42)\nkmeans_mall.fit(X_mall_scaled)\n\n# Calculate Voronoi diagram\nvor = Voronoi(kmeans_mall.cluster_centers_)\n\n# Visualize the Voronoi diagram\nplt.figure(figsize=(10, 6))\n\n# Plot Voronoi diagram\nvoronoi_plot_2d(vor, show_vertices=False, show_line_segments=False)\n\n# Overlay scatter plot on Voronoi diagram with all data points\nplt.scatter(X_mall_scaled[:, 0], X_mall_scaled[:, 1], c=kmeans_mall.labels_, cmap='viridis', edgecolors='k', linewidths=0.5, label='Data Points')\nplt.scatter(kmeans_mall.cluster_centers_[:, 0], kmeans_mall.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n\nplt.title('Voronoi Diagram with Overlaying Scatter Plot')\nplt.xlabel('Standardized Annual Income')\nplt.xlim([-2,3])\nplt.ylabel('Standardized Spending Score')\nplt.ylim([-2,2])\nplt.legend()\nplt.show()\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\nThe voronoi_plot_2d function from scipy.spatial is used to plot the Voronoi diagram. The centroids are marked in red, and each data point is colored based on its assigned cluster. The Voronoi diagram outlines the regions corresponding to each cluster. This visualization provides a clear representation of how the Voronoi diagram defines the boundaries between different clusters. Each region in the Voronoi diagram corresponds to the area where the points are closer to a specific centroid than to any other centroid.\nIn this step, we interpret the results by examining the characteristics of each cluster. Grouping the dataset by cluster labels and calculating the mean of each feature provides insights into the average behavior of customers within each cluster. This analysis can guide marketing strategies or help identify target customer segments based on spending patterns and annual income.\n\n\nAccelerated KMeans & Mini-Batch KMeans\nAccelerated KMeans, often referred to as the Elkan algorithm, is an optimization of the classic Lloyd’s algorithm used in the standard KMeans clustering. The key idea is to reduce the number of distance computations required during each iteration, making the algorithm more efficient.\nIn standard KMeans, the algorithm calculates distances between all data points and cluster centroids for each iteration. Accelerated KMeans introduces bounds and triangular inequalities to avoid unnecessary distance calculations. By skipping certain calculations, it reduces the overall computational cost and speeds up the convergence of the algorithm.\nMini-Batch K-Means is an optimization of the standard KMeans algorithm designed to handle large datasets more efficiently. Instead of using the entire dataset to update centroids at each iteration, Mini-Batch K-Means randomly selects a subset or “mini-batch” of the data. This mini-batch is used to update centroids, making the algorithm faster but introducing a level of stochasticity.\n\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the Mall Customer Segmentation Data\nmall_data = pd.read_csv('mall_customers.csv')\n\n# Select relevant features\nX_mall = mall_data.iloc[:, [3, 4]]\n\n# Standardize the features\nscaler_mall = StandardScaler()\nX_mall_scaled = scaler_mall.fit_transform(X_mall)\n\n# Number of clusters\nn_clusters = 5\n\n# Standard KMeans\nstart_time = time.time()\nkmeans_standard = KMeans(n_clusters=n_clusters, random_state=42)\nkmeans_standard.fit(X_mall_scaled)\nstandard_time = time.time() - start_time\n\n# Accelerated KMeans (Elkan)\nstart_time = time.time()\nkmeans_accelerated = KMeans(n_clusters=n_clusters, algorithm='elkan', random_state=42)\nkmeans_accelerated.fit(X_mall_scaled)\naccelerated_time = time.time() - start_time\n\n# Mini-Batch K-Means\nstart_time = time.time()\nkmeans_mini_batch = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)\nkmeans_mini_batch.fit(X_mall_scaled)\nmini_batch_time = time.time() - start_time\n\n# Visualize the clusters using a scatter plot\nplt.figure(figsize=(15, 6))\n\n# Standard KMeans\nplt.subplot(1, 3, 1)\nplt.scatter(X_mall_scaled[:, 0], X_mall_scaled[:, 1], c=kmeans_standard.labels_, cmap='viridis')\nplt.scatter(kmeans_standard.cluster_centers_[:, 0], kmeans_standard.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title(f'Standard KMeans\\nExecution Time: {accelerated_time:.4f}s')\nplt.xlabel('Standardized Annual Income')\nplt.ylabel('Standardized Spending Score')\nplt.legend()\n\n# Accelerated KMeans (Elkan)\nplt.subplot(1, 3, 2)\nplt.scatter(X_mall_scaled[:, 0], X_mall_scaled[:, 1], c=kmeans_accelerated.labels_, cmap='viridis')\nplt.scatter(kmeans_accelerated.cluster_centers_[:, 0], kmeans_accelerated.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title(f'Accelerated KMeans\\nExecution Time: {standard_time:.4f}s')\nplt.xlabel('Standardized Annual Income')\nplt.ylabel('Standardized Spending Score')\nplt.legend()\n\n# Mini-Batch K-Means\nplt.subplot(1, 3, 3)\nplt.scatter(X_mall_scaled[:, 0], X_mall_scaled[:, 1], c=kmeans_mini_batch.labels_, cmap='viridis')\nplt.scatter(kmeans_mini_batch.cluster_centers_[:, 0], kmeans_mini_batch.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title(f'Mini-Batch K-Means\\nExecution Time: {mini_batch_time:.4f}s')\nplt.xlabel('Standardized Annual Income')\nplt.ylabel('Standardized Spending Score')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/home/priya/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning:\n\nThe default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\nLet’s compare the three algorithms: Standard KMeans, Accelerated KMeans (Elkan), and Mini-Batch K-Means, using the Mall Customers dataset. We’ll measure the execution time for each algorithm and visualize the resulting clusters. We can see that the execution time reduced for Accelarated KMeans and Mini-Batch KMeans, with Mini-Batch KMeans having significantly lower execution time than the standard KMeans algorithm."
  },
  {
    "objectID": "blog2.html#hierarchical-clustering",
    "href": "blog2.html#hierarchical-clustering",
    "title": "Clustering",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering builds a tree-like hierarchy of clusters, either by merging smaller clusters into larger ones (agglomerative) or by recursively splitting clusters into smaller ones (divisive). Hierarchical clustering provides a visual representation of the data’s hierarchical structure, allowing for an intuitive interpretation of cluster relationships.\nAgglomerative Hierarchical Clustering is a bottom-up approach to cluster analysis. It starts by treating each data point as a separate cluster and, at each iteration, merges the closest clusters based on a defined distance metric. This process continues until only one cluster, representing all data points, remains. The result is often visualized using a dendrogram, which displays the hierarchical structure of the clusters.\nInitialization: Start with each data point as an individual cluster. Iteration: 1. Find the two clusters that are closest to each other based on the chosen linkage method and distance metric. 2. Merge these clusters into a new cluster. 3. Update the distance matrix to reflect the newly formed cluster. Stopping Criteria: Continue the iteration until only one cluster remains.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Load the Mall Customer Segmentation Data\nmall_data = pd.read_csv('mall_customers.csv')\n\n# Select relevant features\nX_mall = mall_data.iloc[:, [3, 4]]\n\n# Perform Agglomerative Hierarchical Clustering with a different linkage method\nagglomerative_cluster_mall = AgglomerativeClustering(n_clusters=5, linkage='ward')\nagglomerative_labels_mall = agglomerative_cluster_mall.fit_predict(X_mall)\n\n# Visualize the dendrogram\nlinkage_matrix_mall = linkage(X_mall, method='ward')\nplt.figure(figsize=(12, 6))\ndendrogram(linkage_matrix_mall)\nplt.title('Hierarchical Clustering Dendrogram (Mall Customer Segmentation)')\nplt.xlabel('Customers')\nplt.ylabel('Distance')\nplt.show()\n\n# Visualize the clusters using a scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(X_mall.iloc[:, 0], X_mall.iloc[:, 1], c=agglomerative_labels_mall, cmap='viridis', label='Clusters')\nplt.title('Agglomerative Hierarchical Clustering (Mall Customer Segmentation)')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend(title='Clusters')\nplt.show()\n\n\n\n\n\n\n\nThis dataset contains information about the annual income and spending score of mall customers, making it suitable for exploring natural groupings based on these features. The dendrogram and scatter plot provide insights into the hierarchical structure and resulting clusters of the data."
  },
  {
    "objectID": "blog2.html#density-based-clustering",
    "href": "blog2.html#density-based-clustering",
    "title": "Clustering",
    "section": "Density-Based Clustering",
    "text": "Density-Based Clustering\nDensity-based algorithms, such as DBSCAN (Density-Based Spatial Clustering of Applications with Noise), identify clusters based on regions of higher data point density. Density-based clustering is effective in identifying clusters of varying shapes and sizes and is robust to noise and outliers. Unlike K-Means, DBSCAN doesn’t assume that clusters have a spherical shape or a specific number of clusters in the data. Instead, it identifies regions with high point density as clusters and areas of lower density as noise.\nThe main parameters for DBSCAN are as follows,\nEpsilon \\(\\epsilon\\): The maximum distance between two samples for them to be considered as in the same neighborhood. It determines the radius around a data point.\nMinimum Samples (min_samples): The number of samples (or total weight) in a neighborhood for a data point to be considered as a core point. A core point is a data point that has at least “min_samples” data points within its epsilon neighborhood.\nThe different data points can be grouped into the following categories,\nCore Points: Points with at least min_samples points within an epsilon neighborhood. Form the “dense” regions.\nBorder Points: Points within the epsilon neighborhood of a core point but with fewer than min_samples neighbors. Part of the cluster but less central.\nNoise Points: Points that are neither core nor border points. Considered outliers.\nDensity Reachability: A point A is density-reachable from point B if there is a chain of points P1, P2,…,Pn, where P1=B and Pn=A, such that each Pi+1 is density-reachable from P i.\nThe algorithm can be detailed in the following steps,\nInitialize:\n\nChoose ε and min_samples parameters, where ε is the maximum distance between two samples for one to be considered as in the neighborhood of the other and min_samples is the number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\nMark all points as unvisited.\nSelect a Random Unvisited Point: If the point is a core point, start a new cluster and expand it by adding all directly density-reachable points.\nIf the point is not a core point, mark it as noise.\nContinue the process until all points are visited.\n\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)\n\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\ndbscan2 = DBSCAN(eps=0.2)\ndbscan2.fit(X)\n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_dbscan(dbscan, X, size=100)\n\nplt.subplot(122)\nplot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n\nplt.show()\n\n\n\n\nDBSCAN is advantageous over other clustering methods as it does not assume any specific cluster shape. It can find clusters of arbitrary shapes. It is also very robust to outliers as the noise points are identified naturally. DBSCAN performs automatic determination of cluster numbers and does not require specifying the number of clusters in advance. However, it is very sensitive to the parameters and need to be fine-tuned to get good performance."
  },
  {
    "objectID": "blog2.html#gaussian-mixture-models",
    "href": "blog2.html#gaussian-mixture-models",
    "title": "Clustering",
    "section": "Gaussian Mixture Models",
    "text": "Gaussian Mixture Models\nGaussian Mixture Models represent a probabilistic model for representing the presence of subpopulations within an overall population. It assumes that the data is generated by a mixture of several Gaussian distributions with unknown parameters.\nIt uses a mixture of several Gaussian distributions. The probability density function (PDF) for a GMM is given by:\n\\[\nf(x) = \\sum_{k=1}^{K} \\pi_k \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} \\exp\\left(-\\frac{(x - \\mu_k)^2}{2\\sigma_k^2}\\right)\n\\]\nπk is the weight of the k-th Gaussian component, representing the proportion of data points assigned to that component. \\(\\mu_k\\) is the mean vector of the k-th Gaussian component, \\(\\sigma_k\\) is the covariance matrix of the k-th Gaussian component. \\(f\\) is the Gaussian distribution function. The goal of GMM is to estimate the parameters \\(\\mu_k\\), and \\(\\sigma_k\\) that maximize the likelihood of the observed data.\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\n\nX1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\nX1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\nX2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\nX2 = X2 + [6, -8]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\ngm = GaussianMixture(n_components=3, n_init=10, random_state=42)\ngm.fit(X)\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z,\n                 norm=LogNorm(vmin=1.0, vmax=30.0),\n                 levels=np.logspace(0, 2, 12))\n    plt.contour(xx, yy, Z,\n                norm=LogNorm(vmin=1.0, vmax=30.0),\n                levels=np.logspace(0, 2, 12),\n                linewidths=1, colors='k')\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.show()\n\n\n\n\nBayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) are statistical metrics used for model selection, particularly in the context of Gaussian Mixture Models (GMM) and other probabilistic models. The AIC is a measure of the relative quality of a statistical model for a given set of data. It balances the goodness of fit of the model with the simplicity of the model (to avoid overfitting). Similar to AIC, BIC is used for model selection and balances goodness of fit with model complexity. However, BIC imposes a stronger penalty for models with more parameters."
  },
  {
    "objectID": "blog1.html#bayes-theorem",
    "href": "blog1.html#bayes-theorem",
    "title": "Probability Theory and Random Variables",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nBayes’ Theorem is a powerful tool derived from conditional probability. It allows us to update the probability of a hypothesis based on new evidence. The formula is as follows:\n\\[\nP(A∣B) = P(B∣A)⋅P(A)/P(B)\n\\]\nWhere: \\(P(A∣B)\\) is the posterior probability (probability of A given B). \\(P(B∣A)\\) is the likelihood (probability of B given A). \\(P(A)\\) is the prior probability (probability of A). \\(P(B)\\) is the marginal likelihood (probability of B).\nExample:\nLet’s consider a simple example where we want to predict the probability of a student passing an exam (event A) given that they attended a study session (event B). We have the following probabilities:\nP(A)=0.7 (prior probability of passing) P(B∣A)=0.8 (likelihood of attending a study session given passing) P(B∣¬A)=0.4 (likelihood of attending a study session given not passing)\n\n# Define probabilities\nP_A = 0.7  # Prior probability of passing\nP_B_given_A = 0.8  # Likelihood of attending a study session given passing\nP_B_given_not_A = 0.4  # Likelihood of attending a study session given not passing\n\n# Calculate marginal likelihood P(B)\nP_B = P_B_given_A * P_A + P_B_given_not_A * (1 - P_A)\n\n# Calculate posterior probability P(A|B) using Bayes' Theorem\nP_A_given_B = (P_B_given_A * P_A) / P_B\n\nprint(f\"The probability of passing given attending a study session is: {P_A_given_B:.2f}\")\n\nThe probability of passing given attending a study session is: 0.82\n\n\nIn this example, we use Bayes’ Theorem to update our prior belief (probability of passing) based on new evidence (attending a study session). The calculated posterior probability gives us an updated estimate of the likelihood of passing given the observed evidence.\nThis demonstrates how conditional probability and Bayes’ Theorem are applied in machine learning for making predictions and updating beliefs based on new information."
  },
  {
    "objectID": "blog4.html#multi-class-classification",
    "href": "blog4.html#multi-class-classification",
    "title": "Classification",
    "section": "Multi-Class Classification",
    "text": "Multi-Class Classification\nMulti-class classification is a machine learning task where the goal is to classify instances into one of three or more classes or categories. Unlike binary classification, which involves distinguishing between only two classes, multi-class classification extends this concept to scenarios where there are multiple classes.\nA Decision Tree is a powerful and interpretable machine learning algorithm used for both classification and regression tasks. It works by recursively partitioning the input space based on the features, ultimately creating a tree-like structure where each leaf node represents a class label or a regression value. In the context of classification, I’ll explain the Decision Tree algorithm and its mathematical formulation.\nGini Impurity (for Classification): Given a node with K classes, the Gini impurity (G) is calculated as:\n\\[\n\\text{Gini}(S) = 1 - \\sum_{i=1}^{K} p_i^2\n\\]\nHere, \\(p_i\\) is the probability of class i in the node.\nInformation Gain (for Feature Selection): The information gain (IG) measures the reduction in entropy (or increase in purity) after a dataset is split on a particular feature. Higher information gain indicates a better feature for splitting.\n\\[\n\\text{IG}(D, A) = \\text{Impurity}(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} \\cdot \\text{Impurity}(D_v)\n\\]\nDecision Rule: At each node, a decision rule is formed based on the selected feature and a threshold. For example, “If petal length ≤ 2.45 cm, go left; otherwise, go right.”\nThe Decision Tree algorithm is defined as follows,\n\nFeature Selection: The algorithm selects the best feature to split the data at each node. The “best” feature is chosen based on criteria like Gini impurity or information gain.\nNode Splitting: The selected feature is used to split the dataset into subsets. Each subset is associated with a branch emanating from the node.\nRecursive Process: The splitting process is applied recursively to each subset, creating child nodes. This continues until a stopping criterion is met, such as a maximum depth or a minimum number of samples per leaf.\nLeaf Node Assignment: Each leaf node is assigned a class label based on the majority class of instances in that node.\n\nTo illustrate, let’s use the famous Iris dataset for multiclass classification. The goal is to predict the species of iris flowers based on features like sepal length, sepal width, petal length, and petal width. The code example in Python using scikit-learn demonstrates a simple decision tree classifier for this task.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\niris_data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\niris_data['target'] = iris.target\niris_data['species'] = iris.target_names[iris.target]\n\n# Display the first few rows of the dataset\nprint(iris_data.head())\n\n# Plot the distribution of the target variable (Species)\nsns.countplot(x='species', data=iris_data)\nplt.title('Distribution of Species')\nplt.show()\n\n# Pair plot to visualize relationships between features\nsns.pairplot(iris_data, hue='species', markers=[\"o\", \"s\", \"D\"])\nplt.suptitle('Pair Plot of Iris Dataset', y=1.02)\nplt.show()\n\n# Box plot for each feature by Species\nplt.figure(figsize=(15, 8))\nfor i, feature in enumerate(iris.feature_names):\n    plt.subplot(2, 2, i + 1)\n    sns.boxplot(x='species', y=feature, data=iris_data)\n    plt.title(f'Box Plot of {feature} by Species')\nplt.tight_layout()\nplt.show()\n\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                5.1               3.5                1.4               0.2   \n1                4.9               3.0                1.4               0.2   \n2                4.7               3.2                1.3               0.2   \n3                4.6               3.1                1.5               0.2   \n4                5.0               3.6                1.4               0.2   \n\n   target species  \n0       0  setosa  \n1       0  setosa  \n2       0  setosa  \n3       0  setosa  \n4       0  setosa  \n\n\n\n\n\n\n\n\n\n\n\nThe boxplots show that there are no outliers that we need to remove. We are able to visualize the relationship between different features through the pairplots. We are able to see that the classes or different species are uniformly distributed in the dataset.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.DataFrame(iris.target, columns=['species'])\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the Decision Tree Classifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\ny_prob = model.predict_proba(X_test)  # Decision Trees don't have predict_proba for multi-class, so use decision_function for ROC\n\n# Calculate precision, recall, and F1 score\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\n# Print precision, recall, and F1 score\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Calculate ROC curve and AUC\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(3):  # Three classes in Iris dataset\n    fpr[i], tpr[i], _ = roc_curve((y_test == i).astype(int), y_prob[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 8))\nfor i in range(3):\n    plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n\n# Generate and display the confusion matrix with blue color\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', linewidths=.5, square=True)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\nPrecision: 1.0000\nRecall: 1.0000\nF1 Score: 1.0000\n\n\n\n\n\n\n\n\nWe can see that our model has high accuracy in predicting the output class and also exhibits, high levels of True Postive and True Negative rates, and low levels of False Positve and False Negative rates resulting in higher precision, recall and F1 scores as well."
  },
  {
    "objectID": "blog4.html#measuring-accuracy-using-cross-validation",
    "href": "blog4.html#measuring-accuracy-using-cross-validation",
    "title": "Classification",
    "section": "Measuring Accuracy Using Cross-Validation",
    "text": "Measuring Accuracy Using Cross-Validation\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n\narray([0.95035, 0.96035, 0.9604 ])\n\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\n\nskfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is not\n                                       # already shuffled\nfor train_index, test_index in skfolds.split(X_train, y_train_5):\n    clone_clf = clone(sgd_clf)\n    X_train_folds = X_train[train_index]\n    y_train_folds = y_train_5[train_index]\n    X_test_fold = X_train[test_index]\n    y_test_fold = y_train_5[test_index]\n\n    clone_clf.fit(X_train_folds, y_train_folds)\n    y_pred = clone_clf.predict(X_test_fold)\n    n_correct = sum(y_pred == y_test_fold)\n    print(n_correct / len(y_pred))\n\n0.95035\n0.96035\n0.9604\n\n\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier()\ndummy_clf.fit(X_train, y_train_5)\nprint(any(dummy_clf.predict(X_train)))\n\nFalse\n\n\n\ncross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n\narray([0.90965, 0.90965, 0.90965])"
  },
  {
    "objectID": "blog4.html#confusion-matrix",
    "href": "blog4.html#confusion-matrix",
    "title": "Classification",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_train_5, y_train_pred)\ncm\n\narray([[53892,   687],\n       [ 1891,  3530]])\n\n\n\ny_train_perfect_predictions = y_train_5  # pretend we reached perfection\nconfusion_matrix(y_train_5, y_train_perfect_predictions)\n\narray([[54579,     0],\n       [    0,  5421]])"
  },
  {
    "objectID": "blog4.html#precision-and-recall",
    "href": "blog4.html#precision-and-recall",
    "title": "Classification",
    "section": "Precision and Recall",
    "text": "Precision and Recall\n\nfrom sklearn.metrics import precision_score, recall_score\n\nprecision_score(y_train_5, y_train_pred)  # == 3530 / (687 + 3530)\n\n0.8370879772350012\n\n\n\n# extra code – this cell also computes the precision: TP / (FP + TP)\ncm[1, 1] / (cm[0, 1] + cm[1, 1])\n\n0.8370879772350012\n\n\n\nrecall_score(y_train_5, y_train_pred)  # == 3530 / (1891 + 3530)\n\n0.6511713705958311\n\n\n\n# extra code – this cell also computes the recall: TP / (FN + TP)\ncm[1, 1] / (cm[1, 0] + cm[1, 1])\n\n0.6511713705958311\n\n\n\nfrom sklearn.metrics import f1_score\n\nf1_score(y_train_5, y_train_pred)\n\n0.7325171197343846\n\n\n\n# extra code – this cell also computes the f1 score\ncm[1, 1] / (cm[1, 1] + (cm[1, 0] + cm[0, 1]) / 2)\n\n0.7325171197343847"
  },
  {
    "objectID": "blog4.html#precisionrecall-trade-off",
    "href": "blog4.html#precisionrecall-trade-off",
    "title": "Classification",
    "section": "Precision/Recall Trade-off",
    "text": "Precision/Recall Trade-off\n\ny_scores = sgd_clf.decision_function([some_digit])\ny_scores\n\narray([2164.22030239])\n\n\n\nthreshold = 0\ny_some_digit_pred = (y_scores &gt; threshold)\n\n\ny_some_digit_pred\n\narray([ True])\n\n\n\n# extra code – just shows that y_scores &gt; 0 produces the same result as\n#              calling predict()\ny_scores &gt; 0\n\narray([ True])\n\n\n\nthreshold = 3000\ny_some_digit_pred = (y_scores &gt; threshold)\ny_some_digit_pred\n\narray([False])\n\n\n\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n                             method=\"decision_function\")\n\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\n\nplt.figure(figsize=(8, 4))  # extra code – it's not needed, just formatting\nplt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\nplt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\nplt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n\n# extra code – this section just beautifies and saves Figure 3–5\nidx = (thresholds &gt;= threshold).argmax()  # first index ≥ threshold\nplt.plot(thresholds[idx], precisions[idx], \"bo\")\nplt.plot(thresholds[idx], recalls[idx], \"go\")\nplt.axis([-50000, 50000, 0, 1])\nplt.grid()\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"center right\")\n#save_fig(\"precision_recall_vs_threshold_plot\")\n\nplt.show()\n\n\n\n\n\nimport matplotlib.patches as patches  # extra code – for the curved arrow\n\nplt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n\nplt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n\n# extra code – just beautifies and saves Figure 3–6\nplt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\nplt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\nplt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n         label=\"Point at threshold 3,000\")\nplt.gca().add_patch(patches.FancyArrowPatch(\n    (0.79, 0.60), (0.61, 0.78),\n    connectionstyle=\"arc3,rad=.2\",\n    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n    color=\"#444444\"))\nplt.text(0.56, 0.62, \"Higher\\nthreshold\", color=\"#333333\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.axis([0, 1, 0, 1])\nplt.grid()\nplt.legend(loc=\"lower left\")\n#save_fig(\"precision_vs_recall_plot\")\n\nplt.show()\n\n\n\n\n\nidx_for_90_precision = (precisions &gt;= 0.90).argmax()\nthreshold_for_90_precision = thresholds[idx_for_90_precision]\nthreshold_for_90_precision\n\n3370.0194991439557\n\n\n\ny_train_pred_90 = (y_scores &gt;= threshold_for_90_precision)\n\n\nprecision_score(y_train_5, y_train_pred_90)\n\n0.9000345901072293\n\n\n\nrecall_at_90_precision = recall_score(y_train_5, y_train_pred_90)\nrecall_at_90_precision\n\n0.4799852425751706"
  },
  {
    "objectID": "blog4.html#the-roc-curve",
    "href": "blog4.html#the-roc-curve",
    "title": "Classification",
    "section": "The ROC Curve",
    "text": "The ROC Curve\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n\n\nidx_for_threshold_at_90 = (thresholds &lt;= threshold_for_90_precision).argmax()\ntpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n\nplt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\nplt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\nplt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\nplt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n\n# extra code – just beautifies and saves Figure 3–7\nplt.gca().add_patch(patches.FancyArrowPatch(\n    (0.20, 0.89), (0.07, 0.70),\n    connectionstyle=\"arc3,rad=.4\",\n    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n    color=\"#444444\"))\nplt.text(0.12, 0.71, \"Higher\\nthreshold\", color=\"#333333\")\nplt.xlabel('False Positive Rate (Fall-Out)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid()\nplt.axis([0, 1, 0, 1])\nplt.legend(loc=\"lower right\", fontsize=13)\n#save_fig(\"roc_curve_plot\")\n\nplt.show()\n\n\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_train_5, y_scores)\n\n0.9604938554008616\n\n\nWarning: the following cell may take a few minutes to run.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(random_state=42)\n\n\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n                                    method=\"predict_proba\")\n\n\ny_probas_forest[:2]\n\narray([[0.11, 0.89],\n       [0.99, 0.01]])\n\n\nThese are estimated probabilities. Among the images that the model classified as positive with a probability between 50% and 60%, there are actually about 94% positive images:\n\n# Not in the code\nidx_50_to_60 = (y_probas_forest[:, 1] &gt; 0.50) & (y_probas_forest[:, 1] &lt; 0.60)\nprint(f\"{(y_train_5[idx_50_to_60]).sum() / idx_50_to_60.sum():.1%}\")\n\n94.0%\n\n\n\ny_scores_forest = y_probas_forest[:, 1]\nprecisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(\n    y_train_5, y_scores_forest)\n\n\nplt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n\nplt.plot(recalls_forest, precisions_forest, \"b-\", linewidth=2,\n         label=\"Random Forest\")\nplt.plot(recalls, precisions, \"--\", linewidth=2, label=\"SGD\")\n\n# extra code – just beautifies and saves Figure 3–8\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.axis([0, 1, 0, 1])\nplt.grid()\nplt.legend(loc=\"lower left\")\n#save_fig(\"pr_curve_comparison_plot\")\n\nplt.show()\n\n\n\n\nWe could use cross_val_predict(forest_clf, X_train, y_train_5, cv=3) to compute y_train_pred_forest, but since we already have the estimated probabilities, we can just use the default threshold of 50% probability to get the same predictions much faster:\n\ny_train_pred_forest = y_probas_forest[:, 1] &gt;= 0.5  # positive proba ≥ 50%\nf1_score(y_train_5, y_train_pred_forest)\n\n0.9274509803921569\n\n\n\nroc_auc_score(y_train_5, y_scores_forest)\n\n0.9983436731328145\n\n\n\nprecision_score(y_train_5, y_train_pred_forest)\n\n0.9897468089558485\n\n\n\nrecall_score(y_train_5, y_train_pred_forest)\n\n0.8725327430363402"
  },
  {
    "objectID": "blog4.html#types-of-clustering",
    "href": "blog4.html#types-of-clustering",
    "title": "Classification",
    "section": "Types of Clustering",
    "text": "Types of Clustering\nBinary Classification is the simplest form, where the task involves distinguishing between two classes. For instance, predicting whether an email is spam or not. The mathematical formulation often includes a decision boundary, dividing the feature space into regions associated with each class.\nMulticlass Classification extends the concept to scenarios with more than two classes. A classic example is handwritten digit recognition, where the goal is to classify digits from 0 to 9. Mathematical formulations usually involve multiple decision boundaries, each separating one class from the rest.\nMulti-label Classification allows instances to be assigned to multiple classes simultaneously. For instance, a news article might belong to categories like “Politics,” “Science,” and “Technology” simultaneously. The mathematical formulation involves extending binary classification concepts to multiple classes.\n\nPerformance Metrics\nA confusion matrix is a performance measurement tool for classification algorithms, providing a detailed breakdown of the model’s predictions and actual outcomes. It is particularly useful for evaluating the performance of binary classification models but can be extended to multi-class scenarios as well. The confusion matrix is organized into four main terms:\nTrue Positives (TP): The number of instances correctly predicted as positive (correctly identified as belonging to the class of interest).\nTrue Negatives (TN): The number of instances correctly predicted as negative (correctly identified as not belonging to the class of interest).\nFalse Positives (FP): Also known as Type I errors, the number of instances incorrectly predicted as positive (instances that do not belong to the class of interest but are predicted as such).\nFalse Negatives (FN): Also known as Type II errors, the number of instances incorrectly predicted as negative (instances that belong to the class of interest but are predicted as not belonging).\nPrecision, also known as positive predictive value, measures the accuracy of the positive predictions made by the model. It is the ratio of correctly predicted positive observations to the total predicted positives. Precision is particularly relevant when the cost of false positives is high.\nPrecision = TP/TP+FP\nRecall measures the ability of the model to capture all the positive instances. It is the ratio of correctly predicted positive observations to the total actual positives. The formula is:\nRecall= TP/TP+FN\nRecall is important when the cost of false negatives is high, as it focuses on minimizing instances where positive cases are incorrectly classified as negative.\nThe F1 score is the harmonic mean of precision and recall, providing a balanced measure that considers both false positives and false negatives. It ranges between 0 and 1, with higher values indicating better performance. The formula is:\nF1 Score=2× Precision x Recall/Precision + Recall ​ F1 score is particularly useful when there is an imbalance between the classes or when both false positives and false negatives need to be considered.\nROC Curve: Receiver Operating Characteristic (ROC) curve is a graphical representation of a binary classification model’s performance at various classification thresholds. It illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) across different threshold values. The ROC curve is created by plotting the TPR against the FPR at various threshold values. The area under the ROC curve (AUC-ROC) is a common metric used to quantify the overall performance of the model. AUC-ROC ranges from 0 to 1, where a higher value indicates better performance."
  },
  {
    "objectID": "blog4.html#performance-metrics",
    "href": "blog4.html#performance-metrics",
    "title": "Classification",
    "section": "Performance Metrics",
    "text": "Performance Metrics\nA confusion matrix is a performance measurement tool for classification algorithms, providing a detailed breakdown of the model’s predictions and actual outcomes. It is particularly useful for evaluating the performance of binary classification models but can be extended to multi-class scenarios as well. The confusion matrix is organized into four main terms:\nTrue Positives (TP): The number of instances correctly predicted as positive (correctly identified as belonging to the class of interest).\nTrue Negatives (TN): The number of instances correctly predicted as negative (correctly identified as not belonging to the class of interest).\nFalse Positives (FP): Also known as Type I errors, the number of instances incorrectly predicted as positive (instances that do not belong to the class of interest but are predicted as such).\nFalse Negatives (FN): Also known as Type II errors, the number of instances incorrectly predicted as negative (instances that belong to the class of interest but are predicted as not belonging).\nPrecision, also known as positive predictive value, measures the accuracy of the positive predictions made by the model. It is the ratio of correctly predicted positive observations to the total predicted positives. Precision is particularly relevant when the cost of false positives is high.\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\n\\]\nRecall measures the ability of the model to capture all the positive instances. It is the ratio of correctly predicted positive observations to the total actual positives. The formula is:\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\n\\]\nRecall is important when the cost of false negatives is high, as it focuses on minimizing instances where positive cases are incorrectly classified as negative.\nThe F1 score is the harmonic mean of precision and recall, providing a balanced measure that considers both false positives and false negatives. It ranges between 0 and 1, with higher values indicating better performance. The formula is:\n\\[\nF1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n\\]\nF1 score is particularly useful when there is an imbalance between the classes or when both false positives and false negatives need to be considered.\nROC Curve: Receiver Operating Characteristic (ROC) curve is a graphical representation of a binary classification model’s performance at various classification thresholds. It illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) across different threshold values. The ROC curve is created by plotting the TPR against the FPR at various threshold values. The area under the ROC curve (AUC-ROC) is a common metric used to quantify the overall performance of the model. AUC-ROC ranges from 0 to 1, where a higher value indicates better performance."
  },
  {
    "objectID": "blog4.html#binary-classification",
    "href": "blog4.html#binary-classification",
    "title": "Classification",
    "section": "Binary Classification",
    "text": "Binary Classification\nLogistic Regression is a fundamental machine learning algorithm primarily used for binary and multi-class classification tasks. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability that an input belongs to a specific class. The core of logistic regression lies in the sigmoid function, mapping the linear combination of input features to a range between 0 and 1. This probability is then used to make a binary decision. Logistic Regression finds widespread use in various fields, such as finance, healthcare, and marketing, for tasks like predicting customer churn, credit default, disease diagnosis, and sentiment analysis. Its simplicity, interpretability, and efficiency make it a popular choice for both beginners and professionals in machine learning. The algorithm’s predictive power, coupled with its ability to provide insights into the impact of individual features, contributes to its versatility across different domains.\nLogistic regression can be used for binary and multi-class classification problems. The output is a probability that the given input belongs to a particular class. The sigmoid function (also called logistic function) is crucial in Logistic Regression. It maps any real-valued number to a value between 0 and 1.\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\n\\(z\\) is a linear combination of input features, and the sigmoid function ensures that the output is a probability.\nThe hypothesis function for Logistic Regression is defined as:\n\\[\nh_{\\theta}(x)=\\sigma(\\theta^{T}x),\n\\]\nwhere \\(\\theta\\) are the parameters to be learned, and x is the input feature vector.\nThe cost function is defined as the negative log-likelihood:\n\\[\nJ(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_{\\theta}(x_i)) + (1 - y_i) \\log(1 - h_{\\theta}(x_i)) \\right]\n\\]\nMinimizing the cost function helps in finding the optimal parameters θ.\nWe train the algorithm using Gradient Descent where the parameters θ are updated iteratively using the gradient descent algorithm. We use the open-source “Titanic” dataset for Logistic Regression. In the Titanic dataset, the task is to predict whether a passenger survived or not based on various features. Specifically, the binary classification target variable is “Survived,” which takes the value of 1 if the passenger survived and 0 if the passenger did not survive. The features used for prediction include information such as passenger class (Pclass), sex (Sex), age (Age), the number of siblings/spouses aboard (SibSp), the number of parents/children aboard (Parch), and the fare paid (Fare). The goal is to build a Logistic Regression model that can learn from these features and accurately classify whether a passenger survived the Titanic disaster or not.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the Titanic dataset\ntitanic_data = pd.read_csv('titanic.csv')\n\n# Display the first few rows of the dataset\nprint(titanic_data.head())\n\n# Plot the distribution of the target variable (Survived)\nsns.countplot(x='Survived', data=titanic_data)\nplt.title('Distribution of Survived')\nplt.show()\n\n# Plot the distribution of Pclass (Passenger Class)\nsns.countplot(x='Pclass', data=titanic_data, hue='Survived')\nplt.title('Distribution of Passenger Class by Survived')\nplt.show()\n\n# Plot the distribution of Age\nsns.histplot(x='Age', data=titanic_data, kde=True)\nplt.title('Distribution of Age')\nplt.show()\n\n# Plot the distribution of Fare\nsns.histplot(x='Fare', data=titanic_data, kde=True)\nplt.title('Distribution of Fare')\nplt.show()\n\n# Plot the relationship between Pclass and Fare\nsns.scatterplot(x='Pclass', y='Fare', data=titanic_data, hue='Survived')\nplt.title('Relationship between Passenger Class and Fare by Survived')\nplt.show()\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, we use seaborn and matplotlib to create various plots, such as count plots, histograms, and scatter plots. The goal is to gain insights into the distribution of variables, relationships, and potential patterns that can aid in the classification task.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.datasets import fetch_openml\nimport seaborn as sns\n# Load the Titanic dataset\ntitanic = pd.read_csv('titanic.csv')\ntitanic_data = titanic\n\n# Data preprocessing (handle missing values, encode categorical variables, etc.)\n# ...\n\n# Select relevant features and target variable\nX = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\ny = titanic_data['Survived']\n\n# Convert categorical variables to numerical\nlabel_encoder = LabelEncoder()\nX['Sex'] = label_encoder.fit_transform(X['Sex'])\n\n# Handle missing values (e.g., fill missing ages with the mean)\nX['Age'].fillna(X['Age'].mean(), inplace=True)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the Logistic Regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision, recall, and F1 score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\n# Calculate ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n\n# Generate and display the confusion matrix with blue color\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', linewidths=.5, square=True)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\nPrecision: 0.80\nRecall: 0.72\nF1 Score: 0.76\n\n\n/tmp/ipykernel_260890/3417729564.py:23: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/tmp/ipykernel_260890/3417729564.py:26: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\nWe can see that our model has high accuracy in predicting the output class and also exhibits, high levels of True Postive and True Negative rates, and low levels of False Positve and False Negative rates resulting in higher precision, recall and F1 scores as well."
  },
  {
    "objectID": "blog4.html#multi-label-classification",
    "href": "blog4.html#multi-label-classification",
    "title": "Classification",
    "section": "Multi-Label Classification",
    "text": "Multi-Label Classification\nMulti-label classification is a machine learning task where each instance is associated with multiple labels simultaneously. In contrast to traditional single-label classification, where an instance is assigned to one and only one class, multi-label classification allows an instance to belong to multiple classes or categories. This scenario arises in real-world problems where objects or data points may exhibit characteristics of more than one category.\nOne common algorithm used for multi-label classification is the k-Nearest Neighbors (KNN) algorithm. KNN is a simple and intuitive algorithm that classifies a data point based on the majority class among its k-nearest neighbors. In the context of multi-label classification, KNN can be extended to predict multiple labels by considering the labels of its nearest neighbors and applying a suitable strategy for combining them.\nUse cases for multi-label classification are diverse. For example, in text categorization, a document can belong to multiple topics simultaneously. In image classification, an image might contain several objects, each requiring its own label. Bioinformatics applications, such as predicting the functions of genes, also involve multi-label classification.\nThe mathematical formulation for the multi-label KNN algorithm involves extending the traditional KNN approach to handle multiple labels. A straightforward strategy is to assign labels based on the majority vote among the k-nearest neighbors. However, more sophisticated methods, like distance-weighted voting or considering label relationships, can be employed to improve performance.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nimport seaborn as sns\n\n# Load the digit dataset\ndigits = load_digits()\n\n# Display the first few images and their labels\nplt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(digits.images[i], cmap='gray')\n    plt.title(f\"Label: {digits.target[i]}\")\n    plt.axis('off')\nplt.suptitle('Sample Images from the Digit Dataset', y=1.02)\nplt.show()\n\n# Plot the distribution of digit labels\nsns.countplot(x=digits.target)\nplt.title('Distribution of Digit Labels')\nplt.show()\n\n\n\n\n\n\n\nWe are able to see the sample images from the digit dataset and are also able to see that the different digit labels are uniformly distributed.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Load the Digits dataset (modified for multi-label simulation)\ndigits = load_digits()\nX = pd.DataFrame(digits.data)\ny = np.random.randint(2, size=(len(digits.target), 5))  # Simulating multi-labels (5 binary labels)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the KNN classifier\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = knn_model.predict(X_test)\ny_prob = knn_model.predict_proba(X_test)  # KNN doesn't have predict_proba for multi-label, so using decision_function for ROC\n\n# Calculate precision, recall, and F1 score\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\n# Print precision, recall, and F1 score\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Generate and display the confusion matrix with blue color\nconf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nplt.figure(figsize=(8, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', linewidths=.5, square=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.title('Confusion Matrix')\nplt.show()\n\nPrecision: 0.5217\nRecall: 0.4930\nF1 Score: 0.5059\n\n\n\n\n\nIn a multi-class classification problem, each instance is assigned to one and only one class. In contrast, in a multi-label classification problem, each instance can be associated with multiple classes. To simulate a multi-label scenario using a dataset that is originally designed for multi-class classification (like the Digits dataset), we need to modify the target labels to represent this multi-label structure.\nHere’s how we simulate multi-labels in this example using the Digits dataset:\nOriginal Labels (Multi-Class): The Digits dataset originally contains labels representing digits from 0 to 9. Each instance is assigned a single digit label.\nSimulated Multi-Labels: To simulate a multi-label scenario, we generate random binary labels for each instance. Each binary label corresponds to a specific condition or characteristic. For simplicity, in this example, I generated 5 binary labels for each instance using np.random.randint(2, size=(len(digits.target), 5)). Each binary label can be interpreted as a separate condition that the instance may or may not satisfy."
  },
  {
    "objectID": "blog4.html#types-of-classification",
    "href": "blog4.html#types-of-classification",
    "title": "Classification",
    "section": "Types of Classification",
    "text": "Types of Classification\nBinary Classification is the simplest form, where the task involves distinguishing between two classes. For instance, predicting whether an email is spam or not. The mathematical formulation often includes a decision boundary, dividing the feature space into regions associated with each class.\nMulticlass Classification extends the concept to scenarios with more than two classes. A classic example is handwritten digit recognition, where the goal is to classify digits from 0 to 9. Mathematical formulations usually involve multiple decision boundaries, each separating one class from the rest.\nMulti-label Classification allows instances to be assigned to multiple classes simultaneously. For instance, a news article might belong to categories like “Politics,” “Science,” and “Technology” simultaneously. The mathematical formulation involves extending binary classification concepts to multiple classes."
  },
  {
    "objectID": "blog5.html#gaussian-mixture-models",
    "href": "blog5.html#gaussian-mixture-models",
    "title": "Anomaly Detection",
    "section": "Gaussian Mixture Models",
    "text": "Gaussian Mixture Models\nGaussian Mixture Models represent a probabilistic model for representing the presence of subpopulations within an overall population. It assumes that the data is generated by a mixture of several Gaussian distributions with unknown parameters.\nIt uses a mixture of several Gaussian distributions. The probability density function (PDF) for a GMM is given by:\nP(x)=∑k=1K πk N(x∣μk,Σk) where:\nπk is the weight of the k-th Gaussian component, representing the proportion of data points assigned to that component. μk is the mean vector of the k-th Gaussian component, Σk is the covariance matrix of the k-th Gaussian component. N is the Gaussian distribution function. The goal of GMM is to estimate the parameters πk, μk, and Σk that maximize the likelihood of the observed data.\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nX1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\nX1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\nX2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\nX2 = X2 + [6, -8]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\ngm = GaussianMixture(n_components=3, n_init=10, random_state=42)\ngm.fit(X)\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z,\n                 norm=LogNorm(vmin=1.0, vmax=30.0),\n                 levels=np.logspace(0, 2, 12))\n    plt.contour(xx, yy, Z,\n                norm=LogNorm(vmin=1.0, vmax=30.0),\n                levels=np.logspace(0, 2, 12),\n                linewidths=1, colors='k')\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.show()\n\n\n\n\nBayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) are statistical metrics used for model selection, particularly in the context of Gaussian Mixture Models (GMM) and other probabilistic models. The AIC is a measure of the relative quality of a statistical model for a given set of data. It balances the goodness of fit of the model with the simplicity of the model (to avoid overfitting). Similar to AIC, BIC is used for model selection and balances goodness of fit with model complexity. However, BIC imposes a stronger penalty for models with more parameters.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n\n# Load Credit Card Fraud Detection dataset\nurl = 'creditcard.csv'\ndf = pd.read_csv(url)\n\n# Split the data into features (X) and labels (y)\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Gaussian Mixture Model\ngmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\ngmm.fit(X_train)\n\n# Predict anomaly scores (negative log likelihood) on the test set\nanomaly_scores = -gmm.score_samples(X_test)\n\n# Set a threshold to classify anomalies\nthreshold = np.percentile(anomaly_scores, 95)\npredictions = anomaly_scores &gt; threshold\n\n# Evaluate performance\nprecision = precision_score(y_test, predictions)\nrecall = recall_score(y_test, predictions)\nf1 = f1_score(y_test, predictions)\nconf_matrix = confusion_matrix(y_test, predictions)\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, anomaly_scores)\nroc_auc = auc(fpr, tpr)\n\n# Visualize the ROC curve\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, lw=2, label=f'AUC = {roc_auc:.2f}')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n\n# Visualize the confusion matrix\nplt.figure(figsize=(8, 8))\nplt.imshow(conf_matrix, interpolation='nearest', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.xticks([0, 1], ['Non-Fraud', 'Fraud'])\nplt.yticks([0, 1], ['Non-Fraud', 'Fraud'])\nplt.show()\n\nPrecision: 0.0302\nRecall: 0.8776\nF1 Score: 0.0584"
  },
  {
    "objectID": "blog5.html#anomaly-detection",
    "href": "blog5.html#anomaly-detection",
    "title": "Anomaly Detection",
    "section": "Anomaly Detection",
    "text": "Anomaly Detection\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\n\ndf = pd.read_csv('creditcard.csv')\nfeatures=['V17','V14', 'V11', 'V4', 'V15', 'V13']\nnplots=np.size(features)\nplt.figure(figsize=(15,4*nplots))\ngs = gridspec.GridSpec(nplots,1)\nfor i, feat in enumerate(features):\n    ax = plt.subplot(gs[i])\n    sns.histplot(df[feat][df.Class==1], bins=30, kde=True, stat=\"density\")\n    sns.histplot(df[feat][df.Class==0],bins=30, kde=True, stat=\"density\")\n    ax.legend(['fraudulent', 'non-fraudulent'],loc='best')\n    ax.set_xlabel('')\n    ax.set_title('Distribution of feature: ' + feat)\n\n\n\n\n\n# Create the normal data\nX_normal, y_normal = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=1, random_state=42)\ndf_normal = pd.DataFrame({'feature1': X_normal[:, 0], 'feature2': X_normal[:, 1], 'anomaly_indicator': 0})# Create the anomaly data\nX_anomaly, y_anomaly = make_blobs(n_samples=20, centers=2, n_features=2, cluster_std =10, random_state=0)\ndf_anomaly = pd.DataFrame({'feature1': X_anomaly[:, 0], 'feature2': X_anomaly[:, 1], 'anomaly_indicator': 1})# Combine the normal and the anomaly data\ndf = pd.concat([df_normal, df_anomaly])# Change figure size\nplt.figure(figsize=(12, 8))# Visualization\nsns.scatterplot(x=df['feature1'], y=df['feature2'], hue=df['anomaly_indicator'])\n\n&lt;Axes: xlabel='feature1', ylabel='feature2'&gt;\n\n\n\n\n\n\n# Model dataset\nX = df[df.columns.difference(['anomaly_indicator'])]# GMM model\ngmm = GaussianMixture(n_components=3, n_init=5, random_state=42)# Fit and predict on the data\ny_gmm = gmm.fit_predict(X)\n\n\n# Get the score for each sample\nscore = gmm.score_samples(X)# Save score as a column\ndf['score'] = score# Get the score threshold for anomaly\npct_threshold = np.percentile(score, 4)# Print the score threshold\nprint(f'The threshold of the score is {pct_threshold:.2f}')# Label the anomalies\ndf['anomaly_gmm_pct'] = df['score'].apply(lambda x: 1 if x &lt; pct_threshold else 0)\n\nThe threshold of the score is -6.56\n\n\n\n# Visualize the actual and predicted anomalies\nfig, (ax0, ax1)=plt.subplots(1,2, sharey=True, figsize=(20,12))\n# Ground truth\nax0.set_title('Ground Truth')\nax0.scatter(df['feature1'], df['feature2'], c=df['anomaly_indicator'], cmap='rainbow')\n# GMM Predictions\nax1.set_title('GMM Predict Anomalies Using Percentage')\nax1.scatter(df['feature1'], df['feature2'], c=df['anomaly_gmm_pct'], cmap='rainbow')\n\n&lt;matplotlib.collections.PathCollection at 0x7f127ec46750&gt;\n\n\n\n\n\n\n# Change figure size\nplt.figure(figsize=(12, 8))# Check score distribution\nsns.histplot(df['score'], bins=100, alpha=0.8)# Threshold value\nplt.axvline(x=-5.5, color='orange')\n\n&lt;matplotlib.lines.Line2D at 0x7f127ebfd250&gt;\n\n\n\n\n\n\n# Get the score threshold for anomaly\nvalue_threshold = -5.5# Label the anomalies\ndf['anomaly_gmm_value'] = df['score'].apply(lambda x: 1 if x &lt; value_threshold else 0)# Visualize the actual and predicted anomalies\nfig, (ax0, ax1)=plt.subplots(1,2, sharey=True, figsize=(20,12))\n# Ground truth\nax0.set_title('Ground Truth')\nax0.scatter(df['feature1'], df['feature2'], c=df['anomaly_indicator'], cmap='rainbow')\n# GMM Predictions\nax1.set_title('GMM Predict Anomalies Using Value')\nax1.scatter(df['feature1'], df['feature2'], c=df['anomaly_gmm_value'], cmap='rainbow')\n\n&lt;matplotlib.collections.PathCollection at 0x7f127ec09ad0&gt;"
  },
  {
    "objectID": "blog5.html#anomaly-detection-using-gaussian-mixtures",
    "href": "blog5.html#anomaly-detection-using-gaussian-mixtures",
    "title": "Anomaly Detection",
    "section": "Anomaly Detection using Gaussian Mixtures",
    "text": "Anomaly Detection using Gaussian Mixtures\nOne popular algorithm for anomaly detection is the Gaussian Mixture Model (GMM). GMM is a probabilistic model that assumes the data is generated by a mixture of several Gaussian distributions. In the context of anomaly detection, a GMM is trained on the normal (non-anomalous) instances in the dataset. Anomalies are then identified based on the likelihood of instances under the learned GMM; instances with low likelihoods are considered anomalies.\nIt can be applied to various use cases like, like Fraud detection in finance to identify unusual patterns that may indicate fraudulent activities, such as unauthorized transactions. In Network Security, detecting unusual patterns in network traffic can help identify potential cyberattacks or security breaches.\nFor illustrating Anomaly Detection with GMMs, we are creating a synthetic dataset with the make_blobs function from scikit-learn. This dataset is designed to have a specified number of clusters, and each cluster represents a group of similar instances. In this case, we are creating a dataset with three clusters of 500 points. We also manually introduce anomalies by adding synthetically generated datapoints to the dataset.\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Create synethetic data\nX_syn, y_syn = make_blobs(n_samples=500, centers=4, n_features=2, cluster_std=1, random_state=42)\ndf_syn = pd.DataFrame({'feature1': X_syn[:, 0], 'feature2': X_syn[:, 1], 'anomaly_indicator': 0})# Create the anomaly data\nX_anomaly, y_anomaly = make_blobs(n_samples=20, centers=2, n_features=2, cluster_std =10, random_state=0)\ndf_anomaly = pd.DataFrame({'feature1': X_anomaly[:, 0], 'feature2': X_anomaly[:, 1], 'anomaly_indicator': 1})# Combine the normal and the anomaly data\ndf = pd.concat([df_syn, df_anomaly])# Change figure size\nplt.figure(figsize=(12, 8))# Visualization\nsns.scatterplot(x=df['feature1'], y=df['feature2'], hue=df['anomaly_indicator'])\n\n&lt;Axes: xlabel='feature1', ylabel='feature2'&gt;\n\n\n\n\n\nWe add an anomaly indicator variable and visualize the data points and anomaly on a scatter plot.\n\n# Model dataset\nX = df[df.columns.difference(['anomaly_indicator'])]# GMM model\ngmm = GaussianMixture(n_components=4, n_init=5, random_state=42)# Fit and predict on the data\ny_gmm = gmm.fit_predict(X)\n\nWe use the Gaussian Mixture Model (GMM) for anomaly detection. n_components=3 indicates that we expect the data to be generated from three Gaussian distributions (normal instances and anomalies). The GMM provides negative log likelihood scores for each instance. Higher scores indicate that an instance is less likely to be part of the learned distribution. The negative log likelihood serves as an anomaly score. Therefore, instances with higher negative log likelihoods are considered potential anomalies.\n\n# Get the score for each sample\nscore = gmm.score_samples(X)# Save score as a column\ndf['score'] = score# Get the score threshold for anomaly\nthreshold = np.percentile(score, 4)# Print the score threshold\nprint(f'The threshold score is {threshold:.2f}')# Label the anomalies\ndf['anomaly_gmm'] = df['score'].apply(lambda x: 1 if x &lt; threshold else 0)\n\nThe threshold score is -7.12\n\n\nA threshold can be set on the negative log likelihood scores to classify instances as anomalies. Instances with negative log likelihoods exceeding the threshold are labeled as anomalies. We set a threshold that 4% of the overall datapoints to be outliers and identified the corresponding threshold score which comes to -6.56. Thus, we look for points that posses higher negative log likelihoods than the threshold and label them as outliers.\n\n# Visualize the actual and predicted anomalies\nfig, (ax0, ax1)=plt.subplots(1,2, sharey=True)\n# Ground truth\nax0.set_title('Ground Truth\\n(threshold = -6.56)')\nax0.scatter(df['feature1'], df['feature2'], c=df['anomaly_indicator'], cmap='rainbow')\n# GMM Predictions\nax1.set_title('GMM Predicted Anomalies\\n(threshold = -6.56)')\nax1.scatter(df['feature1'], df['feature2'], c=df['anomaly_gmm'], cmap='rainbow')\n\n&lt;matplotlib.collections.PathCollection at 0x7fcb913ab890&gt;\n\n\n\n\n\nWe visualize the results and see that the ground truth is very close to the output generated by the GMM. We can adjust the threshold as needed based on the desired level of sensitivity for anomaly detection.\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc\n\ny_test = df['anomaly_indicator']\npredictions = df['anomaly_gmm']\n\n# Evaluate performance\nprecision = precision_score(y_test, predictions)\nrecall = recall_score(y_test, predictions)\nf1 = f1_score(y_test, predictions)\naccuracy = accuracy_score(y_test, predictions)\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")\n\nPrecision: 0.7143\nRecall: 0.7500\nF1 Score: 0.7317\nAccuracy: 0.9788\n\n\nWe now perform anomaly detection for a threshold value = -5.56 and see how it impacts the anomaly detection process by visualization, precision, recall and f1 scores.\n\n# Get the score threshold for anomaly\nvalue_threshold = -5.5# Label the anomalies\ndf['anomaly_gmm_v'] = df['score'].apply(lambda x: 1 if x &lt; value_threshold else 0)# Visualize the actual and predicted anomalies\nfig, (ax0, ax1)=plt.subplots(1,2, sharey=True)\n# Ground truth\nax0.set_title('Ground Truth\\n(threshold = -5.56)')\nax0.scatter(df['feature1'], df['feature2'], c=df['anomaly_indicator'], cmap='rainbow')\n# GMM Predictions\nax1.set_title('GMM Predicted Anomalies\\n(threshold = -5.56)')\nax1.scatter(df['feature1'], df['feature2'], c=df['anomaly_gmm_v'], cmap='rainbow')\n\n&lt;matplotlib.collections.PathCollection at 0x7fcb65fb4c50&gt;\n\n\n\n\n\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc\n\ny_test = df['anomaly_indicator']\npredictions = df['anomaly_gmm_v']\n\n# Evaluate performance\nprecision = precision_score(y_test, predictions)\nrecall = recall_score(y_test, predictions)\nf1 = f1_score(y_test, predictions)\naccuracy = accuracy_score(y_test, predictions)\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")\n\nPrecision: 0.2469\nRecall: 1.0000\nF1 Score: 0.3960\nAccuracy: 0.8827\n\n\nThe threshold is very important for anaomaly detection as a lower threshold can be seen to classify normal data points as anomalies and reduces the accuracy, precision and recall as well."
  },
  {
    "objectID": "blog5.html#anomaly-detection-using-pca",
    "href": "blog5.html#anomaly-detection-using-pca",
    "title": "Anomaly Detection",
    "section": "Anomaly Detection using PCA",
    "text": "Anomaly Detection using PCA\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\n\n# Load your dataset (assuming 'data' is your DataFrame)\n# Example with a synthetic dataset\nfrom sklearn.datasets import make_blobs\ndata, _ = make_blobs(n_samples=1000, centers=1, random_state=42, cluster_std=2.0)\n\n# Standardize the data\nscaler = StandardScaler()\ndata_standardized = scaler.fit_transform(data)\n\n# Apply PCA\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(data_standardized)\n\n# Plot the data in the reduced-dimensional space\nplt.figure(figsize=(8, 6))\nplt.scatter(principal_components[:, 0], principal_components[:, 1], alpha=0.8)\nplt.title('PCA: Reduced-dimensional Space')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n# Identify anomalies using Isolation Forest\nisolation_forest = IsolationForest(contamination=0.05, random_state=42)\nisolation_forest.fit(data_standardized)\nanomaly_scores = isolation_forest.decision_function(data_standardized)\n\n# Visualize anomalies\nplt.figure(figsize=(8, 6))\nplt.scatter(principal_components[:, 0], principal_components[:, 1], c=(anomaly_scores &lt; 0), cmap='viridis', alpha=0.8)\nplt.title('Anomalies Identified by Isolation Forest')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()"
  },
  {
    "objectID": "blog5.html#anomaly-detection-using-isolation-forest-and-pca",
    "href": "blog5.html#anomaly-detection-using-isolation-forest-and-pca",
    "title": "Anomaly Detection",
    "section": "Anomaly Detection using Isolation Forest and PCA",
    "text": "Anomaly Detection using Isolation Forest and PCA\nThe Isolation Forest algorithm is an unsupervised machine learning algorithm designed for anomaly detection. It operates by isolating instances that are deemed rare or different from the majority of the data. The core idea behind Isolation Forest is to isolate anomalies rather than trying to model the normal behavior explicitly. It achieves this by constructing isolation trees, which are binary trees where each node represents a feature and each path from the root to a leaf isolates an instance.\nFor each tree, a random subset of the data is used. This randomness allows the algorithm to create diverse trees, making it robust and less prone to overfitting. At each node of the tree, a random feature is selected, and a random split value within the range of the selected feature’s values is chosen. An anomaly is expected to be more isolated and, therefore, is likely to have a shorter average path length in the trees. The average path length for a data point across multiple trees is used as the anomaly score.Instances with shorter path lengths are considered anomalies, as they require fewer splits to be isolated in the trees.\nIsolation Forest is particularly effective in identifying anomalies because it exploits the fact that anomalies are typically rare and have distinctive properties. By isolating instances in a random and binary manner, anomalies are separated from the majority of the data more quickly, making the algorithm efficient and effective.\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and statistics. Its main goal is to transform a high-dimensional dataset into a lower-dimensional form while retaining as much of the original variability as possible. PCA begins by standardizing the dataset, ensuring that each feature has zero mean and unit variance. This is important as it gives equal weight to all features during the analysis.\nPCA then calculates the covariance matrix of the standardized data. The covariance matrix provides information about the relationships between different features. The next step is to perform eigendecomposition on the covariance matrix. This process yields eigenvalues and corresponding eigenvectors. Eigenvectors represent the directions (principal components) of maximum variance in the data, and eigenvalues indicate the amount of variance along each eigenvector.\nPrincipal components are selected based on the eigenvalues. The components associated with the highest eigenvalues capture the most variance in the data. Typically, the principal components are ranked in descending order of their eigenvalues. Finally, the original data is projected onto the selected principal components, resulting in a lower-dimensional representation of the dataset.\nIn the context of anomaly detection, PCA is often employed to reduce the dimensionality of the data and highlight the most significant features. Anomalies may manifest as deviations from the expected patterns in the reduced-dimensional space, making them easier to identify. PCA can be used as a preprocessing step to reduce the dimensionality of the data before applying anomaly detection algorithms, including Isolation Forest. In the context of anomaly detection, PCA can help emphasize the most relevant features, making it easier for algorithms like Isolation Forest to identify deviations from normal patterns.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\n\n# Example with a synthetic dataset\nfrom sklearn.datasets import make_blobs\ndata, _ = make_blobs(n_samples=1000, centers=1, random_state=42, cluster_std=2.0)\n\n# Standardize the data\nscaler = StandardScaler()\ndata_standardized = scaler.fit_transform(data)\n\n# Apply PCA\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(data_standardized)\n\n# Plot the data in the reduced-dimensional space\nplt.figure(figsize=(8, 6))\nplt.scatter(principal_components[:, 0], principal_components[:, 1], alpha=0.8)\nplt.title('PCA: Reduced-dimensional Space')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n# Identify anomalies using Isolation Forest\nisolation_forest = IsolationForest(contamination=0.05, random_state=42)\nisolation_forest.fit(data_standardized)\nanomaly_scores = isolation_forest.decision_function(data_standardized)\n\n# Visualize anomalies\nplt.figure(figsize=(8, 6))\nplt.scatter(principal_components[:, 0], principal_components[:, 1], c=(anomaly_scores &lt; 0), cmap='viridis', alpha=0.8)\nplt.title('Anomalies Identified by Isolation Forest')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n\n\n\n\n\n\nAnomaly detection using Gaussian Mixture Models (GMM) and Isolation Forests with Principal Component Analysis (PCA) offers robust techniques for identifying unusual patterns in complex datasets. GMM excels in capturing intricate data distributions, while Isolation Forests efficiently isolate anomalies through tree-based structures. Incorporating PCA enhances dimensionality reduction, aiding in the identification of subtle anomalies. These methods find applications in diverse fields, including fraud detection, network security, and system health monitoring, where accurate anomaly identification is critical for ensuring the integrity and reliability of systems."
  }
]